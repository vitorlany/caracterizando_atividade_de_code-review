[
    {
        "node": {
            "title": "refactor",
            "url": "https://github.com/ollama/ollama/pull/1",
            "state": "MERGED",
            "createdAt": "2023-06-27T19:29:54Z",
            "mergedAt": "2023-06-27T20:49:08Z",
            "closedAt": "2023-06-27T20:49:08Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 7
            },
            "additions": 196,
            "deletions": 216,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use poetry",
            "url": "https://github.com/ollama/ollama/pull/3",
            "state": "MERGED",
            "createdAt": "2023-06-27T21:44:37Z",
            "mergedAt": "2023-06-28T14:36:35Z",
            "closedAt": "2023-06-28T14:36:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 1363,
            "deletions": 22,
            "body": "`requirements.txt` updated with `poetry export`\r\n\r\ninstall ollama into the poetry virtualenv:\r\n`poetry install && ollama list`\r\n\r\nuse ollama as a python module script:\r\n`poetry run python -m ollama list`\r\n\r\nbinary artifacts can be build and archived:\r\n`poetry build && ls -al dist/`",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add prompt templates as j2 templates",
            "url": "https://github.com/ollama/ollama/pull/7",
            "state": "MERGED",
            "createdAt": "2023-06-27T22:50:30Z",
            "mergedAt": "2023-06-28T14:37:03Z",
            "closedAt": "2023-06-28T14:37:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 16
            },
            "additions": 196,
            "deletions": 440,
            "body": "easier to read and maintain since diffs are much more obvious. this also provides future opportunity for users to define their own prompt templates",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add with symlink",
            "url": "https://github.com/ollama/ollama/pull/10",
            "state": "CLOSED",
            "createdAt": "2023-06-28T16:26:42Z",
            "mergedAt": null,
            "closedAt": "2023-06-30T18:54:22Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "interactive generate",
            "url": "https://github.com/ollama/ollama/pull/11",
            "state": "MERGED",
            "createdAt": "2023-06-28T18:24:35Z",
            "mergedAt": "2023-06-28T18:32:06Z",
            "closedAt": "2023-06-28T18:32:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 32,
            "deletions": 4,
            "body": "generate prompt is now optional. if a prompt is omitted, it will start an interactive session\r\n\r\nprevious behaviour with prompt\r\n\r\n```\r\n$ ollama generate ~/Downloads/vicuna-7b-v1.3.ggmlv3.q4_0.bin 'Hi!'\r\n>>> Hi!\r\n\r\n Hello! How can I help you today?\r\n\r\n```\r\n\r\nnew behaviour without a prompt. ctrl-c to exit interactive mode\r\n\r\n```\r\n$ ollama generate ~/Downloads/vicuna-7b-v1.3.ggmlv3.q4_0.bin\r\n>>> Hi!\r\n\r\n Hello! How can I help you today?\r\n\r\n>>> How are you?\r\n\r\n I am just a computer program, so I don't have feelings or emotions like humans do. I exist solely to provide information and assist with tasks to the best of my abilities. Is there anything specific you would like help with today?\r\n\r\n>>>\r\n```\r\n\r\nnew behaviour with a input file\r\n```\r\n$ ollama generate orca-mini-3b.ggmlv3.q4_0 <questions.txt\r\n>>> Hi!\r\n\r\n Hello! How can I assist you today?\r\n\r\n>>> How are you?\r\n\r\n I'm doing well, thank you for asking. How about you?\r\n\r\n>>>\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add prompt templates as j2 templates",
            "url": "https://github.com/ollama/ollama/pull/12",
            "state": "MERGED",
            "createdAt": "2023-06-28T18:51:07Z",
            "mergedAt": "2023-06-28T18:53:50Z",
            "closedAt": "2023-06-28T18:53:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 16
            },
            "additions": 194,
            "deletions": 440,
            "body": "#7 is missing from main",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update development.md",
            "url": "https://github.com/ollama/ollama/pull/13",
            "state": "MERGED",
            "createdAt": "2023-06-28T19:30:01Z",
            "mergedAt": "2023-06-28T19:44:56Z",
            "closedAt": "2023-06-28T19:44:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 23,
            "deletions": 29,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "batch model",
            "url": "https://github.com/ollama/ollama/pull/15",
            "state": "MERGED",
            "createdAt": "2023-06-28T21:34:34Z",
            "mergedAt": "2023-06-29T00:10:39Z",
            "closedAt": "2023-06-29T00:10:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 22,
            "deletions": 13,
            "body": "add a batch model which is distinct in the way the prompts are displayed to the user. this produces a cleaner output without a trailing `>>>`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update python version",
            "url": "https://github.com/ollama/ollama/pull/16",
            "state": "MERGED",
            "createdAt": "2023-06-28T21:46:35Z",
            "mergedAt": "2023-06-29T00:12:03Z",
            "closedAt": "2023-06-29T00:12:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 166,
            "deletions": 356,
            "body": "python 3.8 is the oldest version still maintained",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use ctransformers as backup to llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/17",
            "state": "MERGED",
            "createdAt": "2023-06-29T00:26:07Z",
            "mergedAt": "2023-06-30T18:46:13Z",
            "closedAt": "2023-06-30T18:46:14Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 12
            },
            "additions": 823,
            "deletions": 74,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update generate to yield object",
            "url": "https://github.com/ollama/ollama/pull/18",
            "state": "MERGED",
            "createdAt": "2023-06-29T00:57:40Z",
            "mergedAt": "2023-06-29T01:06:18Z",
            "closedAt": "2023-06-29T01:06:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 5,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove server extras for now",
            "url": "https://github.com/ollama/ollama/pull/19",
            "state": "MERGED",
            "createdAt": "2023-06-29T00:57:55Z",
            "mergedAt": "2023-06-29T01:04:37Z",
            "closedAt": "2023-06-29T01:04:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 349,
            "deletions": 31,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use difflib.get_close_matches",
            "url": "https://github.com/ollama/ollama/pull/20",
            "state": "MERGED",
            "createdAt": "2023-06-29T16:43:57Z",
            "mergedAt": "2023-06-30T18:54:07Z",
            "closedAt": "2023-06-30T18:54:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 13,
            "body": "same result with less code",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove add cmd",
            "url": "https://github.com/ollama/ollama/pull/24",
            "state": "MERGED",
            "createdAt": "2023-06-29T23:20:02Z",
            "mergedAt": "2023-06-29T23:29:23Z",
            "closedAt": "2023-06-29T23:29:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 24,
            "body": "\ud83e\uddf9 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix run arg parser",
            "url": "https://github.com/ollama/ollama/pull/26",
            "state": "MERGED",
            "createdAt": "2023-06-29T23:32:45Z",
            "mergedAt": "2023-06-29T23:33:54Z",
            "closedAt": "2023-06-29T23:33:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "the bug has no adverse effects other than to the reader. for clarity, rename it to run_parser",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Pull model name",
            "url": "https://github.com/ollama/ollama/pull/29",
            "state": "MERGED",
            "createdAt": "2023-06-30T18:58:00Z",
            "mergedAt": "2023-06-30T18:58:58Z",
            "closedAt": "2023-06-30T18:58:58Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 13,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "just in time install llama-cpp-python",
            "url": "https://github.com/ollama/ollama/pull/32",
            "state": "CLOSED",
            "createdAt": "2023-07-01T00:04:44Z",
            "mergedAt": null,
            "closedAt": "2023-07-06T16:30:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 33,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "upgrade fuzzy search library",
            "url": "https://github.com/ollama/ollama/pull/37",
            "state": "MERGED",
            "createdAt": "2023-07-05T18:16:14Z",
            "mergedAt": "2023-07-05T19:16:19Z",
            "closedAt": "2023-07-05T19:16:19Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 28,
            "deletions": 31,
            "body": "fuzzywuzzy was renamed starting 0.19 so use that instead\r\n\r\nuse process.extract to produce a list of fuzzy matches instead of process.extractOne",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "enable metal gpu acceleration",
            "url": "https://github.com/ollama/ollama/pull/39",
            "state": "MERGED",
            "createdAt": "2023-07-06T01:12:35Z",
            "mergedAt": "2023-07-06T01:45:53Z",
            "closedAt": "2023-07-06T01:45:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 15,
            "deletions": 15,
            "body": "ggml-metal.metal must be in the same directory as the ollama binary otherwise llama.cpp will not be able to find it and load it.\r\n\r\n1. go generate llama/llama_metal.go\r\n2. go build .\r\n3. ./ollama serve",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use prompt templates",
            "url": "https://github.com/ollama/ollama/pull/40",
            "state": "MERGED",
            "createdAt": "2023-07-06T17:40:28Z",
            "mergedAt": "2023-07-06T17:45:48Z",
            "closedAt": "2023-07-06T17:45:48Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 17
            },
            "additions": 110,
            "deletions": 17,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "tcp socket",
            "url": "https://github.com/ollama/ollama/pull/41",
            "state": "MERGED",
            "createdAt": "2023-07-06T17:56:25Z",
            "mergedAt": "2023-07-06T18:15:32Z",
            "closedAt": "2023-07-06T18:15:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 48,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "free llama model",
            "url": "https://github.com/ollama/ollama/pull/42",
            "state": "MERGED",
            "createdAt": "2023-07-06T18:15:23Z",
            "mergedAt": "2023-07-06T18:16:22Z",
            "closedAt": "2023-07-06T18:16:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "more free",
            "url": "https://github.com/ollama/ollama/pull/43",
            "state": "MERGED",
            "createdAt": "2023-07-06T18:25:15Z",
            "mergedAt": "2023-07-06T19:04:44Z",
            "closedAt": "2023-07-06T19:04:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "embed templates",
            "url": "https://github.com/ollama/ollama/pull/45",
            "state": "MERGED",
            "createdAt": "2023-07-06T18:34:05Z",
            "mergedAt": "2023-07-06T18:36:26Z",
            "closedAt": "2023-07-06T18:36:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 14
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Go simple response",
            "url": "https://github.com/ollama/ollama/pull/46",
            "state": "MERGED",
            "createdAt": "2023-07-06T19:36:16Z",
            "mergedAt": "2023-07-06T19:38:47Z",
            "closedAt": "2023-07-06T19:38:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 14,
            "deletions": 29,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Go run",
            "url": "https://github.com/ollama/ollama/pull/49",
            "state": "MERGED",
            "createdAt": "2023-07-06T23:03:39Z",
            "mergedAt": "2023-07-07T00:18:58Z",
            "closedAt": "2023-07-07T00:18:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 235,
            "deletions": 187,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "pass model and predict options",
            "url": "https://github.com/ollama/ollama/pull/52",
            "state": "MERGED",
            "createdAt": "2023-07-07T00:12:59Z",
            "mergedAt": "2023-07-07T17:59:11Z",
            "closedAt": "2023-07-07T17:59:11Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 151,
            "deletions": 16,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "no prompt on empty line",
            "url": "https://github.com/ollama/ollama/pull/54",
            "state": "MERGED",
            "createdAt": "2023-07-07T18:26:41Z",
            "mergedAt": "2023-07-07T18:29:39Z",
            "closedAt": "2023-07-07T18:29:39Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 31,
            "deletions": 26,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix run generate",
            "url": "https://github.com/ollama/ollama/pull/55",
            "state": "MERGED",
            "createdAt": "2023-07-07T18:27:02Z",
            "mergedAt": "2023-07-07T18:37:56Z",
            "closedAt": "2023-07-07T18:37:56Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 37,
            "body": "This fixes the run request where struct defaults are used instead of real defaults.\r\n\r\nThis also removes the existence check for pulled images which @BruceMacD will address server side",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "if directory cannot be resolved, do not fail",
            "url": "https://github.com/ollama/ollama/pull/56",
            "state": "MERGED",
            "createdAt": "2023-07-07T19:28:14Z",
            "mergedAt": "2023-07-08T03:18:25Z",
            "closedAt": "2023-07-08T03:18:25Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 36,
            "deletions": 1,
            "body": "allow for offline mode",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "take all args as one prompt",
            "url": "https://github.com/ollama/ollama/pull/57",
            "state": "MERGED",
            "createdAt": "2023-07-07T20:16:27Z",
            "mergedAt": "2023-07-10T10:05:09Z",
            "closedAt": "2023-07-10T10:05:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 11,
            "deletions": 12,
            "body": "- parse all run arguments into one prompt\r\n- do not echo prompt back on one-shot\r\n- example of summarizing a document\r\n\r\n```\r\n$ ollama run nous-hermes \"$(cat input.txt)\", please summarize this story\r\nThe song \"Summertime\" by Will Smith is about the rapper's life before fame and how it changed when he became successful. The lyrics describe his experiences growing up in Philadelphia and then suddenly becoming rich and famous. The song is a coming-of-age story that describes the rapper's journey from poverty to stardom, and the impact of this change on his life.\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "return error in generate response",
            "url": "https://github.com/ollama/ollama/pull/58",
            "state": "MERGED",
            "createdAt": "2023-07-07T21:27:14Z",
            "mergedAt": "2023-07-10T21:03:47Z",
            "closedAt": "2023-07-10T21:03:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 38,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llama: replace bindings with `llama_*` calls",
            "url": "https://github.com/ollama/ollama/pull/62",
            "state": "CLOSED",
            "createdAt": "2023-07-10T02:44:43Z",
            "mergedAt": null,
            "closedAt": "2023-07-10T22:51:37Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 10
            },
            "additions": 101,
            "deletions": 983,
            "body": "Early PR to replace the C++ binding files with direct calls to llama.cpp from Go. It's missing quite a few features the C++ `binding.cpp` files had although those were almost direct copies of the `main` example in llama.cpp's repo. We should be able to add them back relatively easily. It's most likely slower as well so we'll have to make sure it's as fast / faster than the bindings or c++ example implementation.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Do not seg fault on client disconnect",
            "url": "https://github.com/ollama/ollama/pull/64",
            "state": "MERGED",
            "createdAt": "2023-07-10T11:46:30Z",
            "mergedAt": "2023-07-10T15:00:44Z",
            "closedAt": "2023-07-10T15:00:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 30,
            "deletions": 14,
            "body": "This was nicer to fix on the revised `b2` branch, so this is a pull request into that simplified change",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "call llama.cpp directly from go",
            "url": "https://github.com/ollama/ollama/pull/65",
            "state": "MERGED",
            "createdAt": "2023-07-10T20:36:56Z",
            "mergedAt": "2023-07-11T19:01:03Z",
            "closedAt": "2023-07-11T19:01:03Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 29
            },
            "additions": 35552,
            "deletions": 1292,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "embeddings endpoint",
            "url": "https://github.com/ollama/ollama/pull/66",
            "state": "MERGED",
            "createdAt": "2023-07-11T16:39:48Z",
            "mergedAt": "2023-08-10T15:49:55Z",
            "closedAt": "2023-08-10T15:49:55Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 2
            },
            "additions": 85,
            "deletions": 31,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "app: write logs to ~/.ollama/logs",
            "url": "https://github.com/ollama/ollama/pull/67",
            "state": "MERGED",
            "createdAt": "2023-07-11T17:42:13Z",
            "mergedAt": "2023-07-11T18:45:21Z",
            "closedAt": "2023-07-11T18:45:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 261,
            "deletions": 24,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use embeddings to give the chat client session memory",
            "url": "https://github.com/ollama/ollama/pull/69",
            "state": "CLOSED",
            "createdAt": "2023-07-11T20:53:44Z",
            "mergedAt": null,
            "closedAt": "2023-07-20T18:04:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 174,
            "deletions": 9,
            "body": "Store previous questions and answers in the client during a chat session. Use embeddings to look-up what is relevant to the current context. \r\n\r\nThis is an initial implementation. We will need to iterate to improve this experience through more dynamic prompts and possibly weighting recency in the conversation too. ",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "offline fixes",
            "url": "https://github.com/ollama/ollama/pull/70",
            "state": "MERGED",
            "createdAt": "2023-07-11T20:57:54Z",
            "mergedAt": "2023-07-11T22:50:19Z",
            "closedAt": "2023-07-11T22:50:19Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 143,
            "deletions": 144,
            "body": "two fixes bundled into one pr\r\n\r\n1. pull should not block generate. this changes the status code when the remote models are inaccessible so as to not block a generate which may already be available in the file system\r\n2. use a temporary file during pull operations to separate a partial file from a completed file. this enables a better existence check\r\n\r\nthis pr also refactors both the server and client to better share code between similar operations. of note is the change to the pull operation where instead of taking a channel, `saveModel` takes a funcptr to mirror generate and enabling common stream producer code",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "error checking new model",
            "url": "https://github.com/ollama/ollama/pull/71",
            "state": "MERGED",
            "createdAt": "2023-07-12T00:09:31Z",
            "mergedAt": "2023-07-12T16:20:33Z",
            "closedAt": "2023-07-12T16:20:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "check nil to prevent later nil pointer dereferences",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix eof error in generate",
            "url": "https://github.com/ollama/ollama/pull/73",
            "state": "MERGED",
            "createdAt": "2023-07-12T16:36:33Z",
            "mergedAt": "2023-07-12T18:09:23Z",
            "closedAt": "2023-07-12T18:09:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 20,
            "body": "maybe related to #72 ",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Timings",
            "url": "https://github.com/ollama/ollama/pull/74",
            "state": "MERGED",
            "createdAt": "2023-07-13T01:20:54Z",
            "mergedAt": "2023-07-13T17:17:14Z",
            "closedAt": "2023-07-13T17:17:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 117,
            "deletions": 32,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix pull race",
            "url": "https://github.com/ollama/ollama/pull/76",
            "state": "MERGED",
            "createdAt": "2023-07-13T02:07:48Z",
            "mergedAt": "2023-07-13T02:21:13Z",
            "closedAt": "2023-07-13T02:21:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 13,
            "body": "Fixes #75 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "continue conversation",
            "url": "https://github.com/ollama/ollama/pull/77",
            "state": "MERGED",
            "createdAt": "2023-07-13T18:30:47Z",
            "mergedAt": "2023-07-14T21:57:42Z",
            "closedAt": "2023-07-14T21:57:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 11
            },
            "additions": 47,
            "deletions": 10,
            "body": "feed responses back into the llm",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "basic distribution w/ push/pull",
            "url": "https://github.com/ollama/ollama/pull/78",
            "state": "MERGED",
            "createdAt": "2023-07-14T00:19:28Z",
            "mergedAt": "2023-07-17T00:02:22Z",
            "closedAt": "2023-07-17T00:02:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 1154,
            "deletions": 214,
            "body": "First stab at getting distribution to work. This includes a replacement endpoint for \"pull\", and a new endpoint for \"push\". There is also a new \"create\" command which takes a \"Modelfile\" and parses it to create a new image.\r\n\r\nThere are a few things still missing right now:\r\n\r\n* the \"push\" and \"pull\" commands don't yet exist (only the API endpoints).\r\n* there is status being displayed for both push and pull, but it's not very granular yet (distribution supports the Range header, so we'll need to use that)\r\n* there is currently no way to list each of the models. You'll need to look in `~/.ollama/models/manifests` to see a list of images that you have.\r\n* when creating images, the parser is very rudimentary and really only works w/ happy path.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ollama app welcome screen for first time run",
            "url": "https://github.com/ollama/ollama/pull/80",
            "state": "MERGED",
            "createdAt": "2023-07-14T17:56:08Z",
            "mergedAt": "2023-07-14T23:34:25Z",
            "closedAt": "2023-07-14T23:34:25Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 8
            },
            "additions": 2576,
            "deletions": 343,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix race",
            "url": "https://github.com/ollama/ollama/pull/81",
            "state": "MERGED",
            "createdAt": "2023-07-14T21:36:02Z",
            "mergedAt": "2023-07-14T22:12:01Z",
            "closedAt": "2023-07-14T22:12:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 26,
            "deletions": 31,
            "body": "block on write which only returns when the channel is closed. this is contrary to the previous arrangement where the handler may return but the stream hasn't finished writing. it can lead to the client receiving unexpected responses (since the request has been handled) or worst case a nil-pointer dereference as the stream tries to flush a nil writer",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "windows build",
            "url": "https://github.com/ollama/ollama/pull/82",
            "state": "MERGED",
            "createdAt": "2023-07-15T00:31:56Z",
            "mergedAt": "2023-07-15T03:11:55Z",
            "closedAt": "2023-07-15T03:11:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 15,
            "deletions": 15,
            "body": "make some minor changes so it builds on windows.\r\n\r\nTODO:\r\nmoving the `.*.part` to the full file isn't working correctly but #78 will change how that works so temporary workaround is to copy the `.*.part` to the final name",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix multibyte responses",
            "url": "https://github.com/ollama/ollama/pull/83",
            "state": "MERGED",
            "createdAt": "2023-07-15T01:30:51Z",
            "mergedAt": "2023-07-15T03:12:12Z",
            "closedAt": "2023-07-15T03:12:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "welcome screen improvements",
            "url": "https://github.com/ollama/ollama/pull/86",
            "state": "MERGED",
            "createdAt": "2023-07-17T17:30:06Z",
            "mergedAt": "2023-07-17T17:44:53Z",
            "closedAt": "2023-07-17T17:44:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 80,
            "deletions": 67,
            "body": "- make window draggable\r\n- improve copy command experience on the finish page",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix file paths for windows",
            "url": "https://github.com/ollama/ollama/pull/87",
            "state": "MERGED",
            "createdAt": "2023-07-17T17:51:38Z",
            "mergedAt": "2023-07-17T18:21:25Z",
            "closedAt": "2023-07-17T18:21:25Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 42,
            "deletions": 55,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "modelfile params",
            "url": "https://github.com/ollama/ollama/pull/88",
            "state": "MERGED",
            "createdAt": "2023-07-17T19:13:45Z",
            "mergedAt": "2023-07-17T21:18:57Z",
            "closedAt": "2023-07-17T21:18:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 88,
            "deletions": 70,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix stream errors",
            "url": "https://github.com/ollama/ollama/pull/91",
            "state": "MERGED",
            "createdAt": "2023-07-17T20:41:20Z",
            "mergedAt": "2023-07-20T19:22:00Z",
            "closedAt": "2023-07-20T19:22:00Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 13,
            "deletions": 15,
            "body": "once the stream is created, it's too late to update response headers (i.e. status code). any and all errors must be returned by the stream",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Create model spinner",
            "url": "https://github.com/ollama/ollama/pull/92",
            "state": "MERGED",
            "createdAt": "2023-07-17T21:16:15Z",
            "mergedAt": "2023-07-18T18:15:45Z",
            "closedAt": "2023-07-18T18:15:45Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 59,
            "deletions": 33,
            "body": "add a spinner to create model outputs",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "separate prompt into template and system",
            "url": "https://github.com/ollama/ollama/pull/93",
            "state": "MERGED",
            "createdAt": "2023-07-17T23:45:30Z",
            "mergedAt": "2023-07-20T06:25:34Z",
            "closedAt": "2023-07-20T06:25:34Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 161,
            "deletions": 127,
            "body": "this also updates the scanner with a multiline split function",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "stop condition",
            "url": "https://github.com/ollama/ollama/pull/94",
            "state": "CLOSED",
            "createdAt": "2023-07-18T00:09:42Z",
            "mergedAt": null,
            "closedAt": "2023-07-28T15:27:05Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 22,
            "deletions": 10,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Some simple modelfile examples",
            "url": "https://github.com/ollama/ollama/pull/95",
            "state": "MERGED",
            "createdAt": "2023-07-18T00:18:18Z",
            "mergedAt": "2023-07-18T12:32:39Z",
            "closedAt": "2023-07-18T12:32:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 44,
            "deletions": 3,
            "body": "Added 3 modelfiles as examples. Also update development.doc, but wondering if i should also get rid of dev altogether.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add modelpaths",
            "url": "https://github.com/ollama/ollama/pull/96",
            "state": "MERGED",
            "createdAt": "2023-07-18T00:35:39Z",
            "mergedAt": "2023-07-18T05:44:21Z",
            "closedAt": "2023-07-18T05:44:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 147,
            "deletions": 80,
            "body": "This change adds ModelPath{} which takes care of figuring out the various URL and file paths to a given model.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add new list command",
            "url": "https://github.com/ollama/ollama/pull/97",
            "state": "MERGED",
            "createdAt": "2023-07-18T05:43:08Z",
            "mergedAt": "2023-07-18T16:09:45Z",
            "closedAt": "2023-07-18T16:09:45Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 10
            },
            "additions": 450,
            "deletions": 11,
            "body": "This changes lets you list each of the models which you have pulled locally.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "First stab at a modelfile doc",
            "url": "https://github.com/ollama/ollama/pull/98",
            "state": "MERGED",
            "createdAt": "2023-07-18T15:22:55Z",
            "mergedAt": "2023-07-18T16:16:01Z",
            "closedAt": "2023-07-18T16:16:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 80,
            "deletions": 0,
            "body": "Created a doc that documents the modelfile as it is today. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix mkdir blob path",
            "url": "https://github.com/ollama/ollama/pull/99",
            "state": "MERGED",
            "createdAt": "2023-07-18T18:24:47Z",
            "mergedAt": "2023-07-18T18:29:05Z",
            "closedAt": "2023-07-18T18:29:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 16,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "skip files in the list if we can't get the correct model path",
            "url": "https://github.com/ollama/ollama/pull/100",
            "state": "MERGED",
            "createdAt": "2023-07-18T19:38:43Z",
            "mergedAt": "2023-07-18T19:39:08Z",
            "closedAt": "2023-07-18T19:39:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Session",
            "url": "https://github.com/ollama/ollama/pull/102",
            "state": "MERGED",
            "createdAt": "2023-07-18T19:46:08Z",
            "mergedAt": "2023-07-27T23:46:29Z",
            "closedAt": "2023-07-27T23:46:29Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 5
            },
            "additions": 343,
            "deletions": 212,
            "body": "an active model is kept in memory until another session is requested or the session has expired, freeing any memory associated with the model.\r\n\r\nThis adds a `SessionDuration` field to the generate request to customize the session window (default 5m) and a `SessionExpiresAt` field to the generate response informing users of when the session will expire. \r\n\r\n* A session duration value of `-1` disables session expiration. \r\n* A session duration value of `0` disables model caching, i.e. models will be garbage collected as soon as generation is complete\r\n\r\nresolves #60 \r\nresolves #108 \r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "website content and design update",
            "url": "https://github.com/ollama/ollama/pull/103",
            "state": "MERGED",
            "createdAt": "2023-07-18T19:58:33Z",
            "mergedAt": "2023-07-18T20:18:04Z",
            "closedAt": "2023-07-18T20:18:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 64,
            "deletions": 44,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use readline",
            "url": "https://github.com/ollama/ollama/pull/104",
            "state": "MERGED",
            "createdAt": "2023-07-18T21:32:24Z",
            "mergedAt": "2023-07-19T20:36:24Z",
            "closedAt": "2023-07-19T20:36:24Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 111,
            "deletions": 10,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "attempt two for skipping files in the file walk",
            "url": "https://github.com/ollama/ollama/pull/105",
            "state": "MERGED",
            "createdAt": "2023-07-18T22:36:18Z",
            "mergedAt": "2023-07-18T22:37:01Z",
            "closedAt": "2023-07-18T22:37:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README typo fix",
            "url": "https://github.com/ollama/ollama/pull/106",
            "state": "MERGED",
            "createdAt": "2023-07-18T23:00:59Z",
            "mergedAt": "2023-07-18T23:24:58Z",
            "closedAt": "2023-07-18T23:24:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "- Simple typo fix in README.md\r\n- `13` changed to `13B` (the `B` was missing) on Nous-Hermes model in models table\r\n- Also edited `hous-hermes` to `nous-hermes` (should be n instead of h)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix memory leak in create",
            "url": "https://github.com/ollama/ollama/pull/109",
            "state": "MERGED",
            "createdAt": "2023-07-19T00:15:37Z",
            "mergedAt": "2023-07-19T00:25:19Z",
            "closedAt": "2023-07-19T00:25:19Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 27,
            "deletions": 30,
            "body": "do not buffer the model into memory. instead use `io.Copy` to pass the contents directly to the destination writer, whether that is the hasher or file writer\r\n\r\nthis also significantly improve the hashing performance",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix pull 0 bytes on completed layer",
            "url": "https://github.com/ollama/ollama/pull/110",
            "state": "MERGED",
            "createdAt": "2023-07-19T01:53:18Z",
            "mergedAt": "2023-07-19T02:38:59Z",
            "closedAt": "2023-07-19T02:38:59Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 5
            },
            "additions": 82,
            "deletions": 65,
            "body": "This PR fixes the bug where when the progress bar displays 0B for a layer when the layer already exists:\r\n\r\n```\r\n$ ollama pull llama2\r\npulling manifest\r\npulling 8daa9615cce30c25...   0% |                                                                                                                                                  | ( 0 B/3.5 GB) [0s:0s]\r\npulling c929c04af928be41...   0% |                                                                                                                                                  | ( 0 B/3.5 GB) [0s:0s]\r\npulling cf39c1a5c36937e4... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (3.5/3.5 GB, 53 TB/s)\r\nwriting manifest\r\nsuccess\r\n```\r\n\r\n```\r\n$ ollama pull llama2\r\npulling manifest\r\npulling 8daa9615cce30c25... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (3.5/3.5 GB, 16 TB/s)\r\npulling c929c04af928be41... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (547/547 B, 12 MB/s)\r\npulling cf39c1a5c36937e4... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (225/225 B, 5.0 MB/s)\r\nwriting manifest\r\nsuccess\r\n```\r\n\r\nNow each layer also correctly reports the layer's size rather than the total bundle size.\r\n\r\nRefactor `Pull/PushProgress` into `ProgressResponse` since they share the exact same attributes and remove `Percent` since it's not being used and the caller can easily compute it for themselves",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "resolve modelfile before passing to server",
            "url": "https://github.com/ollama/ollama/pull/112",
            "state": "MERGED",
            "createdAt": "2023-07-19T02:34:32Z",
            "mergedAt": "2023-07-19T02:36:25Z",
            "closedAt": "2023-07-19T02:36:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add wizard vicuna uncensored model link",
            "url": "https://github.com/ollama/ollama/pull/115",
            "state": "MERGED",
            "createdAt": "2023-07-19T05:19:17Z",
            "mergedAt": "2023-07-19T05:58:07Z",
            "closedAt": "2023-07-19T05:58:07Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add license layers to the parser",
            "url": "https://github.com/ollama/ollama/pull/116",
            "state": "MERGED",
            "createdAt": "2023-07-19T05:45:21Z",
            "mergedAt": "2023-07-19T05:49:38Z",
            "closedAt": "2023-07-19T05:49:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Updated modelfile doc to include license",
            "url": "https://github.com/ollama/ollama/pull/125",
            "state": "MERGED",
            "createdAt": "2023-07-19T14:17:05Z",
            "mergedAt": "2023-07-19T15:57:07Z",
            "closedAt": "2023-07-19T15:57:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 5,
            "body": "and attributed midjourneyprompt",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add llama2:13b model to the readme",
            "url": "https://github.com/ollama/ollama/pull/126",
            "state": "MERGED",
            "createdAt": "2023-07-19T15:16:40Z",
            "mergedAt": "2023-07-19T15:21:29Z",
            "closedAt": "2023-07-19T15:21:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update modelfile.md",
            "url": "https://github.com/ollama/ollama/pull/128",
            "state": "MERGED",
            "createdAt": "2023-07-19T18:38:21Z",
            "mergedAt": "2023-07-19T20:40:39Z",
            "closedAt": "2023-07-19T20:40:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 128,
            "deletions": 54,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "vendor in progress bar and change to bytes instead of bibytes",
            "url": "https://github.com/ollama/ollama/pull/130",
            "state": "MERGED",
            "createdAt": "2023-07-19T22:54:28Z",
            "mergedAt": "2023-07-20T00:24:03Z",
            "closedAt": "2023-07-20T00:24:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 1325,
            "deletions": 14,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp to e782c9e735f93ab4767ffc37462c523b73a17ddc",
            "url": "https://github.com/ollama/ollama/pull/131",
            "state": "MERGED",
            "createdAt": "2023-07-19T23:50:32Z",
            "mergedAt": "2023-07-20T19:14:10Z",
            "closedAt": "2023-07-20T19:14:10Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 13
            },
            "additions": 1769,
            "deletions": 650,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update images.go",
            "url": "https://github.com/ollama/ollama/pull/134",
            "state": "MERGED",
            "createdAt": "2023-07-20T06:34:19Z",
            "mergedAt": "2023-07-20T06:46:02Z",
            "closedAt": "2023-07-20T06:46:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Delete models.json",
            "url": "https://github.com/ollama/ollama/pull/136",
            "state": "MERGED",
            "createdAt": "2023-07-20T14:33:15Z",
            "mergedAt": "2023-07-20T14:40:46Z",
            "closedAt": "2023-07-20T14:40:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 38,
            "body": "This is no longer used.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update icon",
            "url": "https://github.com/ollama/ollama/pull/139",
            "state": "MERGED",
            "createdAt": "2023-07-20T15:31:55Z",
            "mergedAt": "2023-07-20T15:55:20Z",
            "closedAt": "2023-07-20T15:55:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove unused code",
            "url": "https://github.com/ollama/ollama/pull/144",
            "state": "MERGED",
            "createdAt": "2023-07-20T18:18:26Z",
            "mergedAt": "2023-07-20T18:57:30Z",
            "closedAt": "2023-07-20T18:57:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 19,
            "body": "cleaning up some unused code I noticed",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "verify blob digest",
            "url": "https://github.com/ollama/ollama/pull/145",
            "state": "MERGED",
            "createdAt": "2023-07-20T18:54:21Z",
            "mergedAt": "2023-07-20T19:14:21Z",
            "closedAt": "2023-07-20T19:14:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 27,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "windows: fix model pulling",
            "url": "https://github.com/ollama/ollama/pull/146",
            "state": "MERGED",
            "createdAt": "2023-07-20T18:54:29Z",
            "mergedAt": "2023-07-20T20:41:54Z",
            "closedAt": "2023-07-20T20:41:54Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 0,
            "body": "There are two issues preventing pull from working as expected in Windows.\r\n\r\n1. Windows dislikes `os.Rename` when the file is still open. One approach is to close the file before calling rename. The approach taken in this PR is to call `os.Symlink` instead\r\n2. Windows errors when file paths contain `:` so replace the `:` in the digest name with `-`, e.g. `sha256:0123456789abcdef...` with `sha256-0123456789abcdef...`. This is done _only_ for the blob file path. Non-file path instances of this string are unchanged",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Improve CLI error display",
            "url": "https://github.com/ollama/ollama/pull/147",
            "state": "MERGED",
            "createdAt": "2023-07-20T18:55:09Z",
            "mergedAt": "2023-07-21T14:10:20Z",
            "closedAt": "2023-07-21T14:10:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 26,
            "deletions": 15,
            "body": "Fixing a couple of error display issues I ran into while running ollama.\r\n\r\n1. Errors returned using gin in the form `{\"error\": \"some message\"}` were not being displayed in the CLI. Update the format of the stream error object so that it will parse both these gin errors and custom errors.\r\n2. Do not return a 500 error when running `ollama list` before pulling any images, return an empty set of images instead.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add llama.cpp mpi, opencl files",
            "url": "https://github.com/ollama/ollama/pull/148",
            "state": "MERGED",
            "createdAt": "2023-07-20T21:18:07Z",
            "mergedAt": "2023-07-20T21:26:46Z",
            "closedAt": "2023-07-20T21:26:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 2283,
            "deletions": 13,
            "body": "the full source from llama.cpp is now included with additional build constraints on components not readily compatible with the current build",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add rm command for models",
            "url": "https://github.com/ollama/ollama/pull/151",
            "state": "MERGED",
            "createdAt": "2023-07-20T22:19:23Z",
            "mergedAt": "2023-07-20T23:09:23Z",
            "closedAt": "2023-07-20T23:09:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 166,
            "deletions": 25,
            "body": "This change adds an \"rm\" command so that you can remove models that you don't want anymore. The handler determines if other manifests require a given layer and will save anything still required. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add ls alias",
            "url": "https://github.com/ollama/ollama/pull/152",
            "state": "MERGED",
            "createdAt": "2023-07-20T22:27:09Z",
            "mergedAt": "2023-07-20T22:28:28Z",
            "closedAt": "2023-07-20T22:28:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow pushing/pulling to insecure registries",
            "url": "https://github.com/ollama/ollama/pull/157",
            "state": "MERGED",
            "createdAt": "2023-07-21T06:31:48Z",
            "mergedAt": "2023-07-21T22:42:19Z",
            "closedAt": "2023-07-21T22:42:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 86,
            "deletions": 39,
            "body": "This change adds the `--insecure` flag to the push/pull commands so that you can push and pull models from local registries.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "restart server more gracefully",
            "url": "https://github.com/ollama/ollama/pull/164",
            "state": "MERGED",
            "createdAt": "2023-07-21T16:49:58Z",
            "mergedAt": "2023-07-22T22:19:22Z",
            "closedAt": "2023-07-22T22:19:22Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 3
            },
            "additions": 15,
            "deletions": 14,
            "body": "fix for https://github.com/jmorganca/ollama/issues/154",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Note that CGO must be enabled in dev docs",
            "url": "https://github.com/ollama/ollama/pull/166",
            "state": "MERGED",
            "createdAt": "2023-07-21T20:36:39Z",
            "mergedAt": "2023-07-21T20:48:10Z",
            "closedAt": "2023-07-21T20:48:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "partial decode ggml bin for more info",
            "url": "https://github.com/ollama/ollama/pull/167",
            "state": "MERGED",
            "createdAt": "2023-07-21T22:10:33Z",
            "mergedAt": "2023-08-11T00:22:40Z",
            "closedAt": "2023-08-11T00:22:40Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 26
            },
            "additions": 336,
            "deletions": 69,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "get the proper path for blobs to delete",
            "url": "https://github.com/ollama/ollama/pull/168",
            "state": "MERGED",
            "createdAt": "2023-07-22T00:14:23Z",
            "mergedAt": "2023-07-22T00:30:40Z",
            "closedAt": "2023-07-22T00:30:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix extended tag names",
            "url": "https://github.com/ollama/ollama/pull/171",
            "state": "MERGED",
            "createdAt": "2023-07-22T02:08:46Z",
            "mergedAt": "2023-07-22T03:27:25Z",
            "closedAt": "2023-07-22T03:27:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix vars.First",
            "url": "https://github.com/ollama/ollama/pull/172",
            "state": "MERGED",
            "createdAt": "2023-07-22T03:45:56Z",
            "mergedAt": "2023-07-22T03:55:06Z",
            "closedAt": "2023-07-22T03:55:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "change error handler behavior and fix error when a model isn't found",
            "url": "https://github.com/ollama/ollama/pull/173",
            "state": "MERGED",
            "createdAt": "2023-07-22T05:46:16Z",
            "mergedAt": "2023-07-22T06:02:12Z",
            "closedAt": "2023-07-22T06:02:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 17,
            "deletions": 36,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allocate a large enough tokens slice",
            "url": "https://github.com/ollama/ollama/pull/174",
            "state": "MERGED",
            "createdAt": "2023-07-22T06:05:37Z",
            "mergedAt": "2023-07-24T15:22:51Z",
            "closedAt": "2023-07-24T15:22:51Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "cherry picked from #102 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update .gitignore",
            "url": "https://github.com/ollama/ollama/pull/175",
            "state": "MERGED",
            "createdAt": "2023-07-22T14:03:26Z",
            "mergedAt": "2023-07-22T16:40:38Z",
            "closedAt": "2023-07-22T16:40:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update modelfile.md",
            "url": "https://github.com/ollama/ollama/pull/177",
            "state": "MERGED",
            "createdAt": "2023-07-22T14:42:44Z",
            "mergedAt": "2023-07-22T15:19:30Z",
            "closedAt": "2023-07-22T15:19:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "fix markdown.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use gin-contrib/cors middleware",
            "url": "https://github.com/ollama/ollama/pull/178",
            "state": "MERGED",
            "createdAt": "2023-07-22T16:04:28Z",
            "mergedAt": "2023-07-22T16:40:01Z",
            "closedAt": "2023-07-22T16:40:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 61,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "change push to chunked uploads from monolithic",
            "url": "https://github.com/ollama/ollama/pull/179",
            "state": "MERGED",
            "createdAt": "2023-07-22T23:16:24Z",
            "mergedAt": "2023-07-23T00:31:26Z",
            "closedAt": "2023-07-23T00:31:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 94,
            "deletions": 52,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Improve command parsing and multiline string handling",
            "url": "https://github.com/ollama/ollama/pull/189",
            "state": "MERGED",
            "createdAt": "2023-07-24T12:50:00Z",
            "mergedAt": "2023-07-25T18:28:11Z",
            "closedAt": "2023-07-25T18:28:11Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 7,
            "body": "This PR enhances the existing parser package. Main improvements include better error handling, optimized string-to-byte conversions, and efficient handling of multiline strings.\r\n\r\nDetailed changes:\r\n- Define a `multilineString` constant for repeated values to avoid duplication.\r\n- Modify the error handling in the `Parse` function to return an error for unknown commands.\r\n- Replace `bytes.ToUpper` and `bytes.ToLower` with `strings.ToUpper` and `strings.ToLower` for faster string conversions.\r\n- Optimize removal of `\"\"\"` from multiline strings by using `bytes.Index` and `bytes.LastIndex` instead of `bytes.Replace`.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add copy command",
            "url": "https://github.com/ollama/ollama/pull/191",
            "state": "MERGED",
            "createdAt": "2023-07-24T14:56:37Z",
            "mergedAt": "2023-07-24T15:27:28Z",
            "closedAt": "2023-07-24T15:27:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 77,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update development.md",
            "url": "https://github.com/ollama/ollama/pull/192",
            "state": "MERGED",
            "createdAt": "2023-07-24T16:44:41Z",
            "mergedAt": "2023-07-24T23:13:22Z",
            "closedAt": "2023-07-24T23:13:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "truncate file size on resume",
            "url": "https://github.com/ollama/ollama/pull/195",
            "state": "MERGED",
            "createdAt": "2023-07-24T18:37:18Z",
            "mergedAt": "2023-07-24T19:58:32Z",
            "closedAt": "2023-07-24T19:58:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 1,
            "body": "when resuming a download of a model truncate the file size to match the expected trunk size, hopefully this mitigates #170",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add devops-engineer example",
            "url": "https://github.com/ollama/ollama/pull/196",
            "state": "MERGED",
            "createdAt": "2023-07-24T18:45:22Z",
            "mergedAt": "2023-07-24T21:10:22Z",
            "closedAt": "2023-07-24T21:10:22Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 0,
            "body": "Another example for a DevOps engineer\r\n\r\n<img width=\"1933\" alt=\"Screenshot 2023-07-24 at 2 45 54 PM\" src=\"https://github.com/jmorganca/ollama/assets/216867/cdf018ce-2c9c-4e0e-a53b-3d708cafcffe\">\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "remove file on digest mismatch",
            "url": "https://github.com/ollama/ollama/pull/197",
            "state": "MERGED",
            "createdAt": "2023-07-24T18:54:33Z",
            "mergedAt": "2023-07-24T19:59:12Z",
            "closedAt": "2023-07-24T19:59:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 2,
            "body": "ideally this never happens (the download resume should prevent this), but if there is a digest mismatch the specific blob should be removed rather than the user manually removing it\r\n\r\nrelated to #170 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "make response errors unique for error trace",
            "url": "https://github.com/ollama/ollama/pull/198",
            "state": "MERGED",
            "createdAt": "2023-07-24T19:04:37Z",
            "mergedAt": "2023-07-24T19:21:18Z",
            "closedAt": "2023-07-24T19:21:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "better error message when model not found on pull",
            "url": "https://github.com/ollama/ollama/pull/202",
            "state": "MERGED",
            "createdAt": "2023-07-24T21:49:03Z",
            "mergedAt": "2023-07-25T14:30:48Z",
            "closedAt": "2023-07-25T14:30:48Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 2,
            "body": "```\r\nollama run orca-dne\r\npulling manifest\r\nError: pull model manifest: model not found\r\n```\r\n\r\nresolves #180",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "pull model on make if not present locally",
            "url": "https://github.com/ollama/ollama/pull/203",
            "state": "MERGED",
            "createdAt": "2023-07-24T22:08:01Z",
            "mergedAt": "2023-07-25T20:53:01Z",
            "closedAt": "2023-07-25T20:53:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 73,
            "deletions": 41,
            "body": "resolves #129",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "enable accelerate",
            "url": "https://github.com/ollama/ollama/pull/205",
            "state": "MERGED",
            "createdAt": "2023-07-25T00:15:20Z",
            "mergedAt": "2023-07-25T03:06:05Z",
            "closedAt": "2023-07-25T03:06:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "missed this define",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "enable k quants",
            "url": "https://github.com/ollama/ollama/pull/209",
            "state": "MERGED",
            "createdAt": "2023-07-25T15:40:15Z",
            "mergedAt": "2023-07-25T18:53:29Z",
            "closedAt": "2023-07-25T18:53:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/211",
            "state": "MERGED",
            "createdAt": "2023-07-25T17:51:16Z",
            "mergedAt": "2023-07-27T23:57:03Z",
            "closedAt": "2023-07-27T23:57:03Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 18
            },
            "additions": 2607,
            "deletions": 1564,
            "body": "update to eb542d39324574a6778fad9ba9e34ba7a14a82a3",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix multiline string",
            "url": "https://github.com/ollama/ollama/pull/212",
            "state": "MERGED",
            "createdAt": "2023-07-25T18:52:00Z",
            "mergedAt": "2023-07-25T18:53:51Z",
            "closedAt": "2023-07-25T18:53:51Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 4,
            "body": "the data needs to remove the multiline quotes but include the command:\r\n\r\ne.g.\r\n\r\n```\r\nTEMPLATE \"\"\"\r\nmy template values\r\n\"\"\"\r\n```\r\n\r\nshould be\r\n\r\n```\r\nTEMPLATE\r\nmy template values\r\n```\r\n\r\nafter scanning",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow for concurrent pulls of the same files",
            "url": "https://github.com/ollama/ollama/pull/214",
            "state": "MERGED",
            "createdAt": "2023-07-25T21:10:24Z",
            "mergedAt": "2023-08-09T15:35:24Z",
            "closedAt": "2023-08-09T15:35:24Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 3
            },
            "additions": 229,
            "deletions": 113,
            "body": "resolves #200 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "embed ggml-metal.metal",
            "url": "https://github.com/ollama/ollama/pull/221",
            "state": "MERGED",
            "createdAt": "2023-07-26T18:52:15Z",
            "mergedAt": "2023-07-28T00:24:42Z",
            "closedAt": "2023-07-28T00:24:42Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 57,
            "deletions": 1,
            "body": "`go:embed ggml-metal.metal` and write it out to the right location on `init()` so llama.cpp can use it.\r\n\r\nwith this change, `ollama` is serveable using `go run . serve` or `go install . && ~/go/bin/ollama serve`\r\n\r\nresolves #48",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "allow removing partial files",
            "url": "https://github.com/ollama/ollama/pull/222",
            "state": "CLOSED",
            "createdAt": "2023-07-26T19:31:19Z",
            "mergedAt": null,
            "closedAt": "2023-07-31T19:46:51Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 72,
            "deletions": 25,
            "body": "resolves #213",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "show system/template/license layers from cmd prompt",
            "url": "https://github.com/ollama/ollama/pull/223",
            "state": "MERGED",
            "createdAt": "2023-07-27T02:07:07Z",
            "mergedAt": "2023-07-27T23:58:40Z",
            "closedAt": "2023-07-27T23:58:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 64,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add stop conditions",
            "url": "https://github.com/ollama/ollama/pull/225",
            "state": "MERGED",
            "createdAt": "2023-07-27T18:28:50Z",
            "mergedAt": "2023-07-28T00:20:56Z",
            "closedAt": "2023-07-28T00:20:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 39,
            "deletions": 13,
            "body": "resolves #140 \r\nresolves #217 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor scan multiline for reuse",
            "url": "https://github.com/ollama/ollama/pull/226",
            "state": "MERGED",
            "createdAt": "2023-07-27T18:32:03Z",
            "mergedAt": "2023-07-27T18:45:41Z",
            "closedAt": "2023-07-27T18:45:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 28,
            "deletions": 7,
            "body": "It's not obvious values are ingested verbatim when not using multiline so `\"` are included in the template. Instead, ingest the value inside the quotes",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update model file docs",
            "url": "https://github.com/ollama/ollama/pull/230",
            "state": "MERGED",
            "createdAt": "2023-07-27T19:16:32Z",
            "mergedAt": "2023-07-28T14:33:53Z",
            "closedAt": "2023-07-28T14:33:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 82,
            "deletions": 19,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update discord invite link",
            "url": "https://github.com/ollama/ollama/pull/231",
            "state": "MERGED",
            "createdAt": "2023-07-27T19:43:21Z",
            "mergedAt": "2023-07-27T19:43:53Z",
            "closedAt": "2023-07-27T19:43:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Update discord invite link",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Allow specifying stop conditions in Modelfile",
            "url": "https://github.com/ollama/ollama/pull/232",
            "state": "MERGED",
            "createdAt": "2023-07-27T21:13:37Z",
            "mergedAt": "2023-07-28T16:31:08Z",
            "closedAt": "2023-07-28T16:31:08Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 31,
            "deletions": 28,
            "body": "This is useful for Modelfiles which define a format. Multi-value paramters are set by listing them in quotes.\r\n\r\nExample Modelfile:\r\n```\r\nFROM llama2\r\nPARAMETER temperature 1\r\nPARAMETER stop \"AI Cat:\" \"Dog:\"\r\n\r\nTEMPLATE \"\"\"\r\n{{- if .First }}\r\n<<SYS>>\r\n{{ .System }}\r\n<</SYS>>\r\n\r\nDog: woof woof woof\r\n\r\nAI Cat: meow meow meeeeow\r\n\r\nDog: bark woof\r\n\r\nAI Cat: mew meow\r\n{{- end }}\r\n\r\nDog: {{ .Prompt }}\r\n\r\nAI Cat:\r\n\"\"\"\r\n\r\nSYSTEM \"\"\"\r\nAI Cat is a highly advanced robot cat that can only respond with meows. She has a comprehensive understanding of cat psychology, but without the human biases that may interfere with therapy.\r\n\"\"\"\r\n```\r\n\r\nTODO:\r\n- [x] documentation ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use max scan token size to hold large objects",
            "url": "https://github.com/ollama/ollama/pull/234",
            "state": "MERGED",
            "createdAt": "2023-07-28T18:44:44Z",
            "mergedAt": "2023-07-28T19:03:51Z",
            "closedAt": "2023-07-28T19:03:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "The internal buffer used by scanner is too small to hold Meta's license so allocate the maximum size set in bufio. It can be potentially higher but it's not necessary right now",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove io/ioutil import",
            "url": "https://github.com/ollama/ollama/pull/235",
            "state": "MERGED",
            "createdAt": "2023-07-28T19:07:44Z",
            "mergedAt": "2023-07-28T19:19:06Z",
            "closedAt": "2023-07-28T19:19:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 4,
            "body": "ioutil is deprecated",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "check os.Walk err",
            "url": "https://github.com/ollama/ollama/pull/236",
            "state": "MERGED",
            "createdAt": "2023-07-28T19:15:46Z",
            "mergedAt": "2023-07-28T21:14:21Z",
            "closedAt": "2023-07-28T21:14:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow multiline text using three double-quotes",
            "url": "https://github.com/ollama/ollama/pull/239",
            "state": "MERGED",
            "createdAt": "2023-07-29T19:31:33Z",
            "mergedAt": "2023-07-29T20:35:23Z",
            "closedAt": "2023-07-29T20:35:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 19,
            "deletions": 0,
            "body": "This change will allow you to start a multi-line string using three double quotes. It looks something like:\r\n```\r\n>>> \"\"\"What\r\n... do you think\r\n... about multi-line\r\n... strings?\r\n... \"\"\"\r\n```\r\nTo use a multi-line string, you have to begin the sentence with the three double quotes. To end it, you just have to put the three double quotes at the end of a line (or on a line by itself).",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "tell users to check the server error logs",
            "url": "https://github.com/ollama/ollama/pull/244",
            "state": "MERGED",
            "createdAt": "2023-07-31T15:50:37Z",
            "mergedAt": "2023-08-02T21:08:11Z",
            "closedAt": "2023-08-02T21:08:11Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 1,
            "body": "when possible tell users to check the error logs to get more info on why their command failed",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow specifying zero values in modelfile",
            "url": "https://github.com/ollama/ollama/pull/245",
            "state": "MERGED",
            "createdAt": "2023-07-31T19:11:10Z",
            "mergedAt": "2023-08-02T21:07:53Z",
            "closedAt": "2023-08-02T21:07:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 101,
            "deletions": 25,
            "body": "Previously specifying a zero value in a Modelfile configuration parameters would cause specified value to be overwritten by the default value.\r\n\r\nEx:\r\n```\r\nFROM llama2\r\nPARAMETER num_gpu 0\r\n```\r\nThis gets overridden at runtime to 1, since the empty (zero) fields are omitted from the Options struct JSON.\r\n\r\nThis change fixes this behavior by setting default parameters at model create time.\r\n```\r\n$ ollama create ai-cat -f ./Modelfile\r\n# defaults are set now and written to file\r\n```\r\n\r\nBackwards compatibility for Modelfiles created previously is supported through loading the specified parameters into a DefaultOptions struct then merging them.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "check server is running before running command",
            "url": "https://github.com/ollama/ollama/pull/246",
            "state": "MERGED",
            "createdAt": "2023-07-31T20:27:02Z",
            "mergedAt": "2023-08-02T14:51:24Z",
            "closedAt": "2023-08-02T14:51:24Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 91,
            "deletions": 25,
            "body": "Check if the server is running rather than returning confusing \"connection refused\" errors. \r\n\r\nIf the mac app is available start it, otherwise tell the user to run `ollama serve`.\r\n\r\nresolves #47 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "log prediction failures",
            "url": "https://github.com/ollama/ollama/pull/247",
            "state": "MERGED",
            "createdAt": "2023-07-31T20:47:07Z",
            "mergedAt": "2023-07-31T21:39:20Z",
            "closedAt": "2023-07-31T21:39:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "this will help track down #241 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add \"Awesome projects built with Ollama\" section to README, including Continue",
            "url": "https://github.com/ollama/ollama/pull/249",
            "state": "MERGED",
            "createdAt": "2023-07-31T21:01:22Z",
            "mergedAt": "2023-08-01T15:07:50Z",
            "closedAt": "2023-08-01T15:07:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "Format and text are up for debate, but here's a description of Continue",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fixes",
            "url": "https://github.com/ollama/ollama/pull/250",
            "state": "MERGED",
            "createdAt": "2023-08-01T04:35:17Z",
            "mergedAt": "2023-08-01T04:47:42Z",
            "closedAt": "2023-08-01T04:47:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 18,
            "deletions": 6,
            "body": "- Fix multiple LICENSE in Modelfile\r\n- Check error from `filepath.Walk`\r\n- fix build on Linux\r\n- remove unnecessary `fmt.Sprintf`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add model update to README.md",
            "url": "https://github.com/ollama/ollama/pull/252",
            "state": "MERGED",
            "createdAt": "2023-08-01T16:14:59Z",
            "mergedAt": "2023-08-01T19:06:33Z",
            "closedAt": "2023-08-01T19:06:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use a pipe to push to registry with progress",
            "url": "https://github.com/ollama/ollama/pull/253",
            "state": "MERGED",
            "createdAt": "2023-08-01T19:17:50Z",
            "mergedAt": "2023-08-03T19:11:23Z",
            "closedAt": "2023-08-03T19:11:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 41,
            "deletions": 54,
            "body": "switch to a monolithic upload instead of a chunk upload through a pipe to report progress",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update llama cpp",
            "url": "https://github.com/ollama/ollama/pull/255",
            "state": "MERGED",
            "createdAt": "2023-08-01T23:22:47Z",
            "mergedAt": "2023-08-02T00:18:33Z",
            "closedAt": "2023-08-02T00:18:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 19
            },
            "additions": 2688,
            "deletions": 573,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "override ggml-metal if the file is different",
            "url": "https://github.com/ollama/ollama/pull/260",
            "state": "MERGED",
            "createdAt": "2023-08-02T19:51:00Z",
            "mergedAt": "2023-08-02T20:01:46Z",
            "closedAt": "2023-08-02T20:01:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 35,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: support OLLAMA_CLIENT_HOST environment variable",
            "url": "https://github.com/ollama/ollama/pull/262",
            "state": "MERGED",
            "createdAt": "2023-08-02T22:05:18Z",
            "mergedAt": "2023-08-16T15:03:48Z",
            "closedAt": "2023-08-16T15:03:48Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 64,
            "deletions": 13,
            "body": "This commit adds support for the OLLAMA_CLIENT_HOST environment variable. This variable can be used to specify the host to which the client should connect. This is useful when the client is running somewhere other than the host where the server is running.\r\n\r\nThe new api.FromEnv function is used to configure clients from the environment. Clients wishing to use the environment variable being consistent with the Ollama CLI can use this new function.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 10
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/265",
            "state": "MERGED",
            "createdAt": "2023-08-03T00:21:46Z",
            "mergedAt": "2023-08-03T02:38:32Z",
            "closedAt": "2023-08-03T02:38:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README.md: Add info about `serve`, logging, and env vars (+ some icons)",
            "url": "https://github.com/ollama/ollama/pull/271",
            "state": "CLOSED",
            "createdAt": "2023-08-03T19:56:15Z",
            "mergedAt": null,
            "closedAt": "2023-10-24T22:17:14Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Decode ggml 2: Use decoded values",
            "url": "https://github.com/ollama/ollama/pull/272",
            "state": "MERGED",
            "createdAt": "2023-08-03T22:48:36Z",
            "mergedAt": "2023-08-11T00:22:48Z",
            "closedAt": "2023-08-11T00:22:48Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 3
            },
            "additions": 36,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Create a sentiments example",
            "url": "https://github.com/ollama/ollama/pull/273",
            "state": "MERGED",
            "createdAt": "2023-08-03T23:39:06Z",
            "mergedAt": "2023-08-31T23:31:59Z",
            "closedAt": "2023-08-31T23:31:59Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 60,
            "deletions": 0,
            "body": "A simple example for sentiments analysis and a writer of lists of 10 tweets",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "configurable rope frequency parameters",
            "url": "https://github.com/ollama/ollama/pull/276",
            "state": "MERGED",
            "createdAt": "2023-08-04T05:34:23Z",
            "mergedAt": "2023-08-07T20:39:38Z",
            "closedAt": "2023-08-07T20:39:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 27,
            "deletions": 21,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update to nous-hermes modelfile",
            "url": "https://github.com/ollama/ollama/pull/284",
            "state": "CLOSED",
            "createdAt": "2023-08-04T15:57:43Z",
            "mergedAt": null,
            "closedAt": "2023-08-08T23:04:49Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 2
            },
            "additions": 8,
            "deletions": 1,
            "body": "- The nous-hermes model will now accept a system prompt from a model that uses nous-hermes.\r\n- also updated the midjourney-prompter to use a better name.\r\n\r\nas per Hugging Face (https://huggingface.co/NousResearch/Nous-Hermes-13b#prompt-format), the prompt template is:\r\n\r\n```\r\nPrompt Format\r\nThe model follows the Alpaca prompt format:\r\n\r\n### Instruction:\r\n\r\n### Response:\r\n\r\nor\r\n\r\n### Instruction:\r\n\r\n### Input:\r\n\r\n### Response:\r\n\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "embed text document in modelfile",
            "url": "https://github.com/ollama/ollama/pull/288",
            "state": "MERGED",
            "createdAt": "2023-08-04T23:01:19Z",
            "mergedAt": "2023-08-09T14:26:20Z",
            "closedAt": "2023-08-09T14:26:20Z",
            "reviews": {
                "totalCount": 21
            },
            "files": {
                "totalCount": 10
            },
            "additions": 371,
            "deletions": 52,
            "body": "Allow embedding information into Modelfiles. This is an initial version that only supports embedding text files, other file types to follow.\r\n\r\n```\r\nFROM llama2\r\nEMBED /path/to/doc.txt\r\nTEMPLATE \"\"\"\r\nContext:\r\n{{ .Embed }}\r\nUser:\r\n{{ .User }}\r\n\"\"\"\r\n```\r\n\r\nTODO before merge:\r\n- [x] Test library `FROM` image (local and pull)\r\n- [x] Test `FROM` local bin file\r\n- [x] Update docs\r\n\r\nResolves #237",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "First draft of API Docs",
            "url": "https://github.com/ollama/ollama/pull/289",
            "state": "MERGED",
            "createdAt": "2023-08-04T23:10:03Z",
            "mergedAt": "2023-08-07T20:46:22Z",
            "closedAt": "2023-08-07T20:46:22Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 2
            },
            "additions": 381,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "implement loading ggml lora adapters through the modelfile",
            "url": "https://github.com/ollama/ollama/pull/290",
            "state": "MERGED",
            "createdAt": "2023-08-05T00:21:45Z",
            "mergedAt": "2023-08-11T00:23:01Z",
            "closedAt": "2023-08-11T00:23:01Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 6
            },
            "additions": 75,
            "deletions": 14,
            "body": "LoRA adapters can be added to Ollama models through the Modelfile and automatically applied when the model is loaded:\r\n\r\n```\r\nFROM llama2:13b\r\nTEMPLATE {{ .Prompt }}\r\nADAPTER ./llama2-13b-storywriter-lora.ggml.bin\r\n```\r\n\r\nA few caveats:\r\n* LoRA adapters must be GGML. If the adapter isn't GGML, it can be converted with the `convert-lora-to-ggml.py` script in https://github.com/ggerganov/llama.cpp\r\n* Using adapters with quantized weights might not produce good results\r\n* Using adapters disables mmap\r\n* It's possible to apply multiple adapters but\r\n  1. Ordering is important\r\n  2. There may be unintended side effects since most adapters are not intended to be layered on other adapters\r\n  3. Performance may degrade with more adapters",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "pass flags to `serve` to allow setting allowed-origins + host and port",
            "url": "https://github.com/ollama/ollama/pull/301",
            "state": "MERGED",
            "createdAt": "2023-08-07T03:41:01Z",
            "mergedAt": "2023-08-08T14:41:43Z",
            "closedAt": "2023-08-08T14:41:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 140,
            "deletions": 10,
            "body": "resolves: https://github.com/jmorganca/ollama/issues/300 and https://github.com/jmorganca/ollama/issues/282\r\n\r\nexample usage:\r\n```\r\nollama serve --port 9999 --allowed-origins \"http://foo.example.com,http://192.0.0.1\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "DockerIt example",
            "url": "https://github.com/ollama/ollama/pull/303",
            "state": "MERGED",
            "createdAt": "2023-08-07T20:47:27Z",
            "mergedAt": "2023-08-16T15:04:35Z",
            "closedAt": "2023-08-16T15:04:35Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 53,
            "deletions": 0,
            "body": "This is a simple example of a model to generate Dockerfiles, along with a Python script to build and run the resulting docker image.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "missed a backtick",
            "url": "https://github.com/ollama/ollama/pull/304",
            "state": "MERGED",
            "createdAt": "2023-08-07T20:54:31Z",
            "mergedAt": "2023-08-07T22:14:06Z",
            "closedAt": "2023-08-07T22:14:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "somehow missed one backtick in near the top",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "automatically set num_keep if num_keep < 0",
            "url": "https://github.com/ollama/ollama/pull/306",
            "state": "MERGED",
            "createdAt": "2023-08-07T23:17:42Z",
            "mergedAt": "2023-08-08T16:25:35Z",
            "closedAt": "2023-08-08T16:25:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 28,
            "deletions": 14,
            "body": "num_keep defines how many tokens to keep in the context when truncating inputs. if left to its default value of -1, the server will calculate num_keep to be the left of the system instructions\r\n\r\nresolves #299 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "quantize f32, f16",
            "url": "https://github.com/ollama/ollama/pull/307",
            "state": "CLOSED",
            "createdAt": "2023-08-07T23:38:20Z",
            "mergedAt": null,
            "closedAt": "2023-08-30T15:55:34Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 5
            },
            "additions": 187,
            "deletions": 2,
            "body": "if the input model in a modelfile is a ggml f32 or f16 file type, and the `FROM` line contains the `AS` keyword, quantize the model to the specified level\r\n\r\nExample Modelfile:\r\n```\r\nFROM /path/to/my/f32.bin AS Q4_0\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add an example on multiline input",
            "url": "https://github.com/ollama/ollama/pull/311",
            "state": "MERGED",
            "createdAt": "2023-08-09T10:56:20Z",
            "mergedAt": "2023-08-10T15:22:29Z",
            "closedAt": "2023-08-10T15:22:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "Source https://github.com/jmorganca/ollama/issues/169\r\n\r\nI believe this example is useful for first-time users \ud83d\ude03 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add embed docs for modelfile",
            "url": "https://github.com/ollama/ollama/pull/312",
            "state": "MERGED",
            "createdAt": "2023-08-09T20:15:56Z",
            "mergedAt": "2023-08-17T17:37:43Z",
            "closedAt": "2023-08-17T17:37:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 0,
            "body": "I removed the embed instruction from our model documentation since its not in a release. Staging it here for a release.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix embeddings invalid values",
            "url": "https://github.com/ollama/ollama/pull/313",
            "state": "MERGED",
            "createdAt": "2023-08-09T20:37:52Z",
            "mergedAt": "2023-08-10T14:17:01Z",
            "closedAt": "2023-08-10T14:17:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 39,
            "body": "Embeddings were occasionally returning invalid values which meant we needed to reload and retry. This fix removes the cache token count which was causing this issue, and improves results. This also matches the llama.cpp example more closely.\r\n\r\nIt also adds the`unsafe.Slice` parsing that Mike suggested in my previous PR, upon further tests this actually works (and it seems faster).",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Token auth",
            "url": "https://github.com/ollama/ollama/pull/314",
            "state": "MERGED",
            "createdAt": "2023-08-09T22:17:28Z",
            "mergedAt": "2023-08-10T18:34:25Z",
            "closedAt": "2023-08-10T18:34:25Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 3
            },
            "additions": 233,
            "deletions": 6,
            "body": "This change implements token authorization for the ollama server.\r\n\r\nThe basic steps for using auth are:\r\n  1. make an authenticated call to the registry; if the registry returns a 401 w/ the Www-Authenticate header, then\r\n  2. look for an SSH ed25519 key pair called `~/.ollama/id_ed25519`\r\n  3. make a call to the token endpoint from the Www-Authenticate header w/ the signed Authorization header (this will be in the form `Authorization: <pub key>:<signature>`). The other params are given in the original 401 Www-Authenticate header which will include the realm and the scope\r\n  4. the token endpoint will issue a new signed JWT for the source specified with the correct scope\r\n  5. the request is made again, this time filling in the header as `Authorization: Bearer <jwt>`\r\n  6. success (the model can be pushed or pulled)\r\n\r\n\r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "fix a typo in the tweetwriter example Modelfile",
            "url": "https://github.com/ollama/ollama/pull/316",
            "state": "MERGED",
            "createdAt": "2023-08-10T11:44:23Z",
            "mergedAt": "2023-08-10T14:19:53Z",
            "closedAt": "2023-08-10T14:19:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: check GetBlobsPath error",
            "url": "https://github.com/ollama/ollama/pull/317",
            "state": "MERGED",
            "createdAt": "2023-08-10T16:43:53Z",
            "mergedAt": "2023-08-10T16:57:50Z",
            "closedAt": "2023-08-10T16:57:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 3,
            "body": "The error returned by `server.GetBlobsPath` in `showLayer` was never checked. Check the error and return if not nil. Also, make newlines at the end of error messages consistent and fix a typo.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "RFC: optional generate header to not stream response",
            "url": "https://github.com/ollama/ollama/pull/319",
            "state": "CLOSED",
            "createdAt": "2023-08-10T20:49:59Z",
            "mergedAt": null,
            "closedAt": "2023-09-28T21:02:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 0,
            "body": "Add an optional request header to the generate endpoint that returns the full response in one JSON body, rather than streaming:\r\n```\r\ncurl -X POST -H \"Content-Type: application/json\" -H \"X-Streamed: false\" -d '{\r\n    \"model\": \"llama2\",\r\n    \"prompt\": \"why is the sky blue?\"\r\n}' 'localhost:11434/api/generate'\r\n```\r\n\r\nThe issue suggests setting the `Content-Type` header to `application/json` to indicate the result should not be streamed, but thats not quite right since the content-type indicates the type of content in the request, rather than the response. \r\n\r\nWe also can't use the `Accept: application/json`, this indicates the response that is expected, but clients would also use `Accept: application/json` in the case of a streaming response, because the returned objects will be json.\r\n\r\nresolves #281\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "length check for parameters",
            "url": "https://github.com/ollama/ollama/pull/321",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:09:25Z",
            "mergedAt": "2023-08-10T23:23:10Z",
            "closedAt": "2023-08-10T23:23:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "no warning on comments",
            "url": "https://github.com/ollama/ollama/pull/322",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:22:55Z",
            "mergedAt": "2023-08-10T23:24:41Z",
            "closedAt": "2023-08-10T23:24:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix could not convert int",
            "url": "https://github.com/ollama/ollama/pull/323",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:22:58Z",
            "mergedAt": "2023-08-10T23:24:33Z",
            "closedAt": "2023-08-10T23:24:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Generate private/public keypair for use w/ auth",
            "url": "https://github.com/ollama/ollama/pull/324",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:24:30Z",
            "mergedAt": "2023-08-11T17:58:23Z",
            "closedAt": "2023-08-11T17:58:23Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 237,
            "deletions": 0,
            "body": "This change automatically creates a new OpenSSH compatible ed25519 key pair in your `~/.ollama` directory. The public key can be uploaded to Ollama and can be subsequently used to authenticate.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "s/parmeter/parameter/",
            "url": "https://github.com/ollama/ollama/pull/325",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:27:11Z",
            "mergedAt": "2023-08-11T00:30:02Z",
            "closedAt": "2023-08-11T00:30:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document num_gqa parameter",
            "url": "https://github.com/ollama/ollama/pull/326",
            "state": "MERGED",
            "createdAt": "2023-08-10T23:59:01Z",
            "mergedAt": "2023-08-11T01:18:38Z",
            "closedAt": "2023-08-11T01:18:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "It is required to be adjusted for some models, see https://github.com/jmorganca/ollama/issues/320 for more context",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add tutorials for using Langchain with ollama",
            "url": "https://github.com/ollama/ollama/pull/329",
            "state": "MERGED",
            "createdAt": "2023-08-11T04:30:50Z",
            "mergedAt": "2023-08-11T22:19:39Z",
            "closedAt": "2023-08-11T22:19:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 163,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "ggml: fix off by one error",
            "url": "https://github.com/ollama/ollama/pull/333",
            "state": "MERGED",
            "createdAt": "2023-08-11T17:45:56Z",
            "mergedAt": "2023-08-11T17:51:07Z",
            "closedAt": "2023-08-11T17:51:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 2,
            "body": "remove used Unknown FileType",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add maximum retries when pushing",
            "url": "https://github.com/ollama/ollama/pull/334",
            "state": "MERGED",
            "createdAt": "2023-08-11T18:02:11Z",
            "mergedAt": "2023-08-11T22:41:55Z",
            "closedAt": "2023-08-11T22:41:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 35,
            "deletions": 21,
            "body": "This change prevents the client from getting into an endless loop when trying to push an image which the user does not have access to push.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update langchainpy.md",
            "url": "https://github.com/ollama/ollama/pull/340",
            "state": "MERGED",
            "createdAt": "2023-08-14T09:13:37Z",
            "mergedAt": "2023-08-14T13:38:42Z",
            "closedAt": "2023-08-14T13:38:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "base_url value for Ollama object creation is corrected.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "do not regenerate embeddings",
            "url": "https://github.com/ollama/ollama/pull/341",
            "state": "MERGED",
            "createdAt": "2023-08-14T13:37:55Z",
            "mergedAt": "2023-08-15T19:10:23Z",
            "closedAt": "2023-08-15T19:10:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 49,
            "deletions": 9,
            "body": "- re-use previously evaluated embeddings when possible\r\n- change embeddings digest identifier to be based on model name and embedded file path\r\n\r\nThis change opens previously generated embeddings for the same model/file and re-uses them when possible. This means that running create on the same file will not generate the embeddings again. This also means that only the difference between the current version of the file and the old version of the file will have the embeddings re-generated.\r\n\r\nresolves #331 \r\nresolves #332 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "log embedding eval timing",
            "url": "https://github.com/ollama/ollama/pull/343",
            "state": "MERGED",
            "createdAt": "2023-08-14T15:51:52Z",
            "mergedAt": "2023-08-14T16:15:56Z",
            "closedAt": "2023-08-14T16:15:56Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "always remove from in progress map on download",
            "url": "https://github.com/ollama/ollama/pull/344",
            "state": "MERGED",
            "createdAt": "2023-08-14T16:15:10Z",
            "mergedAt": "2023-08-15T16:20:32Z",
            "closedAt": "2023-08-15T16:20:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "set non-zero error code on error",
            "url": "https://github.com/ollama/ollama/pull/345",
            "state": "MERGED",
            "createdAt": "2023-08-14T18:17:38Z",
            "mergedAt": "2023-08-16T16:20:28Z",
            "closedAt": "2023-08-16T16:20:28Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 25,
            "deletions": 4,
            "body": "ollama should exit non-zero when operations fail",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add context to api docs",
            "url": "https://github.com/ollama/ollama/pull/346",
            "state": "MERGED",
            "createdAt": "2023-08-14T18:23:30Z",
            "mergedAt": "2023-08-15T14:43:22Z",
            "closedAt": "2023-08-15T14:43:22Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 41,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cross repo blob mount",
            "url": "https://github.com/ollama/ollama/pull/348",
            "state": "MERGED",
            "createdAt": "2023-08-14T22:08:27Z",
            "mergedAt": "2023-08-16T16:20:36Z",
            "closedAt": "2023-08-16T16:20:36Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 8,
            "body": "implement registry's cross repo blob mount",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "close open files",
            "url": "https://github.com/ollama/ollama/pull/349",
            "state": "MERGED",
            "createdAt": "2023-08-14T23:09:15Z",
            "mergedAt": "2023-08-14T23:15:58Z",
            "closedAt": "2023-08-14T23:15:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/350",
            "state": "MERGED",
            "createdAt": "2023-08-14T23:09:45Z",
            "mergedAt": "2023-08-14T23:15:52Z",
            "closedAt": "2023-08-14T23:15:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 18
            },
            "additions": 115,
            "deletions": 58,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use loaded llm for generating model file embeddings",
            "url": "https://github.com/ollama/ollama/pull/351",
            "state": "MERGED",
            "createdAt": "2023-08-15T14:01:49Z",
            "mergedAt": "2023-08-15T19:12:02Z",
            "closedAt": "2023-08-15T19:12:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 17,
            "deletions": 25,
            "body": "use the llm loader when generating embeddings for a modelfile rather than loading a new llm into memory\r\n\r\nresolves #310 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "retry download on network errors",
            "url": "https://github.com/ollama/ollama/pull/354",
            "state": "MERGED",
            "createdAt": "2023-08-15T18:09:21Z",
            "mergedAt": "2023-08-17T14:31:45Z",
            "closedAt": "2023-08-17T14:31:45Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 51,
            "deletions": 22,
            "body": "add a retry mechanism to retry download on error",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix request copies",
            "url": "https://github.com/ollama/ollama/pull/360",
            "state": "MERGED",
            "createdAt": "2023-08-16T18:32:26Z",
            "mergedAt": "2023-08-17T16:58:43Z",
            "closedAt": "2023-08-17T16:58:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 25,
            "deletions": 37,
            "body": "`makeRequest` makes copies of the request body via bytes.Buffer and bytes.Reader in anticipation of a possible retry. While the memory requirements are negligible for most requests, the copies become significant when pushing a model blob. A sufficiently large model will exhaust all memory on the system causing the process to be kill by the host OS.\r\n\r\nThis copy also produces inaccurate progress updates. Since the progress is set from the Pipe, with the copy, it's really measuring how quickly the files are being copied into the buffer and not how quickly the request body is sent over the wire\r\n\r\nInstead of retrying on all requests, only retry when starting a new upload. This is the only time, for now, a request should be retried due to authentication.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "linux cuda build tag",
            "url": "https://github.com/ollama/ollama/pull/363",
            "state": "CLOSED",
            "createdAt": "2023-08-16T21:04:44Z",
            "mergedAt": null,
            "closedAt": "2023-08-23T21:12:52Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 4
            },
            "additions": 21,
            "deletions": 0,
            "body": "This change allows building a linux go binary using a cuda tag. The goal of this tag behaviour is that running `go build .` will still build a Go binary that works on CPU, while adding the cuda tag allows building a go binary that uses nvidia graphics cards (via cuda). This allows everyone that has been using ollama on linux to continue to build from source without any breaking changes.\r\n\r\nTo build with cuda:\r\n`go build -tags cuda .`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "reimplement chunked uploads",
            "url": "https://github.com/ollama/ollama/pull/364",
            "state": "MERGED",
            "createdAt": "2023-08-16T21:45:42Z",
            "mergedAt": "2023-08-17T16:58:51Z",
            "closedAt": "2023-08-17T16:58:51Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 54,
            "deletions": 26,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "set the scopes correctly",
            "url": "https://github.com/ollama/ollama/pull/368",
            "state": "MERGED",
            "createdAt": "2023-08-17T04:39:29Z",
            "mergedAt": "2023-08-17T04:42:02Z",
            "closedAt": "2023-08-17T04:42:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 1,
            "body": "This change fixes the scope authorization to allow cross-repo pushes to work correctly.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "model and file type as strings",
            "url": "https://github.com/ollama/ollama/pull/372",
            "state": "MERGED",
            "createdAt": "2023-08-17T18:41:58Z",
            "mergedAt": "2023-08-17T22:10:59Z",
            "closedAt": "2023-08-17T22:10:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 133,
            "deletions": 48,
            "body": "instead of representing model and file type as their native int values in manifest config, represent them as user-friendly strings",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix push manifest",
            "url": "https://github.com/ollama/ollama/pull/375",
            "state": "MERGED",
            "createdAt": "2023-08-17T22:16:13Z",
            "mergedAt": "2023-08-17T22:33:31Z",
            "closedAt": "2023-08-17T22:33:31Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 44,
            "deletions": 36,
            "body": "When all blobs already exist in the registry, only the manifest is pushed. This request requires authn so must follow the same token exchange flow",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ignore nil map values",
            "url": "https://github.com/ollama/ollama/pull/376",
            "state": "MERGED",
            "createdAt": "2023-08-17T22:51:03Z",
            "mergedAt": "2023-08-17T22:57:12Z",
            "closedAt": "2023-08-17T22:57:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "some api clients may pass a nil value so best to ignore it",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Strip protocol from model path",
            "url": "https://github.com/ollama/ollama/pull/377",
            "state": "MERGED",
            "createdAt": "2023-08-18T00:40:27Z",
            "mergedAt": "2023-08-22T04:56:57Z",
            "closedAt": "2023-08-22T04:56:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 231,
            "deletions": 43,
            "body": "Took a whack at fixing https://github.com/jmorganca/ollama/issues/371 and reorganized the switch logic slightly as well.\r\n\r\nWasn't sure if it was better to strip all protocols or just `https://`, so if you'd like the latter I can switch it to just a `strings.TrimPrefix`. Happy to back out the updated switch code as well, just figured I'd do it while I was in there.\r\n\r\nThanks for a great tool, loving it so far. Hope this is helpful.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "copy metadata from source",
            "url": "https://github.com/ollama/ollama/pull/378",
            "state": "MERGED",
            "createdAt": "2023-08-18T04:56:04Z",
            "mergedAt": "2023-08-18T20:49:09Z",
            "closedAt": "2023-08-18T20:49:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 36,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "retry on unauthorized chunk push",
            "url": "https://github.com/ollama/ollama/pull/381",
            "state": "MERGED",
            "createdAt": "2023-08-18T17:24:22Z",
            "mergedAt": "2023-08-18T20:49:25Z",
            "closedAt": "2023-08-18T20:49:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 45,
            "body": "The token printed for authorized requests has a lifetime of 1h. If an upload exceeds 1h, a chunk push will fail since the token is only created on a \"start upload\" request.\r\n\r\nThis replaces the Pipe with SectionReader which is simpler and implements Seek, a requirement for makeRequestWithRetry. This is slightly worse than using a Pipe since the progress update is directly tied to the chunk size instead of controlled separately, i.e. increasing chunk size will decrease how often the client gets an progress update",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add version",
            "url": "https://github.com/ollama/ollama/pull/392",
            "state": "MERGED",
            "createdAt": "2023-08-22T01:26:20Z",
            "mergedAt": "2023-08-22T16:50:25Z",
            "closedAt": "2023-08-22T16:50:25Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 7
            },
            "additions": 47,
            "deletions": 36,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use url.URL",
            "url": "https://github.com/ollama/ollama/pull/393",
            "state": "MERGED",
            "createdAt": "2023-08-22T01:56:35Z",
            "mergedAt": "2023-08-22T22:51:33Z",
            "closedAt": "2023-08-22T22:51:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 80,
            "deletions": 55,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "build release mode",
            "url": "https://github.com/ollama/ollama/pull/397",
            "state": "MERGED",
            "createdAt": "2023-08-22T16:49:22Z",
            "mergedAt": "2023-08-22T17:48:46Z",
            "closedAt": "2023-08-22T17:48:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Mxyng/cleanup",
            "url": "https://github.com/ollama/ollama/pull/398",
            "state": "MERGED",
            "createdAt": "2023-08-22T19:41:43Z",
            "mergedAt": "2023-08-22T22:51:41Z",
            "closedAt": "2023-08-22T22:51:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 125,
            "deletions": 112,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "subprocess llama.cpp server",
            "url": "https://github.com/ollama/ollama/pull/401",
            "state": "MERGED",
            "createdAt": "2023-08-23T00:26:33Z",
            "mergedAt": "2023-08-30T20:35:03Z",
            "closedAt": "2023-08-30T20:35:03Z",
            "reviews": {
                "totalCount": 39
            },
            "files": {
                "totalCount": 37
            },
            "additions": 958,
            "deletions": 43928,
            "body": "This is a pretty big change that moves llama.cpp from a library within cgo to an external process that we manage.\r\n\r\nWhy?\r\n- This makes building for multiple platforms easier (no more windows cgo incompatibilities)\r\n- We can fallback to non-gpu runners when needed\r\n- Approximately ~200ms faster on average in my tests\r\n- Way less code in our repo\r\n- Maybe easier to manage our build matrix\r\n\r\nMinor Breaking Changes\r\n- Generate response no longer includes sample account or sample duration. These metrics are not included in the response from the llama.cpp server.\r\n- Only one LoRA adapter is supported at a time. The llama.cpp server isn't built for this at the moment. Allowing multiple seems like it would be a pretty simple PR to open with llama.cpp.\r\n\r\nFeatures\r\n- Use the existing loading logic to manage a llama.cpp server\r\n- Package in llama.cpp CPU and GPU runtimes in the Go binary\r\n- Removes vendored llama.cpp code\r\n- No more cgo\r\n\r\nThere's a lot of changes in this PR, here are the files to look at:\r\n- llm/llama.go\r\n- llm/llama_generate.go\r\n- llm/llama_generate_darwin.go\r\n- api/types.go\r\n- app/src/index.ts\r\n- server/routes.go",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add 34b model type",
            "url": "https://github.com/ollama/ollama/pull/405",
            "state": "MERGED",
            "createdAt": "2023-08-24T17:36:03Z",
            "mergedAt": "2023-08-24T17:37:22Z",
            "closedAt": "2023-08-24T17:37:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Build Automation",
            "url": "https://github.com/ollama/ollama/pull/407",
            "state": "CLOSED",
            "createdAt": "2023-08-24T22:13:18Z",
            "mergedAt": null,
            "closedAt": "2023-09-12T19:26:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 73,
            "deletions": 0,
            "body": "Working on moving our builds to a github runner on release. This is a CPU amd64 build. \r\n\r\nNo arm64 in this PR because github doesn't have an arm64 runner, but we need to build the llama.cpp exe on an arm64 machine for the arm64 release. We can use an external build runner to accomplish this.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "patch llama.cpp for 34B",
            "url": "https://github.com/ollama/ollama/pull/411",
            "state": "MERGED",
            "createdAt": "2023-08-25T17:07:10Z",
            "mergedAt": "2023-08-25T18:59:05Z",
            "closedAt": "2023-08-25T18:59:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update README.md",
            "url": "https://github.com/ollama/ollama/pull/412",
            "state": "MERGED",
            "createdAt": "2023-08-25T18:45:01Z",
            "mergedAt": "2023-08-27T04:26:34Z",
            "closedAt": "2023-08-27T04:26:34Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 17,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "delete all models (not just 1st) in `ollama rm`",
            "url": "https://github.com/ollama/ollama/pull/415",
            "state": "MERGED",
            "createdAt": "2023-08-26T04:39:03Z",
            "mergedAt": "2023-08-26T07:47:56Z",
            "closedAt": "2023-08-26T07:47:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 4,
            "body": "Previously, `ollama rm model1 model2 modelN` would only delete `model1`. The other model command-line arguments would be silently ignored. Now, all models mentioned are deleted.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "treat `ollama run model < file` as entire prompt, not prompt-per-line",
            "url": "https://github.com/ollama/ollama/pull/416",
            "state": "CLOSED",
            "createdAt": "2023-08-26T04:59:56Z",
            "mergedAt": null,
            "closedAt": "2023-11-14T16:30:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 15,
            "body": "Previously, `ollama run` treated a non-terminal stdin (such as `ollama run model < file`) as containing one prompt per line. To run inference on a multi-line prompt, the only non-API workaround was to run `ollama run` interactively and wrap the prompt in `\"\"\"...\"\"\"`.\r\n\r\nNow, `ollama run` treats a non-terminal stdin as containing a single prompt. For example, if `myprompt.txt` is a multi-line file, then `ollama run model < myprompt.txt` would treat `myprompt.txt`'s entire contents as the prompt.\r\n\r\nThis breaks backcompat, but I believe this behavior is better than the old behavior. It is strictly more powerful than the prior behavior because callers can split a file by lines outside of Ollama and then invoke `ollama run` once per line on their own.\r\n\r\nThis is related to https://github.com/jmorganca/ollama/issues/357, but that refers to interactive usage.\r\n\r\nFixes #568 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "add 34b to mem check",
            "url": "https://github.com/ollama/ollama/pull/420",
            "state": "MERGED",
            "createdAt": "2023-08-26T15:29:38Z",
            "mergedAt": "2023-08-26T21:15:52Z",
            "closedAt": "2023-08-26T21:15:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow F16 to use metal",
            "url": "https://github.com/ollama/ollama/pull/421",
            "state": "MERGED",
            "createdAt": "2023-08-26T15:39:14Z",
            "mergedAt": "2023-08-29T13:32:59Z",
            "closedAt": "2023-08-29T13:32:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 6,
            "body": "warning F16 uses significantly more memory than quantized model so the standard requires don't apply.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "set default template",
            "url": "https://github.com/ollama/ollama/pull/426",
            "state": "MERGED",
            "createdAt": "2023-08-26T19:21:52Z",
            "mergedAt": "2023-08-26T21:15:38Z",
            "closedAt": "2023-08-26T21:15:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "fixes #413 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update upload chunks",
            "url": "https://github.com/ollama/ollama/pull/428",
            "state": "MERGED",
            "createdAt": "2023-08-27T04:59:06Z",
            "mergedAt": "2023-08-30T14:47:17Z",
            "closedAt": "2023-08-30T14:47:17Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 91,
            "deletions": 51,
            "body": "This PR increases the upload chunk size which will improve throughput. In order to prove more responsive progress bar, this PR changes the file reader back to a pipe. It keeps the main reader as a SectionReader for simplicity.\r\n\r\nMinor change to HTTP status code checks: errors states has been loosened to < 400 (http.StatusBadRequest) for success and >= 400 for failures.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add model IDs",
            "url": "https://github.com/ollama/ollama/pull/439",
            "state": "MERGED",
            "createdAt": "2023-08-29T03:37:36Z",
            "mergedAt": "2023-08-29T03:50:24Z",
            "closedAt": "2023-08-29T03:50:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 27,
            "deletions": 19,
            "body": "This change shows a portion (first 12 hex chars) of the sha256 sum of the manifest when running `ollama ls`. This makes it really easy at a glance to tell if two models are the same, and will make it easier in the future to match models inside of the ollama library.\r\n\r\nIt looks something like:\r\n```\r\nNAME                                    ID              SIZE    MODIFIED\r\ncodellama:34b-instruct                  901abb8f0f4b    19 GB   3 days ago\r\ncodellama:latest                        adf065e2ff94    3.8 GB  3 days ago\r\ncodeup:13b                              400f83199325    7.4 GB  2 weeks ago\r\nllama-mario:latest                      d5793f033f5c    7.3 GB  4 weeks ago\r\nllama2:13b                              156106c1e540    7.3 GB  4 weeks ago\r\nllama2:latest                           5c1a4ea68dd0    3.8 GB  9 hours ago\r\nllama2-uncensored:7b-chat-q6_K          26cf13ee4cfe    5.5 GB  8 days ago\r\nllama2-uncensored:latest                5823fb1154c5    3.8 GB  4 weeks ago\r\nnous-hermes:latest                      bfba379045c1    7.3 GB  6 weeks ago\r\n```\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "build: add Docker Compose file and service for running Ollama with Do\u2026",
            "url": "https://github.com/ollama/ollama/pull/440",
            "state": "CLOSED",
            "createdAt": "2023-08-29T13:38:50Z",
            "mergedAt": null,
            "closedAt": "2023-11-29T21:22:40Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 54,
            "deletions": 0,
            "body": "\r\n- Add Docker Compose file for running Ollama with Docker\r\n- Create a new file `docker-compose.yaml`\r\n- Define the `ollama` service in the Docker Compose file\r\n- Build the image and set the image name to `jmorganca/ollama`\r\n- Mount the `runtime/ollama` directory to `/home/ollama` in the container",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "GGUF support",
            "url": "https://github.com/ollama/ollama/pull/441",
            "state": "MERGED",
            "createdAt": "2023-08-29T22:02:08Z",
            "mergedAt": "2023-09-07T17:55:37Z",
            "closedAt": "2023-09-07T17:55:37Z",
            "reviews": {
                "totalCount": 17
            },
            "files": {
                "totalCount": 10
            },
            "additions": 541,
            "deletions": 137,
            "body": "This change adds support for running GGUF models which are currently in beta with llama.cpp. We will continue to run GGML models and this transition will be seamless to users.\r\n\r\n- Adds a llama.cpp mainline submodule which runs `GGUF` models\r\n- Dynamically select the right runner for the model type\r\n- Moved a some code to different files\r\n\r\n```\r\n./ollama run gguf-codellama hello world\r\n\r\nThis is your first interaction with me. I am a bot, and I am created by you. Please ask me any questions you would like answered.\r\n```\r\n\r\nAs mentioned in #423 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "treat stop as stop sequences, not exact tokens",
            "url": "https://github.com/ollama/ollama/pull/442",
            "state": "MERGED",
            "createdAt": "2023-08-30T05:19:09Z",
            "mergedAt": "2023-08-30T15:53:42Z",
            "closedAt": "2023-08-30T15:53:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 109,
            "deletions": 17,
            "body": "The `stop` option to the generate API is a list of sequences that should cause generation to stop. Although these are commonly called \"stop tokens\", they do not necessarily correspond to LLM tokens (per the LLM's tokenizer). For example, if the caller sends a generate request with `\"stop\":[\"\\n\"]`, then generation should stop on any token containing `\\n` (and trim `\\n` from the output), not just if the token exactly matches `\\n`. If `stop` were interpreted strictly as LLM tokens, then it would require callers of the generate API to know the LLM's tokenizer and enumerate many tokens in the `stop` list.\n\nFixes https://github.com/jmorganca/ollama/issues/295.\n\nExample output (note that generation ends on a token ` not` that is truncated to ` n` because the stop sequence is `ot`):\n\n```\n% curl -d '{\"prompt\":\"const primes=[1,2,3,\",\"model\":\"codellama:7b\",\"options\":{\"seed\":1337,\"temperature\":0,\"num_ctx\":100,\"stop\":[\"ot\"]}}' http://localhost:11434/api/generate\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.435096Z\",\"response\":\" The\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.486337Z\",\"response\":\" code\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.53943Z\",\"response\":\" you\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.593747Z\",\"response\":\" provided\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.648514Z\",\"response\":\" is\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.702975Z\",\"response\":\" n\",\"done\":false}\n{\"model\":\"codellama:7b\",\"created_at\":\"2023-08-30T05:17:54.702999Z\",\"done\":true, ...}\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "windows: fix filepath bugs",
            "url": "https://github.com/ollama/ollama/pull/443",
            "state": "MERGED",
            "createdAt": "2023-08-30T18:16:07Z",
            "mergedAt": "2023-08-31T21:19:10Z",
            "closedAt": "2023-08-31T21:19:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 49,
            "deletions": 73,
            "body": "List and Delete has the same issue where the path was constructed using Linux/macOS path separators which does not work in Windows. This PR fixes and simplifies the code.\r\n\r\nFix `filenameWithPath` which also assumes a Linux/macOS path separator when looking for `~`.\r\n\r\nUse `filenameWithPath` to resolve adapter filepath",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix spelling errors in example prompts",
            "url": "https://github.com/ollama/ollama/pull/448",
            "state": "MERGED",
            "createdAt": "2023-08-31T15:35:07Z",
            "mergedAt": "2023-08-31T15:57:07Z",
            "closedAt": "2023-08-31T15:57:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update readme",
            "url": "https://github.com/ollama/ollama/pull/450",
            "state": "MERGED",
            "createdAt": "2023-09-01T14:55:13Z",
            "mergedAt": "2023-09-01T15:21:50Z",
            "closedAt": "2023-09-01T15:21:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update readme",
            "url": "https://github.com/ollama/ollama/pull/451",
            "state": "MERGED",
            "createdAt": "2023-09-01T15:26:45Z",
            "mergedAt": "2023-09-01T20:44:14Z",
            "closedAt": "2023-09-01T20:44:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 34,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "first pass at linux gpu support",
            "url": "https://github.com/ollama/ollama/pull/454",
            "state": "MERGED",
            "createdAt": "2023-09-01T20:39:21Z",
            "mergedAt": "2023-09-12T15:04:35Z",
            "closedAt": "2023-09-12T15:04:35Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 9
            },
            "additions": 158,
            "deletions": 22,
            "body": "This is the basic implementation of enabling a linux build with GPU support.\r\n\r\nBuilding for Linux with CPU support is unchanged (generate and build as normal).\r\n\r\nBuilding for Linux with GPU requires generating with the `gpu` tag. This is to allow non-GPU linux builds to continue to be built locally without issue.\r\n\r\nHow to build/run:\r\n- generate the required dependencies: `go generate ./...`\r\n- build the binary `go build .`\r\nand run as normal:\r\n`./ollama serve &`\r\n`./ollama run llama2`\r\n\r\nFollow up:\r\n- Packaging nvidia drivers or downloading them automatically\r\n- Better heuristics for determining the number of layers to load into GPU\r\n\r\nPart of #259 ",
            "participants": {
                "totalCount": 8
            },
            "comments": {
                "totalCount": 20
            }
        }
    },
    {
        "node": {
            "title": "generate in build script",
            "url": "https://github.com/ollama/ollama/pull/455",
            "state": "CLOSED",
            "createdAt": "2023-09-01T20:50:01Z",
            "mergedAt": null,
            "closedAt": "2023-09-05T14:45:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 2,
            "body": "Add llama.cpp exe generation to the build script. Still need to figure out a way to pack both amd64 and arm64 to make this binary truly universal. ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "do not HTML-escape prompt",
            "url": "https://github.com/ollama/ollama/pull/457",
            "state": "MERGED",
            "createdAt": "2023-09-01T22:17:50Z",
            "mergedAt": "2023-09-02T00:41:53Z",
            "closedAt": "2023-09-02T00:41:54Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 24,
            "deletions": 1,
            "body": "The `html/template` package automatically HTML-escapes interpolated strings in templates. This behavior is undesirable because it causes prompts like `<h1>hello` to be escaped to `&lt;h1&gt;hello` before being passed to the LLM.\n\nThe included test case passes, but before the code change, it failed:\n\n```\n--- FAIL: TestModelPrompt\n    images_test.go:21: got \"a&lt;h1&gt;b\", want \"a<h1>b\"\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "generate binary dependencies based on `GOARCH` on macos",
            "url": "https://github.com/ollama/ollama/pull/459",
            "state": "MERGED",
            "createdAt": "2023-09-02T21:54:52Z",
            "mergedAt": "2023-09-05T16:53:58Z",
            "closedAt": "2023-09-05T16:53:58Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 6,
            "body": "This will allow building a universal binary (or cross compiling for `amd64`) on `arm64` Macs:\r\n\r\n```\r\n% GOARCH=amd64 go generate ./...\r\n% GOARCH=amd64 go build .\r\n% file ./ollama\r\n./ollama: Mach-O 64-bit executable x86_64\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix inherit params",
            "url": "https://github.com/ollama/ollama/pull/461",
            "state": "MERGED",
            "createdAt": "2023-09-03T18:12:32Z",
            "mergedAt": "2023-09-05T19:30:23Z",
            "closedAt": "2023-09-05T19:30:23Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 33,
            "deletions": 8,
            "body": "params from inherited models are not merged into the new model\r\n\r\ntest cases:\r\n\r\n- add parameters to model with no parameters:\r\ninput:\r\n```\r\nFROM orca-mini:3b\r\nPARAMETER temperature 0\r\n```\r\noutput:\r\n```json\r\n{\r\n  \"temperature\": 0\r\n}\r\n```\r\n\r\n- no parameters with model with parameters:\r\ninput:\r\n```\r\nFROM codellama:7b-code\r\n```\r\noutput:\r\n```json\r\n{\r\n  \"rope_frequency_base\": 1000000\r\n}\r\n```\r\n\r\n- add parameters to model with parameters:\r\ninput:\r\n```\r\nFROM codellama:7b-code\r\nPARAMETER temperature 0\r\n```\r\noutput:\r\n```json\r\n{\r\n  \"rope_frequency_base\": 1000000,\r\n  \"temperature\": 0\r\n}\r\n\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove marshalPrompt which is no longer needed",
            "url": "https://github.com/ollama/ollama/pull/462",
            "state": "MERGED",
            "createdAt": "2023-09-03T18:13:11Z",
            "mergedAt": "2023-09-05T18:48:42Z",
            "closedAt": "2023-09-05T18:48:42Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 38,
            "deletions": 82,
            "body": "llama.cpp server handles truncating input",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix not forwarding last token",
            "url": "https://github.com/ollama/ollama/pull/463",
            "state": "MERGED",
            "createdAt": "2023-09-03T21:48:36Z",
            "mergedAt": "2023-09-05T16:01:32Z",
            "closedAt": "2023-09-05T16:01:32Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 45,
            "body": "llama.cpp server serves the last token along with `stop: true`\r\n\r\nalso remove unused fields",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix num_keep",
            "url": "https://github.com/ollama/ollama/pull/464",
            "state": "MERGED",
            "createdAt": "2023-09-03T21:49:19Z",
            "mergedAt": "2023-09-05T18:30:45Z",
            "closedAt": "2023-09-05T18:30:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "num_keep calculation is erroneously adding a token which causes the llm to output `\\u001c` after truncating",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "metal: add missing barriers for mul-mat",
            "url": "https://github.com/ollama/ollama/pull/469",
            "state": "MERGED",
            "createdAt": "2023-09-05T20:08:44Z",
            "mergedAt": "2023-09-05T23:37:13Z",
            "closedAt": "2023-09-05T23:37:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 111,
            "deletions": 2,
            "body": "port \r\nhttps://github.com/ggerganov/llama.cpp/pull/2699\r\nto fix null response on generate",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "backport as separate patches",
            "url": "https://github.com/ollama/ollama/pull/470",
            "state": "MERGED",
            "createdAt": "2023-09-05T21:37:32Z",
            "mergedAt": "2023-09-05T23:27:25Z",
            "closedAt": "2023-09-05T23:27:25Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 77,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix empty response",
            "url": "https://github.com/ollama/ollama/pull/471",
            "state": "MERGED",
            "createdAt": "2023-09-05T22:03:48Z",
            "mergedAt": "2023-09-05T22:23:05Z",
            "closedAt": "2023-09-05T22:23:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Added missing options params to the embeddings docs",
            "url": "https://github.com/ollama/ollama/pull/472",
            "state": "MERGED",
            "createdAt": "2023-09-05T23:59:24Z",
            "mergedAt": "2023-09-06T00:18:49Z",
            "closedAt": "2023-09-06T00:18:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "create manifests directory",
            "url": "https://github.com/ollama/ollama/pull/473",
            "state": "MERGED",
            "createdAt": "2023-09-06T00:12:07Z",
            "mergedAt": "2023-09-06T00:37:41Z",
            "closedAt": "2023-09-06T00:37:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 1,
            "body": "`ollama list` on a brand new install will panic because the manifests directory doesn't exist",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add show command",
            "url": "https://github.com/ollama/ollama/pull/474",
            "state": "MERGED",
            "createdAt": "2023-09-06T01:13:42Z",
            "mergedAt": "2023-09-06T18:04:17Z",
            "closedAt": "2023-09-06T18:04:17Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 5
            },
            "additions": 299,
            "deletions": 50,
            "body": "This change adds the ability to inspect various parts of a given model. It adds functionality from both the CLI (via the `ollama show` command) and through the REPL (through the various `/show ...` commands).\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "tighten up the error string for `ollama show` flags",
            "url": "https://github.com/ollama/ollama/pull/476",
            "state": "MERGED",
            "createdAt": "2023-09-06T20:37:08Z",
            "mergedAt": "2023-09-06T20:38:49Z",
            "closedAt": "2023-09-06T20:38:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix model manifests",
            "url": "https://github.com/ollama/ollama/pull/477",
            "state": "MERGED",
            "createdAt": "2023-09-06T21:19:54Z",
            "mergedAt": "2023-09-06T21:30:08Z",
            "closedAt": "2023-09-06T21:30:08Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "follow up to #473 which only created the parent directory, not manifests itself",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove unused openssh key types",
            "url": "https://github.com/ollama/ollama/pull/478",
            "state": "MERGED",
            "createdAt": "2023-09-06T22:13:05Z",
            "mergedAt": "2023-09-06T22:18:54Z",
            "closedAt": "2023-09-06T22:18:54Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 81,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update dockerfile",
            "url": "https://github.com/ollama/ollama/pull/479",
            "state": "MERGED",
            "createdAt": "2023-09-06T22:25:53Z",
            "mergedAt": "2023-09-06T22:44:24Z",
            "closedAt": "2023-09-06T22:44:24Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 13,
            "deletions": 11,
            "body": "```\r\ndocker build -t ollama .\r\ndocker run -d -p 11434:11434 -v $HOME/.ollama:/home/ollama/.ollama ollama\r\n```\r\n\r\nThis container image does not build GPU. That'll come later, after #454 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "[docs] Improve build instructions",
            "url": "https://github.com/ollama/ollama/pull/482",
            "state": "MERGED",
            "createdAt": "2023-09-07T07:27:13Z",
            "mergedAt": "2023-09-07T10:43:26Z",
            "closedAt": "2023-09-07T10:43:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Go is required and not installed by default.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: retry push on expired token",
            "url": "https://github.com/ollama/ollama/pull/486",
            "state": "MERGED",
            "createdAt": "2023-09-07T19:04:36Z",
            "mergedAt": "2023-09-07T20:58:34Z",
            "closedAt": "2023-09-07T20:58:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 32,
            "deletions": 23,
            "body": "There's two bug that need to be fixed:\r\n\r\n1. `makeRequest` to the `redirectURL` should not supply `regOpts` since it's not the registry. This erroneously overrides the `Authorization` Header making the request invalid.\r\n2. The upload chunk was not resetting the section correctly. It also should to interrupt the goroutine writing into the pipe",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update dockerignore",
            "url": "https://github.com/ollama/ollama/pull/487",
            "state": "MERGED",
            "createdAt": "2023-09-07T20:36:33Z",
            "mergedAt": "2023-09-07T21:16:17Z",
            "closedAt": "2023-09-07T21:16:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add cuda docker image",
            "url": "https://github.com/ollama/ollama/pull/488",
            "state": "MERGED",
            "createdAt": "2023-09-07T21:54:50Z",
            "mergedAt": "2023-09-08T14:38:20Z",
            "closedAt": "2023-09-08T14:38:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 62,
            "deletions": 38,
            "body": "`Dockerfile.cuda` requires [`nvidia-container-toolkit`](https://gitlab.com/nvidia/container-toolkit/container-toolkit) to run successfully:\r\n\r\n```\r\n$ docker build -t ollama:cuda -f Dockerfile.cuda .\r\n$ docker run -d --gpus=all -p 11434:11434 -v $HOME/.ollama:/home/ollama/.ollama ollama:cuda\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add autoprune to remove unused layers",
            "url": "https://github.com/ollama/ollama/pull/491",
            "state": "MERGED",
            "createdAt": "2023-09-07T23:06:48Z",
            "mergedAt": "2023-09-11T18:46:35Z",
            "closedAt": "2023-09-11T18:46:35Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 136,
            "deletions": 19,
            "body": "This change will remove any unused layers for models. It runs at server startup, and will also clean up on `pull` or `create` commands which can orphan older layers.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix nil pointer dereference",
            "url": "https://github.com/ollama/ollama/pull/492",
            "state": "MERGED",
            "createdAt": "2023-09-08T00:24:48Z",
            "mergedAt": "2023-09-08T00:25:23Z",
            "closedAt": "2023-09-08T00:25:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add `format` to model config layer",
            "url": "https://github.com/ollama/ollama/pull/497",
            "state": "MERGED",
            "createdAt": "2023-09-08T16:12:31Z",
            "mergedAt": "2023-09-09T21:53:44Z",
            "closedAt": "2023-09-09T21:53:44Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use cmake toolchain to configure build",
            "url": "https://github.com/ollama/ollama/pull/500",
            "state": "CLOSED",
            "createdAt": "2023-09-09T00:36:39Z",
            "mergedAt": null,
            "closedAt": "2023-09-11T16:39:37Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 10
            },
            "additions": 45,
            "deletions": 26,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update docker image",
            "url": "https://github.com/ollama/ollama/pull/507",
            "state": "MERGED",
            "createdAt": "2023-09-11T18:16:10Z",
            "mergedAt": "2023-09-14T18:25:59Z",
            "closedAt": "2023-09-14T18:25:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 11,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "create the blobs directory correctly",
            "url": "https://github.com/ollama/ollama/pull/508",
            "state": "MERGED",
            "createdAt": "2023-09-11T21:53:52Z",
            "mergedAt": "2023-09-11T21:54:52Z",
            "closedAt": "2023-09-11T21:54:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "support for packaging in multiple cuda runners",
            "url": "https://github.com/ollama/ollama/pull/509",
            "state": "MERGED",
            "createdAt": "2023-09-11T23:38:14Z",
            "mergedAt": "2023-09-14T19:08:13Z",
            "closedAt": "2023-09-14T19:08:13Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 5
            },
            "additions": 95,
            "deletions": 37,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "amd64 linux build runner",
            "url": "https://github.com/ollama/ollama/pull/518",
            "state": "CLOSED",
            "createdAt": "2023-09-12T19:26:16Z",
            "mergedAt": null,
            "closedAt": "2023-09-21T13:48:18Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 6
            },
            "additions": 206,
            "deletions": 36,
            "body": "Add automation that automatically creates a single ollama binary for amd64 linux builds. \r\n\r\nLimitations:\r\n  - Requires glibc 2.29 (the glibc version ubuntu 20.04 has packed in), ideally we build on an ubuntu 16.04 or 18.04 runner instead to maximize glibc compatibility, but that will require a custom runner. `glibc` is used by linux to access kernal functionality so it cant really be updated by an end-user without updating their OS.\r\n\r\nFuture work:\r\n  - Ideally I'd rather just install both version of nvcc on one runner and swap between them. I tried this and I hit some issues with the wrong cuda version being referenced during builds.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Mxyng/decode",
            "url": "https://github.com/ollama/ollama/pull/519",
            "state": "MERGED",
            "createdAt": "2023-09-12T19:35:57Z",
            "mergedAt": "2023-09-13T19:43:57Z",
            "closedAt": "2023-09-13T19:43:58Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 5
            },
            "additions": 134,
            "deletions": 157,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix ggml arm64 linux cuda build",
            "url": "https://github.com/ollama/ollama/pull/520",
            "state": "MERGED",
            "createdAt": "2023-09-12T20:49:57Z",
            "mergedAt": "2023-09-12T21:06:48Z",
            "closedAt": "2023-09-12T21:06:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 35,
            "deletions": 2,
            "body": "Apply patch to support CUDA's half type for aarch64",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add a simple python client to access ollama",
            "url": "https://github.com/ollama/ollama/pull/522",
            "state": "MERGED",
            "createdAt": "2023-09-13T00:38:52Z",
            "mergedAt": "2023-09-14T23:37:38Z",
            "closedAt": "2023-09-14T23:37:38Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 225,
            "deletions": 0,
            "body": "These are some simple python bindings for interacting with the local Ollama server. Most of the functions should be pretty straight forward, and each of the streaming endpoints has a default way of handling the output but can be passed in a \"callback\" function to override the default.\r\n\r\nThe callback functions can be as simple as:\r\n```\r\ndef my_callback(chunk):\r\n        \"\"\"\r\n        Callback function to handle individual chunks of the streaming response.\r\n   \r\n        Parameters:\r\n        - chunk (dict): The individual chunk of JSON data from the streaming response.\r\n        \"\"\"\r\n   \r\n        # Here, we are simply printing the entire chunk as a JSON string with indentation\r\n        # for readability. In a real application, you would likely want to do something\r\n        # more specific with the data in each chunk.\r\n        print(json.dumps(chunk, indent=4))\r\n   \r\n        # If you want to specifically print the 'response' field, you can do so like this:\r\n        # response_piece = chunk.get('response')\r\n        # if response_piece:\r\n        #     print(response_piece)\r\n```\r\n\r\nIt's been a little while since I put together a python library, so I'm not sure if I hit all of the correct idioms here. To test it out, you can do something like:\r\n\r\n```\r\n>>> import sys\r\n>>> sys.path.append(\"<path to git/ollama/api>\")\r\n>>> import client\r\n```\r\n\r\nTo run something like generate:\r\n```\r\n>>> response, history = client.generate(\"llama2\", \"Is friendship like magic?\")\r\n\r\nThis will return a string for the output and the context history which you can feed back in with:\r\n>>> client.generate(\"llama2\", \"Friendship sure feels like magic\", context=history)\r\n\r\nTo hook in the above callback, use:\r\n>>> client.generate(\"llama2\", \"Is the universe a giant black hole?\", callback=my_callback)\r\n\r\n\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "subprocess improvements",
            "url": "https://github.com/ollama/ollama/pull/524",
            "state": "MERGED",
            "createdAt": "2023-09-13T19:42:08Z",
            "mergedAt": "2023-09-18T19:16:33Z",
            "closedAt": "2023-09-18T19:16:33Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 4
            },
            "additions": 120,
            "deletions": 105,
            "body": "- increase start-up timeout\r\n- when runner fails to start fail rather than timing out\r\n- try runners in order rather than choosing 1 runner\r\n- embed metal runner in metal dir rather than gpu\r\n- refactor logging and error messages\r\n\r\nresolves #485 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove unused",
            "url": "https://github.com/ollama/ollama/pull/526",
            "state": "MERGED",
            "createdAt": "2023-09-13T21:48:55Z",
            "mergedAt": "2023-09-14T20:10:59Z",
            "closedAt": "2023-09-14T20:10:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update API docs",
            "url": "https://github.com/ollama/ollama/pull/527",
            "state": "MERGED",
            "createdAt": "2023-09-14T00:00:30Z",
            "mergedAt": "2023-09-14T15:51:26Z",
            "closedAt": "2023-09-14T15:51:26Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 113,
            "deletions": 25,
            "body": "cleanup docs, add show and push.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "implement ProgressWriter",
            "url": "https://github.com/ollama/ollama/pull/530",
            "state": "MERGED",
            "createdAt": "2023-09-14T18:10:05Z",
            "mergedAt": "2023-09-15T19:43:46Z",
            "closedAt": "2023-09-15T19:43:46Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 90,
            "deletions": 80,
            "body": "Simplify writing progress by implementing a ProgressWriter. Using `io.TeeReader`, the ProgressWriter will be responsible for updating user on the progress of the upload (and eventually down) without manually defining copy chunks.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "set request.ContentLength",
            "url": "https://github.com/ollama/ollama/pull/531",
            "state": "MERGED",
            "createdAt": "2023-09-14T18:11:06Z",
            "mergedAt": "2023-09-14T20:33:11Z",
            "closedAt": "2023-09-14T20:33:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "This informs the HTTP client the content length is known and disables chunked Transfer-Encoding",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove `.First`",
            "url": "https://github.com/ollama/ollama/pull/532",
            "state": "CLOSED",
            "createdAt": "2023-09-15T05:12:25Z",
            "mergedAt": null,
            "closedAt": "2024-01-09T18:58:37Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 5
            },
            "additions": 2,
            "deletions": 11,
            "body": "This change removes the need for `.First`",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "linux installer script",
            "url": "https://github.com/ollama/ollama/pull/534",
            "state": "MERGED",
            "createdAt": "2023-09-15T16:42:29Z",
            "mergedAt": "2023-09-22T16:01:03Z",
            "closedAt": "2023-09-22T16:01:03Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 1
            },
            "additions": 142,
            "deletions": 0,
            "body": "Add an install script to the website which downloads the appropriate linux package, unpackages it, adds it to the /usr/local/bin directory, and adds ollama as start-up service.\r\n\r\nBefore our next release we will:\r\n\r\n- Do linux amd64 and aarch64 builds with CUDA enabled.\r\n- Add them to the pre-release of the jmorgan/ollama repo with the names ollama-linux-arm64.tar.gz and ollama-linux-amd64.tar.gz\r\n- This install script will automatically download the latest version of these files from github releases which are not pre-release.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "only add a layer if there is actual data",
            "url": "https://github.com/ollama/ollama/pull/535",
            "state": "MERGED",
            "createdAt": "2023-09-15T20:59:14Z",
            "mergedAt": "2023-09-18T20:47:46Z",
            "closedAt": "2023-09-18T20:47:46Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 5,
            "body": "This is a simple change which checks the layer size before adding it to the overall model. Registry balks if you try to send it an empty layer on an `ollama push`.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "explicitly follow upload redirects",
            "url": "https://github.com/ollama/ollama/pull/536",
            "state": "MERGED",
            "createdAt": "2023-09-15T21:58:10Z",
            "mergedAt": "2023-09-20T18:27:03Z",
            "closedAt": "2023-09-20T18:27:03Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 49,
            "deletions": 19,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix error on upload chunk",
            "url": "https://github.com/ollama/ollama/pull/537",
            "state": "MERGED",
            "createdAt": "2023-09-15T22:59:52Z",
            "mergedAt": "2023-09-16T00:48:40Z",
            "closedAt": "2023-09-16T00:48:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Cmd changes",
            "url": "https://github.com/ollama/ollama/pull/541",
            "state": "MERGED",
            "createdAt": "2023-09-16T19:20:17Z",
            "mergedAt": "2023-09-18T19:26:56Z",
            "closedAt": "2023-09-18T19:26:56Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 75,
            "deletions": 52,
            "body": "This changes the API so that it can use `POST /api/generate` to pre-load a model if the prompt is empty. It also changes the REPL so that it can pre-load the model by making a call to generate automatically, and includes a change for using placeholder text.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Docker Cuda File update & Documentation Addition.",
            "url": "https://github.com/ollama/ollama/pull/552",
            "state": "CLOSED",
            "createdAt": "2023-09-18T21:36:52Z",
            "mergedAt": null,
            "closedAt": "2023-10-24T23:13:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 17,
            "deletions": 2,
            "body": "Adding ability have cuda work on docker with the ubuntu image provided, along with a docker.md for commands that can be added documenting around docker usage",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add word wrapping for lines which are longer than the terminal width",
            "url": "https://github.com/ollama/ollama/pull/553",
            "state": "MERGED",
            "createdAt": "2023-09-19T05:15:36Z",
            "mergedAt": "2023-09-22T20:36:08Z",
            "closedAt": "2023-09-22T20:36:08Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 65,
            "deletions": 25,
            "body": "This change makes it so the REPL will properly wrap a line on a word boundary. The way it works is that it walks through each character of each token returned by the server, and then keeps a buffer of the last word. If the maximum boundary length is exceeded, it will backtrack using ANSI escape codes to the length of the current word buffer, erase to the end of the line, give a line feed, and then add the word fragment to the new line. This requires that the terminal allow ANSI graphics which should be OK for any modern terminal. If you run this headless, it will default to not wrapping any lines.\r\n\r\nRight now I've set the width to 5 characters less than the terminal width, but we can potentially make this a setting in the future.\r\n\r\nThis fixes issue #150 \r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix path for windows",
            "url": "https://github.com/ollama/ollama/pull/554",
            "state": "MERGED",
            "createdAt": "2023-09-19T16:36:53Z",
            "mergedAt": "2023-09-19T16:42:12Z",
            "closedAt": "2023-09-19T16:42:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 7,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix build",
            "url": "https://github.com/ollama/ollama/pull/555",
            "state": "MERGED",
            "createdAt": "2023-09-19T17:42:34Z",
            "mergedAt": "2023-09-19T17:51:58Z",
            "closedAt": "2023-09-19T17:51:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "pack in cuda libs",
            "url": "https://github.com/ollama/ollama/pull/556",
            "state": "MERGED",
            "createdAt": "2023-09-20T16:42:58Z",
            "mergedAt": "2023-09-20T22:02:37Z",
            "closedAt": "2023-09-20T22:02:37Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 14
            },
            "additions": 52,
            "deletions": 115,
            "body": "This change packs CUDA libs into the llama runner and tells the runner to use those libs.\r\n\r\nHere is the example generate in my case.\r\n```\r\ngo generate ./...\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix impossible condition",
            "url": "https://github.com/ollama/ollama/pull/557",
            "state": "MERGED",
            "createdAt": "2023-09-20T18:28:15Z",
            "mergedAt": "2023-09-20T18:51:01Z",
            "closedAt": "2023-09-20T18:51:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 4,
            "body": "split from #536 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add dockerfile for building linux binaries",
            "url": "https://github.com/ollama/ollama/pull/558",
            "state": "MERGED",
            "createdAt": "2023-09-20T18:40:01Z",
            "mergedAt": "2023-09-22T19:20:13Z",
            "closedAt": "2023-09-22T19:20:13Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 43,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove tmp directories created by previous servers",
            "url": "https://github.com/ollama/ollama/pull/559",
            "state": "MERGED",
            "createdAt": "2023-09-20T20:13:11Z",
            "mergedAt": "2023-09-21T19:38:49Z",
            "closedAt": "2023-09-21T19:38:49Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 6
            },
            "additions": 56,
            "deletions": 60,
            "body": "with packing in cuda libs these start to get pretty big, clean temp `ollama-` dirs up before creating new ones.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix OLLAMA_HOST parsing for ip6",
            "url": "https://github.com/ollama/ollama/pull/562",
            "state": "MERGED",
            "createdAt": "2023-09-21T00:59:40Z",
            "mergedAt": "2023-09-21T02:54:47Z",
            "closedAt": "2023-09-21T02:54:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 17,
            "body": "Fix the environment parsing for `OLLAMA_HOST` so it can recognize ipv6 addresses, e.g. ipv6 loopback `[::1]:11434`\r\n\r\nSome examples:\r\n\r\nDefault\r\n\r\n```\r\n$ OLLAMA_HOST='' ollama serve\r\n2023/09/20 17:55:23 routes.go:540: Listening on 127.0.0.1:11434\r\n```\r\n\r\nIPv6 loopback\r\n\r\n```\r\n$ OLLAMA_HOST='[::1]:11434' ollama serve\r\n2023/09/20 17:58:08 routes.go:540: Listening on [::1]:11434\r\n```\r\n\r\nRandom port (any IPv4 & IPv6 address)\r\n\r\n```\r\n$ OLLAMA_HOST=':0' ollama serve\r\n2023/09/20 17:58:26 routes.go:540: Listening on [::]:63574\r\n```\r\n\r\nOnly IPv4\r\n\r\n```\r\n$ OLLAMA_HOST='127.0.0.1:12345' ollama serve\r\n2023/09/20 17:58:37 routes.go:540: Listening on 127.0.0.1:12345\r\n```\r\n\r\nOnly IPv6\r\n\r\n```\r\n$ OLLAMA_HOST='[::1]:12345' ollama serve\r\n2023/09/20 17:59:23 routes.go:540: Listening on [::1]:12345\r\n```\r\n\r\nOnly setting the address\r\n\r\n```\r\n$ OLLAMA_HOST='0.0.0.0' ollama serve\r\n2023/09/20 18:54:18 routes.go:540: Listening on [::]:11434\r\n```\r\n\r\nIt also removes `OLLAMA_PORT`\r\n\r\nResolves #560 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Use API to check if model exists and pull if necessary",
            "url": "https://github.com/ollama/ollama/pull/566",
            "state": "MERGED",
            "createdAt": "2023-09-21T16:52:05Z",
            "mergedAt": "2023-09-21T17:35:14Z",
            "closedAt": "2023-09-21T17:35:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 29,
            "deletions": 35,
            "body": "Resolves #484 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update submodule",
            "url": "https://github.com/ollama/ollama/pull/567",
            "state": "MERGED",
            "createdAt": "2023-09-21T20:13:28Z",
            "mergedAt": "2023-09-21T20:22:23Z",
            "closedAt": "2023-09-21T20:22:23Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "silence warm up log",
            "url": "https://github.com/ollama/ollama/pull/569",
            "state": "MERGED",
            "createdAt": "2023-09-21T21:54:44Z",
            "mergedAt": "2023-09-21T23:52:43Z",
            "closedAt": "2023-09-21T23:52:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 30,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix HEAD request",
            "url": "https://github.com/ollama/ollama/pull/570",
            "state": "MERGED",
            "createdAt": "2023-09-21T23:41:17Z",
            "mergedAt": "2023-09-21T23:56:17Z",
            "closedAt": "2023-09-21T23:56:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 8,
            "body": "HEAD request should respond like their GET counterparts except without a response body.\r\n\r\nThe previous implementation didn't quite satisfy this since it doesn't attach a response body so the content length is zero while the GET request responded with `Ollama is running`, content length 17.\r\n\r\nDespite having a response body, gin will omit it in the actual response:\r\n\r\n```\r\n$ http HEAD :11434/\r\nHTTP/1.1 200 OK\r\nContent-Length: 17\r\nContent-Type: text/plain; charset=utf-8\r\nDate: Thu, 21 Sep 2023 23:41:08 GMT\r\n\r\n\r\n\r\n\r\n$ http GET :11434/\r\nHTTP/1.1 200 OK\r\nContent-Length: 17\r\nContent-Type: text/plain; charset=utf-8\r\nDate: Thu, 21 Sep 2023 23:41:11 GMT\r\n\r\nOllama is running\r\n\r\n\r\n\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update dockerfile.cuda",
            "url": "https://github.com/ollama/ollama/pull/571",
            "state": "MERGED",
            "createdAt": "2023-09-22T00:59:12Z",
            "mergedAt": "2023-09-22T19:34:42Z",
            "closedAt": "2023-09-22T19:34:42Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 14,
            "deletions": 34,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added a new community project",
            "url": "https://github.com/ollama/ollama/pull/574",
            "state": "MERGED",
            "createdAt": "2023-09-22T17:17:40Z",
            "mergedAt": "2023-09-25T14:40:59Z",
            "closedAt": "2023-09-25T14:40:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix ipv6 parse ip",
            "url": "https://github.com/ollama/ollama/pull/575",
            "state": "MERGED",
            "createdAt": "2023-09-22T17:42:18Z",
            "mergedAt": "2023-09-22T18:47:11Z",
            "closedAt": "2023-09-22T18:47:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "`net.ParseIP` for IPv6 doesn't expect `[]` so trim it",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ubuntu cuda drivers",
            "url": "https://github.com/ollama/ollama/pull/576",
            "state": "MERGED",
            "createdAt": "2023-09-22T18:13:11Z",
            "mergedAt": "2023-09-22T18:43:14Z",
            "closedAt": "2023-09-22T18:43:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 20,
            "deletions": 2,
            "body": "Add automatic cuda driver install to our install script.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "close llm on interrupt",
            "url": "https://github.com/ollama/ollama/pull/577",
            "state": "MERGED",
            "createdAt": "2023-09-22T18:26:21Z",
            "mergedAt": "2023-09-22T18:41:53Z",
            "closedAt": "2023-09-22T18:41:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "switch to forked readline lib which doesn't wreck the repl prompt",
            "url": "https://github.com/ollama/ollama/pull/578",
            "state": "MERGED",
            "createdAt": "2023-09-22T19:17:00Z",
            "mergedAt": "2023-09-22T19:17:45Z",
            "closedAt": "2023-09-22T19:17:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 4,
            "body": "There's a bug in the readline library for non-Windows systems which causes the placeholder text to drop a character. This switches us over to a patched version temporarily.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 11
            }
        }
    },
    {
        "node": {
            "title": "debian installer support",
            "url": "https://github.com/ollama/ollama/pull/579",
            "state": "MERGED",
            "createdAt": "2023-09-22T20:59:34Z",
            "mergedAt": "2023-09-23T13:46:48Z",
            "closedAt": "2023-09-23T13:46:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 158,
            "deletions": 122,
            "body": "- normalize os name to lowercase\r\n- check needed commands are available\r\n- dont check sudo when root user\r\n- share common install commands\r\n- support debian cuda install\r\n- skip aarm cuda install\r\n- system user shared home dir",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor and add other platforms",
            "url": "https://github.com/ollama/ollama/pull/580",
            "state": "MERGED",
            "createdAt": "2023-09-22T23:16:38Z",
            "mergedAt": "2023-09-23T13:42:41Z",
            "closedAt": "2023-09-23T13:42:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 155,
            "deletions": 202,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix end-of-line issue with the new prompt",
            "url": "https://github.com/ollama/ollama/pull/582",
            "state": "MERGED",
            "createdAt": "2023-09-24T00:04:07Z",
            "mergedAt": "2023-09-24T00:20:30Z",
            "closedAt": "2023-09-24T00:20:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 5,
            "body": "The readline library had this fix which overwrote the end of the `S` in \"Send a message...\" which prevented the cursor from moving up the screen whenever you backspaced through the end of the line. We removed it to fix the placeholder text issue, but then the bug crept back in.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add the example for ask the mentors",
            "url": "https://github.com/ollama/ollama/pull/585",
            "state": "MERGED",
            "createdAt": "2023-09-24T22:59:46Z",
            "mergedAt": "2023-10-09T20:58:14Z",
            "closedAt": "2023-10-09T20:58:14Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 5
            },
            "additions": 115,
            "deletions": 0,
            "body": "this is an example that will be used in a blog post about talking to mentors",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix linux installer warning logs",
            "url": "https://github.com/ollama/ollama/pull/588",
            "state": "MERGED",
            "createdAt": "2023-09-25T15:18:45Z",
            "mergedAt": "2023-09-25T15:22:56Z",
            "closedAt": "2023-09-25T15:22:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Fixes warnings on `lshw` command when not running as root user. Adds missing `warning` logger.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update install.sh",
            "url": "https://github.com/ollama/ollama/pull/589",
            "state": "MERGED",
            "createdAt": "2023-09-25T17:05:11Z",
            "mergedAt": "2023-09-25T18:08:25Z",
            "closedAt": "2023-09-25T18:08:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 11,
            "body": "minor changes to warnings/errors",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix dkms install",
            "url": "https://github.com/ollama/ollama/pull/590",
            "state": "MERGED",
            "createdAt": "2023-09-25T18:28:28Z",
            "mergedAt": "2023-09-25T19:17:32Z",
            "closedAt": "2023-09-25T19:17:32Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "unbound max num gpu layers",
            "url": "https://github.com/ollama/ollama/pull/591",
            "state": "MERGED",
            "createdAt": "2023-09-25T18:59:44Z",
            "mergedAt": "2023-09-25T22:36:46Z",
            "closedAt": "2023-09-25T22:36:46Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 4
            },
            "additions": 36,
            "deletions": 29,
            "body": "Load as many layers into VRAM as possible using model file size as a rough heuristic for the amount of memory required for a layer. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix dkms on debian",
            "url": "https://github.com/ollama/ollama/pull/592",
            "state": "MERGED",
            "createdAt": "2023-09-25T19:58:07Z",
            "mergedAt": "2023-09-25T19:59:01Z",
            "closedAt": "2023-09-25T19:59:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "debian installs dkms into `/usr/sbin`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update install.sh",
            "url": "https://github.com/ollama/ollama/pull/593",
            "state": "MERGED",
            "createdAt": "2023-09-25T20:40:09Z",
            "mergedAt": "2023-09-25T21:09:40Z",
            "closedAt": "2023-09-25T21:09:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "exit on unknown distro",
            "url": "https://github.com/ollama/ollama/pull/594",
            "state": "MERGED",
            "createdAt": "2023-09-25T22:30:03Z",
            "mergedAt": "2023-09-25T22:30:58Z",
            "closedAt": "2023-09-25T22:30:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ignore systemctl is-system-running exit code",
            "url": "https://github.com/ollama/ollama/pull/595",
            "state": "MERGED",
            "createdAt": "2023-09-25T22:47:54Z",
            "mergedAt": "2023-09-25T22:49:47Z",
            "closedAt": "2023-09-25T22:49:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update install.sh",
            "url": "https://github.com/ollama/ollama/pull/596",
            "state": "MERGED",
            "createdAt": "2023-09-25T23:12:52Z",
            "mergedAt": "2023-09-26T00:59:14Z",
            "closedAt": "2023-09-26T00:59:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 1,
            "body": "This prevents the service from restarting too early and not detecting GPU before drivers are installed.\r\n\r\nFix PATH for WSL user. WSL preinstalls CUDA toolkit but it's in a non-standard path (`/usr/lib/wsl/lib`). While this is set for a normal WSL user, it's not set for the ollama user. This change sets PATH of the ollama service to the PATH of the caller",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "build slim, GPU-less docker image",
            "url": "https://github.com/ollama/ollama/pull/597",
            "state": "CLOSED",
            "createdAt": "2023-09-25T23:15:46Z",
            "mergedAt": null,
            "closedAt": "2024-04-14T22:46:54Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 47,
            "deletions": 9,
            "body": "build a cpu-only docker image which is significantly smaller than the gpu image\r\n\r\n```\r\nollama          cuda              dfdbcb88bc3d   4 minutes ago   754MB\r\nollama          slim              fb2e67c26718   7 minutes ago   148MB\r\n```\r\n\r\nRelated #516 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add painter message for exit",
            "url": "https://github.com/ollama/ollama/pull/598",
            "state": "MERGED",
            "createdAt": "2023-09-25T23:31:15Z",
            "mergedAt": "2023-09-26T22:17:40Z",
            "closedAt": "2023-09-26T22:17:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "tell the user how to exit. other options are available (`/exit`, `ctrl+c`) but don't want to be too verbose",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update install.sh",
            "url": "https://github.com/ollama/ollama/pull/599",
            "state": "MERGED",
            "createdAt": "2023-09-26T01:10:06Z",
            "mergedAt": "2023-09-26T01:24:13Z",
            "closedAt": "2023-09-26T01:24:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md for linux + cleanup",
            "url": "https://github.com/ollama/ollama/pull/601",
            "state": "MERGED",
            "createdAt": "2023-09-26T06:30:46Z",
            "mergedAt": "2023-09-26T06:44:53Z",
            "closedAt": "2023-09-26T06:44:53Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 99,
            "deletions": 99,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added ollama gui interface",
            "url": "https://github.com/ollama/ollama/pull/604",
            "state": "CLOSED",
            "createdAt": "2023-09-26T16:06:02Z",
            "mergedAt": null,
            "closedAt": "2023-09-29T02:10:46Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "This was already added but since the new release i think it didn't came through the merge i suppose.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "do not unload nouveau driver",
            "url": "https://github.com/ollama/ollama/pull/605",
            "state": "MERGED",
            "createdAt": "2023-09-26T16:37:51Z",
            "mergedAt": "2023-09-26T16:53:05Z",
            "closedAt": "2023-09-26T16:53:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "unloading this driver on a desktop kills the display which is not optimal. instead, inform the user they need to reboot",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ordered list of install locations",
            "url": "https://github.com/ollama/ollama/pull/606",
            "state": "MERGED",
            "createdAt": "2023-09-26T16:39:12Z",
            "mergedAt": "2023-09-29T18:30:46Z",
            "closedAt": "2023-09-29T18:30:46Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 4,
            "body": "select install dir based what's in the PATH. if `/usr/local/bin` is in the path, install into there, otherwise `/usr/bin` or `/bin`, in order",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update build scripts",
            "url": "https://github.com/ollama/ollama/pull/608",
            "state": "MERGED",
            "createdAt": "2023-09-26T19:03:59Z",
            "mergedAt": "2023-09-29T18:30:26Z",
            "closedAt": "2023-09-29T18:30:26Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 6
            },
            "additions": 67,
            "deletions": 23,
            "body": "update target arch so VERSION and GOFLAGS are consistently set. In particular Dockerfile.build's ARGs are out of scope when using in `go build` so it's not being set correctly\r\n\r\nResolves #607 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fedora install fixes",
            "url": "https://github.com/ollama/ollama/pull/609",
            "state": "MERGED",
            "createdAt": "2023-09-26T21:34:51Z",
            "mergedAt": "2023-09-27T15:43:48Z",
            "closedAt": "2023-09-27T15:43:48Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "- Do not install `kernel-headers` which is not available or needed in Fedora\r\n- Update CUDA version check to pick-up nvidia package\r\n- Remove ollama bin from temporary directory after install, since I was seeing \"no space left\" errors upon running models.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix error messages for unknown commands in the repl",
            "url": "https://github.com/ollama/ollama/pull/611",
            "state": "MERGED",
            "createdAt": "2023-09-27T00:33:10Z",
            "mergedAt": "2023-09-28T21:19:46Z",
            "closedAt": "2023-09-28T21:19:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "prune empty directories",
            "url": "https://github.com/ollama/ollama/pull/612",
            "state": "MERGED",
            "createdAt": "2023-09-27T00:40:27Z",
            "mergedAt": "2023-09-29T18:23:40Z",
            "closedAt": "2023-09-29T18:23:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 54,
            "deletions": 0,
            "body": "Resolves #270",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added `num_predict` to `docs/modelfile.md`",
            "url": "https://github.com/ollama/ollama/pull/614",
            "state": "MERGED",
            "createdAt": "2023-09-27T01:47:10Z",
            "mergedAt": "2023-09-27T14:26:09Z",
            "closedAt": "2023-09-27T14:26:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "Upstreaming learning from https://github.com/jmorganca/ollama/issues/581 to docs",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix model name not matching",
            "url": "https://github.com/ollama/ollama/pull/616",
            "state": "MERGED",
            "createdAt": "2023-09-27T02:50:15Z",
            "mergedAt": "2023-09-27T03:54:19Z",
            "closedAt": "2023-09-27T03:54:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added missing return preventing SIGSEGV because of missing resp",
            "url": "https://github.com/ollama/ollama/pull/621",
            "state": "MERGED",
            "createdAt": "2023-09-27T10:49:31Z",
            "mergedAt": "2023-09-28T21:25:23Z",
            "closedAt": "2023-09-28T21:25:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Closes #619 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "parallel chunked downloads",
            "url": "https://github.com/ollama/ollama/pull/626",
            "state": "MERGED",
            "createdAt": "2023-09-27T23:32:50Z",
            "mergedAt": "2023-10-06T20:01:29Z",
            "closedAt": "2023-10-06T20:01:29Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 5
            },
            "additions": 258,
            "deletions": 175,
            "body": "this change chunks the download into smaller parts that can be downloaded at the same time. this should result in a bump in download speeds\r\n\r\nTODO:\r\n- [x] handle concurrent requests for the same blobs\r\n- [x] handle resuming interrupted downloads",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Update modelfile.md to reflect the usage of num_gpu.",
            "url": "https://github.com/ollama/ollama/pull/629",
            "state": "MERGED",
            "createdAt": "2023-09-28T04:09:38Z",
            "mergedAt": "2023-09-28T14:21:21Z",
            "closedAt": "2023-09-28T14:21:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The current docs for the parameter num_gpu are inaccurate for linux.\r\nRef: https://github.com/jmorganca/ollama/issues/618",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document response stream chunk delimiter.",
            "url": "https://github.com/ollama/ollama/pull/632",
            "state": "MERGED",
            "createdAt": "2023-09-28T13:24:16Z",
            "mergedAt": "2023-09-30T04:45:52Z",
            "closedAt": "2023-09-30T04:45:52Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 0,
            "body": "Discussion on discord at https://discord.com/channels/1128867683291627614/1128867684130508875/1156838261919076352",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "do not download updates multiple times",
            "url": "https://github.com/ollama/ollama/pull/633",
            "state": "MERGED",
            "createdAt": "2023-09-28T14:20:30Z",
            "mergedAt": "2023-09-28T19:29:18Z",
            "closedAt": "2023-09-28T19:29:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 40,
            "body": "We've hit a bug in the Electron auto-updater that prevents the toolbar app from restarting after update when `autoUpdater.checkForUpdates()` is called more than once. The root cause of this is not clear, it may be related to [this Electron issue](https://github.com/electron-userland/electron-builder/issues/7800). In any case we shouldnt be downloading the update multiple times, so prevent checking for updates once we know one is available.\r\n\r\nAlso log Electron app errors to our server.log file so we can actually diagnose these issues in the wild.\r\n\r\nresolves #587 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use int64 consistently",
            "url": "https://github.com/ollama/ollama/pull/634",
            "state": "MERGED",
            "createdAt": "2023-09-28T18:07:46Z",
            "mergedAt": "2023-09-28T21:17:47Z",
            "closedAt": "2023-09-28T21:17:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 59,
            "deletions": 59,
            "body": "this reduces type conversion",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "do not reload model when only prompt template changes",
            "url": "https://github.com/ollama/ollama/pull/635",
            "state": "CLOSED",
            "createdAt": "2023-09-28T18:51:14Z",
            "mergedAt": null,
            "closedAt": "2023-10-18T18:11:15Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 4
            },
            "additions": 123,
            "deletions": 57,
            "body": "Say I have 2 models, both are based on llama2, but they have different prompts.\r\n```\r\nFROM llama2\r\nTEMPLATE \"\"\"\r\nyou are a dog\r\n\"\"\"\r\n```\r\nand\r\n```\r\nFROM llama2\r\nTEMPLATE \"\"\"\r\nyou are a cat\r\n\"\"\"\r\n```\r\nIf I am building something that swaps requests between these models a lot our current logic will re-load the models every time, even though the only thing changing is the prompt template. \r\n\r\nThis change adds a `runner digest` which uses only fields relevant to running the model to determine if a running model should be swapped out.\r\n\r\nAs a side-effect, this also fixes the `/show modelfile` to actually show the library model name, rather than the file name when using a base model.\r\n```\r\nollama run mistral\r\n>>> /show modelfile\r\n# Modelfile generated by \"ollama show\"\r\n# To build a new Modelfile based on this one, replace the FROM line with:\r\n# FROM mistral:latest\r\n\r\nFROM registry.ollama.ai/library/mistral:latest\r\nTEMPLATE \"\"\"[INST] {{ .Prompt }} [/INST]\r\n\"\"\"\r\nSYSTEM \"\"\"\"\"\"\r\n```\r\n\r\n- Also remove calculation on system prompt from template that makes sure the first system command is kept via `num_keep`. This isn't needed with our new prompt templates.\r\n\r\nResolves #337 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "windows runner fixes",
            "url": "https://github.com/ollama/ollama/pull/637",
            "state": "MERGED",
            "createdAt": "2023-09-28T20:13:13Z",
            "mergedAt": "2023-09-29T15:47:55Z",
            "closedAt": "2023-09-29T15:47:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 4,
            "body": "- use filepath for runner files\r\n- get embedded files with unix filepath\r\n- runner is only available is embedded directories have files",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "optional parameter to not stream response",
            "url": "https://github.com/ollama/ollama/pull/639",
            "state": "MERGED",
            "createdAt": "2023-09-28T21:02:35Z",
            "mergedAt": "2023-10-11T16:54:27Z",
            "closedAt": "2023-10-11T16:54:27Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 94,
            "deletions": 18,
            "body": "Add an optional `stream` parameter to the generate endpoint (and other endpoints that stream a response) to return the full response in one JSON body, rather than streaming:\r\n```\r\ncurl -X POST -H \"Content-Type: application/json\" -d '{\r\n    \"model\": \"llama2\",\r\n    \"prompt\": \"why is the sky blue?\",\r\n    \"stream\": false\r\n}' 'localhost:11434/api/generate'\r\n```\r\n\r\nWhen `stream` is not specified it defaults to true. \r\n\r\nresolves https://github.com/jmorganca/ollama/issues/281",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "allow the user to cancel generating with ctrl-C",
            "url": "https://github.com/ollama/ollama/pull/641",
            "state": "MERGED",
            "createdAt": "2023-09-28T21:56:55Z",
            "mergedAt": "2023-09-29T00:13:01Z",
            "closedAt": "2023-09-29T00:13:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 3,
            "body": "The change allows the user to cancel generating using Ctrl-C in the REPL. It handles both the cases where the request is canceled before the stream starts back, as well as when the request is streaming.\r\n\r\nIt also changes the REPL so that you can't accidentally hit Ctrl-C and exit the REPL. Instead it will prompt the user to use Ctrl-D or `/bye` instead.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Dynamically support ROCm, CUDA, or OpenCL in the GPU-accelerated binary",
            "url": "https://github.com/ollama/ollama/pull/642",
            "state": "CLOSED",
            "createdAt": "2023-09-29T00:30:59Z",
            "mergedAt": null,
            "closedAt": "2023-10-01T17:33:09Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 3
            },
            "additions": 107,
            "deletions": 18,
            "body": "This PR changes the way CMake generation works for the cuda binary, and adds support for querying AMD VRAM. ROCm or CUDA (or OpenCL if neither are available) support are enabled dynamically for gguf, and CUDA or OpenCL support are enabled dynamically for ggml. This is performed by running a CMake managing go script in go generate to query via heuristics the presence of various accelerator SDKs, and enable them in the following order: CUDA, ROCm, OpenCL.\r\n\r\nThe VRAM detection change uses rocm-info. Note that devices with both an AMD and nVidia GPU will use CUDA and report CUDA VRAM by default, so the binary name default of cuda is still appropriate, but it might make sense to call it gpu or accelerated or something in the future.\r\n\r\nSecond try since GitHub automatically closed #628 when I cleared my commits and reapplied the changes in the UI :cry: . I compressed some commits and rebased on head. Comments welcome, it's easier to see what is going on from the files changed view.\r\n \r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "remove unused push/pull params",
            "url": "https://github.com/ollama/ollama/pull/650",
            "state": "MERGED",
            "createdAt": "2023-09-29T21:26:11Z",
            "mergedAt": "2023-09-29T21:27:19Z",
            "closedAt": "2023-09-29T21:27:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "pythonic python client",
            "url": "https://github.com/ollama/ollama/pull/653",
            "state": "CLOSED",
            "createdAt": "2023-09-30T03:43:51Z",
            "mergedAt": null,
            "closedAt": "2024-01-11T23:52:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 206,
            "deletions": 277,
            "body": "new features:\r\n- chat\r\n```python\r\nclient.chat('name', messages=[\r\n  {\r\n    'role': 'system',\r\n    'content': 'you are a good bot',\r\n  },\r\n])\r\n```\r\n- create with a string input instead of a file\r\n```python\r\nclient.create('name', modelfile='''\r\nFROM llama2\r\nPARAMETER stop </s>\r\n''')\r\n```\r\n\r\nkey differences:\r\n- errors are not caught since they should be handled by the caller, e.g. for authorization\r\n- responses are either returned in full if `stream=False` or return as a generator if `stream=True`\r\n- stream errors are raised\r\n- no callbacks\r\n- no prints\r\n- use PEP257 doc strings \r\n\r\nexample usage:\r\n\r\nnon-streaming:\r\n```shell\r\n$ PYTHONPATH=/path/to/ollama/repo python -c 'import api.client; print(api.client.generate(\"llama2\", \"hello\"))'\r\n{'model': 'llama2', 'created_at': '2023-09-30T03:42:23.93685Z', 'done': True, 'context': [29961, 25580, 29962, 22172, 518, 29914, 25580, 29962, 29871, 15043, 29991, 739, 29915, 29879, 7575, 304, 5870, 366, 29889, 26077, 29991, 1128, 508, 306, 1371, 366, 9826, 29973], 'total_duration': 2113510125, 'load_duration': 1169916, 'prompt_eval_count': 1, 'eval_count': 20, 'eval_duration': 2071018000, 'response': \" Hello! It's nice to meet you. everybody! How can I help you today?\"}\r\n```\r\n\r\n```python\r\nimport api.client\r\nprint(api.client.generate('llama2', 'hello'))\r\n```\r\n\r\nstreaming:\r\n```shell\r\n$ PYTHONPATH=/path/to/ollama/repo python -c 'import api.client; [print(x) for x in api.client.generate(\"llama2\", \"hello\", stream=True)]'\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.293429Z', 'response': ' Hello', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.37183Z', 'response': '!', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.447804Z', 'response': ' It', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.524626Z', 'response': \"'\", 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.599881Z', 'response': 's', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.675419Z', 'response': ' nice', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.751513Z', 'response': ' to', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.828406Z', 'response': ' meet', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.903462Z', 'response': ' you', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:23.980436Z', 'response': '.', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.056455Z', 'response': ' nobody', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.133128Z', 'response': '.', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.209594Z', 'response': ' How', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.295103Z', 'response': ' can', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.374437Z', 'response': ' I', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.449904Z', 'response': ' help', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.525837Z', 'response': ' you', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.602418Z', 'response': ' today', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.681443Z', 'response': '?', 'done': False}\r\n{'model': 'llama2', 'created_at': '2023-09-30T04:05:24.762006Z', 'done': True, 'context': [29961, 25580, 29962, 22172, 518, 29914, 25580, 29962, 29871, 15043, 29991, 739, 29915, 29879, 7575, 304, 5870, 366, 29889, 23196, 29889, 1128, 508, 306, 1371, 366, 9826, 29973], 'total_duration': 2571336583, 'load_duration': 1762500, 'prompt_eval_count': 1, 'eval_count': 20, 'eval_duration': 2495695000}\r\n```\r\n\r\n```python\r\nimport api.client\r\nfor chunk in api.client.generate('llama2', 'hello', stream=True):\r\n  print(chunk)\r\n```\r\n\r\nstreaming just the response text:\r\n```shell\r\n$ PYTHONPATH=/path/to/ollama/repo python -c 'import api.client; [print(x.get(\"response\", \"\"), end=\"\", flush=True) for x in api.client.generate(\"llama2\", \"hello\", stream=True)]; print()'\r\n Hello! It's nice to meet you. surely. How can I assist you today? Is there something on your mind that you would like to talk about or ask?\r\n```\r\n\r\n```python\r\nimport api.client\r\nfor chunk in api.client.generate('llama2', 'hello', stream=True):\r\n  print(chunk.get('response', ''), end='', flush=True)\r\nprint()\r\n```\r\n\r\nHere's an example of llava in a xkcd explainer:\r\n\r\n```python\r\nimport requests\r\nimport api.client as client\r\n\r\nr = requests.get('https://imgs.xkcd.com/comics/standards.png')\r\nr.raise_for_status()\r\n\r\nfor r in client.generate('mike/llava:13b', 'explain this comic', images=[r.content], stream=True):\r\n  print(r.get('response'), end='', flush=True)\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix docker build",
            "url": "https://github.com/ollama/ollama/pull/659",
            "state": "MERGED",
            "createdAt": "2023-09-30T19:50:42Z",
            "mergedAt": "2023-09-30T20:34:01Z",
            "closedAt": "2023-09-30T20:34:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Resolves #652 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "don't wordwrap when stdout is redirected or piped",
            "url": "https://github.com/ollama/ollama/pull/662",
            "state": "MERGED",
            "createdAt": "2023-09-30T23:27:30Z",
            "mergedAt": "2023-10-02T18:50:55Z",
            "closedAt": "2023-10-02T18:50:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 24,
            "body": "This turns off wordwrap when stdout is being redirected like `ollama run llama2 \"tell me a story\" > file.txt`.\r\n\r\nAddresses #656 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix for #586, seed and temperature settings",
            "url": "https://github.com/ollama/ollama/pull/663",
            "state": "CLOSED",
            "createdAt": "2023-10-01T11:12:37Z",
            "mergedAt": null,
            "closedAt": "2023-10-02T18:54:02Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "Fix for #586. Seed was omitted in the params to the llama.cpp server and temperature had an `omitempty` filter specified, breaking support for `0` temperature.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "add some missing code directives in docs",
            "url": "https://github.com/ollama/ollama/pull/664",
            "state": "MERGED",
            "createdAt": "2023-10-01T13:24:48Z",
            "mergedAt": "2023-10-01T18:51:01Z",
            "closedAt": "2023-10-01T18:51:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 24,
            "deletions": 25,
            "body": "- mostly just adding bash to shell scripts",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Relay default values to llama runner",
            "url": "https://github.com/ollama/ollama/pull/672",
            "state": "MERGED",
            "createdAt": "2023-10-02T17:32:53Z",
            "mergedAt": "2023-10-02T18:53:16Z",
            "closedAt": "2023-10-02T18:53:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 43,
            "deletions": 44,
            "body": "Thanks to @hallh for #663. This change cherry-picks that PR, relays all our defaults, and does some re-organizing of the code to make it easier to read.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "clean up num_gpu calculation code",
            "url": "https://github.com/ollama/ollama/pull/673",
            "state": "MERGED",
            "createdAt": "2023-10-02T18:11:47Z",
            "mergedAt": "2023-10-02T18:53:42Z",
            "closedAt": "2023-10-02T18:53:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "there were some unreachable code paths and unused variables here from iterations on an old branch, remove them",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "output type parsed from modelfile",
            "url": "https://github.com/ollama/ollama/pull/678",
            "state": "MERGED",
            "createdAt": "2023-10-02T20:55:36Z",
            "mergedAt": "2023-10-05T18:58:04Z",
            "closedAt": "2023-10-05T18:58:04Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 7,
            "body": "This is an attempt to prevent #648 and make it easier to diagnose problems in this area in the future. I couldnt reproduce the exact problem but this could be the issue.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "`Modelfile` syntax highlighting",
            "url": "https://github.com/ollama/ollama/pull/679",
            "state": "MERGED",
            "createdAt": "2023-10-02T20:56:26Z",
            "mergedAt": "2023-10-06T19:59:45Z",
            "closedAt": "2023-10-06T19:59:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 11,
            "body": "Pertains to https://github.com/jmorganca/ollama/issues/649:\r\n- Highlighted `Modelfile` in `modelfile.md`\r\n- Made it clear the name can be lowercase",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "show a default message when license/parameters/etc aren't specified",
            "url": "https://github.com/ollama/ollama/pull/681",
            "state": "MERGED",
            "createdAt": "2023-10-02T21:33:32Z",
            "mergedAt": "2023-10-02T21:34:53Z",
            "closedAt": "2023-10-02T21:34:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 20,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "decode starcoder",
            "url": "https://github.com/ollama/ollama/pull/686",
            "state": "MERGED",
            "createdAt": "2023-10-03T02:52:46Z",
            "mergedAt": "2023-10-03T05:47:19Z",
            "closedAt": "2023-10-03T05:47:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 32,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Increase client/server streaming buffer size to prevent `token too long` error",
            "url": "https://github.com/ollama/ollama/pull/692",
            "state": "MERGED",
            "createdAt": "2023-10-03T20:28:42Z",
            "mergedAt": "2023-10-04T18:09:00Z",
            "closedAt": "2023-10-04T18:09:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 11,
            "deletions": 3,
            "body": "In the case of a large input the response from `/generate` would be very long due to the encoded context length. Increase the buffer size to prevent this error.\r\n\r\nresolves #687 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "enable q8, q5, 5_1, and f32 for linux gpu",
            "url": "https://github.com/ollama/ollama/pull/699",
            "state": "MERGED",
            "createdAt": "2023-10-04T19:13:54Z",
            "mergedAt": "2023-10-05T16:53:47Z",
            "closedAt": "2023-10-05T16:53:47Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 14,
            "body": "The bug effecting running these quantizations on Metal does not effect nvidia GPUs, so only turn off GPU support for these model types when Ollama is running on MacOS.\r\n\r\nTested and verified.\r\n\r\nResolves #671",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "rename server subprocess",
            "url": "https://github.com/ollama/ollama/pull/700",
            "state": "MERGED",
            "createdAt": "2023-10-04T19:53:55Z",
            "mergedAt": "2023-10-06T14:15:42Z",
            "closedAt": "2023-10-06T14:15:42Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 5
            },
            "additions": 16,
            "deletions": 6,
            "body": "rename llama.cpp `server.exe` to `ollama-runner`. This makes it easier to see that the subprocess is associated with ollama.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix go test./... issue: fmt.Println arg list ends with redundant newline",
            "url": "https://github.com/ollama/ollama/pull/705",
            "state": "MERGED",
            "createdAt": "2023-10-04T22:02:22Z",
            "mergedAt": "2023-10-05T15:11:05Z",
            "closedAt": "2023-10-05T15:11:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "`go test ./...` currently fails with:\r\n\r\n```\r\n# github.com/jmorganca/ollama/cmd\r\ncmd/cmd.go:690:7: fmt.Println arg list ends with redundant newline\r\ncmd/cmd.go:698:7: fmt.Println arg list ends with redundant newline\r\ncmd/cmd.go:704:7: fmt.Println arg list ends with redundant newline\r\ncmd/cmd.go:710:7: fmt.Println arg list ends with redundant newline\r\n?       github.com/jmorganca/ollama     [no test files]\r\n?       github.com/jmorganca/ollama/api [no test files]\r\n?       github.com/jmorganca/ollama/llm [no test files]\r\n?       github.com/jmorganca/ollama/llm/llama.cpp       [no test files]\r\n?       github.com/jmorganca/ollama/parser      [no test files]\r\n?       github.com/jmorganca/ollama/progressbar [no test files]\r\nok      github.com/jmorganca/ollama/format      0.003s\r\n?       github.com/jmorganca/ollama/vector      [no test files]\r\n?       github.com/jmorganca/ollama/version     [no test files]\r\nok      github.com/jmorganca/ollama/server      0.005s\r\nFAIL\r\n```\r\n\r\nThis commit changes fmt.Println to just fmt.Print so that `go test ./...` passes:\r\n\r\n```\r\n?       github.com/jmorganca/ollama     [no test files]\r\n?       github.com/jmorganca/ollama/api [no test files]\r\n?       github.com/jmorganca/ollama/cmd [no test files]\r\n?       github.com/jmorganca/ollama/llm [no test files]\r\n?       github.com/jmorganca/ollama/llm/llama.cpp       [no test files]\r\n?       github.com/jmorganca/ollama/parser      [no test files]\r\n?       github.com/jmorganca/ollama/progressbar [no test files]\r\nok      github.com/jmorganca/ollama/format      (cached)\r\n?       github.com/jmorganca/ollama/vector      [no test files]\r\n?       github.com/jmorganca/ollama/version     [no test files]\r\nok      github.com/jmorganca/ollama/server      (cached)\r\n```\r\n\r\nI'm on Arch Linux using Go 1.21.1 linux/amd64.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "revise help text",
            "url": "https://github.com/ollama/ollama/pull/706",
            "state": "MERGED",
            "createdAt": "2023-10-05T00:39:02Z",
            "mergedAt": "2023-10-05T18:36:08Z",
            "closedAt": "2023-10-05T18:36:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 42,
            "deletions": 5,
            "body": "Fix up the help text to be more usable.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add homebrew CI configuration",
            "url": "https://github.com/ollama/ollama/pull/709",
            "state": "CLOSED",
            "createdAt": "2023-10-05T12:09:02Z",
            "mergedAt": null,
            "closedAt": "2024-08-22T13:51:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 0,
            "body": "* Add a `build.yml` file for building and testing Ollama with Go 1.20 and 1.21 on both Ubuntu and macOS.\r\n* Add a `homebrew.yml` file for automatically updating the Homebrew package when new releases are tagged. It can be configured by adding a personal access token with \"repo\" and \"workflow\" enabled to the GitHub secrets for this project. More info here: https://github.com/mislav/bump-homebrew-formula-action",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 10
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp gguf to latest",
            "url": "https://github.com/ollama/ollama/pull/710",
            "state": "MERGED",
            "createdAt": "2023-10-05T16:22:46Z",
            "mergedAt": "2023-10-17T20:55:17Z",
            "closedAt": "2023-10-17T20:55:17Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 10,
            "body": "- Update 0001-remove-warm-up-logging.patch\r\n\r\nThere have been some bug fixes and improvements, updating the llama.cpp gguf runner to latest to get these in our next release. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "validate api options fields from map",
            "url": "https://github.com/ollama/ollama/pull/711",
            "state": "MERGED",
            "createdAt": "2023-10-05T18:57:33Z",
            "mergedAt": "2023-10-12T15:18:11Z",
            "closedAt": "2023-10-12T15:18:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 14,
            "deletions": 1,
            "body": "We use a map to set options from the API so that we can see which option fields were specified, otherwise we override default options with zero values. The issue here is that there was no validation that the input option fields were valid, so using an incorrect field by mistake did not return an error.\r\n\r\nNew response:\r\n```\r\ncurl -X 'POST' -d '{\"prompt\":\"hello\", \"model\": \"mistral\", \"options\": {\"seed\": 1234, \"temperature\": 0, \"test\": 1234}}' 'http://127.0.0.1:11434/api/generate'\r\n{\"error\":\"invalid options: test\"}\r\n```\r\n\r\nfrom #694",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "not found error before pulling model",
            "url": "https://github.com/ollama/ollama/pull/718",
            "state": "MERGED",
            "createdAt": "2023-10-06T15:17:05Z",
            "mergedAt": "2023-10-06T20:06:20Z",
            "closedAt": "2023-10-06T20:06:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "When attempting to run a model through the API before pulling it a cryptic \"no such file or directory\" error was returned with the error path. \r\n\r\nImprove this error to suggest pulling the model first, like the CLI does automatically.\r\n```\r\ncurl -X 'POST' -d '{\"prompt\":\"hello\", \"model\": \"mistral\"}' 'http://127.0.0.1:11434/api/generate'\r\n{\"error\":\"model 'mistral' not found, try pulling it first\"}%  \r\n```\r\n\r\nresolves #715 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "relay model runner error message to client",
            "url": "https://github.com/ollama/ollama/pull/720",
            "state": "MERGED",
            "createdAt": "2023-10-06T16:52:04Z",
            "mergedAt": "2023-10-12T15:16:37Z",
            "closedAt": "2023-10-12T15:16:37Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 42,
            "deletions": 15,
            "body": "This got missed in the migration to subprocesses. \r\n\r\nOld error displayed in CLI:\r\n```\r\nfailed to start llama runner\r\n```\r\n\r\nNew error (will relay the actual error from the model runner):\r\n```\r\nError: llama runner failed: out of memory\r\n```\r\n\r\nresolves #630 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add feedback for reading model metadata",
            "url": "https://github.com/ollama/ollama/pull/722",
            "state": "MERGED",
            "createdAt": "2023-10-06T18:22:24Z",
            "mergedAt": "2023-10-06T20:05:33Z",
            "closedAt": "2023-10-06T20:05:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "When creating a model from a large base layer (ex: 70B) reading model metadata is slow since the weights file is large.\r\n\r\nWithout feedback here the creation is in the \"looking for model\" stage for a long time, which makes it look like something has gone wrong.\r\n\r\nNew behavior:\r\n```\r\nparsing modelfile\r\nlooking for model\r\n\u280b reading model metadata\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Documenting how to view `Modelfile`s",
            "url": "https://github.com/ollama/ollama/pull/723",
            "state": "MERGED",
            "createdAt": "2023-10-06T20:27:54Z",
            "mergedAt": "2023-11-20T20:24:29Z",
            "closedAt": "2023-11-20T20:24:29Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 33,
            "deletions": 0,
            "body": "Upstreaming info from https://github.com/jmorganca/ollama/issues/685:\r\n\r\n- Documented tags page in https://ollama.ai/library\r\n- Documented `ollama show --modelfile`",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "improve vram safety with 5% vram memory buffer",
            "url": "https://github.com/ollama/ollama/pull/724",
            "state": "MERGED",
            "createdAt": "2023-10-06T20:39:38Z",
            "mergedAt": "2023-10-10T20:16:09Z",
            "closedAt": "2023-10-10T20:16:09Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 7,
            "body": "In testing how much VRAM should be allocated we typically used a model which could be entirely loaded into VRAM. This masked an issue when a model is larger than the available VRAM it is possible to consume all available VRAM and fail with an error:\r\n```\r\nError: llama runner failed: out of memory\r\n```\r\n\r\nThis change leaves a 10% buffer on available VRAM to prevent running out of memory.\r\n\r\nTested on a T4:\r\n- `llama2:7b`: easily offloads all layers to GPU\r\n- `llama2:13b`: easily offloads all layers to GPU\r\n- `llama2:70b`: offloaded 29 layers to GPU, was slow but did not run out of memory on load (as it did before)\n\n\nResolves #725",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Update api.md",
            "url": "https://github.com/ollama/ollama/pull/741",
            "state": "MERGED",
            "createdAt": "2023-10-09T15:09:18Z",
            "mergedAt": "2023-10-09T20:01:47Z",
            "closedAt": "2023-10-09T20:01:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Avoid triple ticks in visual editor and also copied in clipboard.\r\n\r\n![CleanShot 2023-10-09 at 17 07 51@2x](https://github.com/jmorganca/ollama/assets/12672541/be329fc6-23e0-4897-a611-2ce42c3b7cf0)\r\n\r\n![CleanShot 2023-10-09 at 17 08 51@2x](https://github.com/jmorganca/ollama/assets/12672541/a14daee3-9558-4be7-9b5d-b90b1ad8dcd2)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "handle upstream proxies",
            "url": "https://github.com/ollama/ollama/pull/743",
            "state": "MERGED",
            "createdAt": "2023-10-09T18:51:18Z",
            "mergedAt": "2023-10-10T16:59:06Z",
            "closedAt": "2023-10-10T16:59:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 58,
            "deletions": 45,
            "body": "`http.ProxyFromEnvironment` returns the appropriate `*_PROXY` for the request. e.g. `HTTP_PROXY` for `http://` requests, `HTTPS_PROXY` for `https://` requests.\r\n\r\nResolves #729",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't assume download has started if cancelled during preparation",
            "url": "https://github.com/ollama/ollama/pull/747",
            "state": "MERGED",
            "createdAt": "2023-10-10T03:18:35Z",
            "mergedAt": "2023-10-10T17:12:29Z",
            "closedAt": "2023-10-10T17:12:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Discards download progress if cancelled (e.g. by `ctrl+c`) while preparing to download",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "concurrent uploads",
            "url": "https://github.com/ollama/ollama/pull/750",
            "state": "MERGED",
            "createdAt": "2023-10-10T20:48:46Z",
            "mergedAt": "2023-11-01T22:00:01Z",
            "closedAt": "2023-11-01T22:00:01Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 4
            },
            "additions": 287,
            "deletions": 189,
            "body": "follow a similar pattern as downloads with some key differences:\r\n\r\n1. uploads parts are serialized based on a nextURL channel which informs the next part where to upload to\r\n2. redirects send to nextURL before following the redirect which allows parts to be uploaded concurrently\r\n3. progress is tracked with blobUploadWriter which tracks how many bytes per part is written. this is necessary for redirect which partially reads the initial request before following the redirect; it needs to rewind the overall progress\r\n\r\nTODO: \r\n- [ ] verify calculated md5 with etag and retry the part if it differs",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Proposal: Add zero-configuration networking support via zeroconf",
            "url": "https://github.com/ollama/ollama/pull/751",
            "state": "CLOSED",
            "createdAt": "2023-10-10T21:15:24Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T01:49:21Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 78,
            "deletions": 1,
            "body": "This proposal allows the Ollama service to be made discoverable via [zero configuration networking](https://en.wikipedia.org/wiki/Zero-configuration_networking) across the user's local network via Bonjour/Zeroconf/Avahi aka [Multicast DNS (mDNS)](https://en.wikipedia.org/wiki/Multicast_DNS) using the [`zeroconf` Go library](https://github.com/grandcat/zeroconf)so that other clients can connect to and use it without needing to know the host's IP address.\r\n\r\nThis opens up many different applications for consuming Ollama models served by other network devices.\r\n\r\nMy particular use case is to add support for Network Discoverable Ollama models to an [Obsidian Plugin that I maintain](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant) so that users won't have to configure IP addresses in Obsidian or update them if/when IP addresses on their local network change (and also won't have to get into configuring a static IP for the device that is serving their local models).\r\n\r\n**Note**: Network discovery is entirely opt-in via the `OLLAMA_DISCOVERY` environment variable flag being set to `ENABLED` and will automatically update the `OLLAMA_HOST` to `0.0.0.0` (the demo GIF was recorded with an earlier iteration of this PR that also required the user to manually set the host IP).\r\n\r\n## Demo\r\n\r\n![Ollama-Network-Discovery-Demo](https://github.com/jmorganca/ollama/assets/1667415/5122405e-5d1e-423a-9279-1c6efcbbcd8f)\r\n\r\n**Note**: To test this functionality, I created a simple Node.js script on another machine on my network and had it use the [`bonjour`](https://www.npmjs.com/package/bonjour) package to search for a service with the name `OllamaProvider`. It gets the IP address and port associated with that service and then makes requests to it. I only showed the IP address at the beginning of the GIF to emphasize that the requests are coming from a different machine.\r\n\r\nIt also adds a menu entry with the service name if Network Discovery has been enabled.\r\n\r\n![Screen Shot 2023-10-10 at 5 09 30 PM](https://github.com/jmorganca/ollama/assets/1667415/983cccd6-370f-4ab6-b7a2-8794ed841c7b)\r\n\r\n## Instructions for Testing\r\n\r\n1. Checkout this PR: `gh pr checkout https://github.com/jmorganca/ollama/pull/751`\r\n2. Generate: `go generate ./...`\r\n3. Build: `go build .`\r\n4. Build and run the App:\r\n   1. `cd ./app`\r\n   2. `npm install`\r\n   3. `OLLAMA_DISCOVERY=ENABLED npm start`\r\n5. Search for and connect to your Network Service (_I've provided an example discovery script below_)\r\n\r\n### Network Discovery Script\r\n\r\n```js\r\n// you'll need to `npm install bonjour` first\r\n// it's recommended to put this in a subdirectory\r\n// and run `npm init` before installing packages\r\nconst bonjour = require(\"bonjour\")();\r\n\r\n// this demo script can be run via node to find\r\n// and connect to a Network Service with the name\r\n//defined in OLLAMA_SERVICE_NAME\r\nconst OLLAMA_SERVICE_NAME = 'OllamaProvider'\r\n\r\n// iterate through services\r\nbonjour.find({}, (service) => {\r\n  if (service.name === OLLAMA_SERVICE_NAME) {\r\n    const address = service.addresses[0];\r\n    const port = service.port;\r\n\r\n    const baseUrl = new URL(address);\r\n\r\n    baseUrl.port = port;\r\n\r\n    const modelsUrl = new URL(\"/api/tags\", baseUrl);\r\n\r\n    // get available models\r\n    fetch(modelsUrl)\r\n      .then(async (response) => await response.json())\r\n      .then((response) => {\r\n        console.log(response);\r\n      })\r\n      .catch((error) => {\r\n        console.error(error);\r\n      })\r\n  }\r\n});\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "prevent waiting on exited command",
            "url": "https://github.com/ollama/ollama/pull/752",
            "state": "MERGED",
            "createdAt": "2023-10-10T22:03:14Z",
            "mergedAt": "2023-10-11T16:32:14Z",
            "closedAt": "2023-10-11T16:32:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 27,
            "body": "this fixes the issue in main from #724 that causes the server to wait on a subprocess which has already exited ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "rename the examples to be more descriptive",
            "url": "https://github.com/ollama/ollama/pull/753",
            "state": "MERGED",
            "createdAt": "2023-10-11T00:40:54Z",
            "mergedAt": "2023-10-12T18:24:12Z",
            "closedAt": "2023-10-12T18:24:12Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 51
            },
            "additions": 1347,
            "deletions": 23,
            "body": "also add a few readmes. ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "cleanup format time",
            "url": "https://github.com/ollama/ollama/pull/757",
            "state": "MERGED",
            "createdAt": "2023-10-11T18:06:06Z",
            "mergedAt": "2023-10-11T18:12:29Z",
            "closedAt": "2023-10-11T18:12:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 17,
            "deletions": 157,
            "body": "only `HumanTime` is actually being used",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "deprecate modelfile embed command",
            "url": "https://github.com/ollama/ollama/pull/759",
            "state": "MERGED",
            "createdAt": "2023-10-11T19:53:25Z",
            "mergedAt": "2023-10-16T15:07:37Z",
            "closedAt": "2023-10-16T15:07:37Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 9
            },
            "additions": 19,
            "deletions": 301,
            "body": "Embeddings in Modelfiles are a convenient idea, allowing the model to be packaged with embeddings created for it specifically, but the user-experience of this implementation isn't up to par.\r\n\r\nThis change leaves the `/embed` endpoint, but deprecates `EMBED` in the modelfile.\r\n- Ollama doesn't have any models designed for embedding generation\r\n- Generating embeddings from modelfile is slow, error prone, and only supports single line text inputs\r\n- Retrieving embeddings was too slow to be useful, and there was no mechanism to connect an external vector database\r\n\r\nInstead of using the Modelfile the right way to do this is with an external tool such as PrivateGPT or LlamaIndex that uses Ollama as the runner.\r\n\r\nNew behavior:\r\nOn create a modelfile with the embed command:\r\n```\r\n$ ollama create embed-test -f /Users/bruce/modelfiles/embedded/Modelfile\r\n\u280b parsing modelfile  Error: deprecated command: EMBED\r\n```\r\n\r\nOn running a modelfile with the embed command:\r\n```\r\n2023/10/11 15:46:52 images.go:190: WARNING: model contains embeddings, but embeddings in modelfiles have been deprecated and will be ignored.\r\n```\r\n\r\nOn running a modelfile with the embed in the template:\r\n```\r\n$ ollama run embed-test\r\n\u2826   Error: template: :5:7: executing \"\" at <.Embed>: can't evaluate field Embed in type struct { First bool; System string; Prompt string; Context []int }\r\n```",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Mxyng/more downloads",
            "url": "https://github.com/ollama/ollama/pull/760",
            "state": "MERGED",
            "createdAt": "2023-10-11T20:51:53Z",
            "mergedAt": "2023-10-11T21:33:10Z",
            "closedAt": "2023-10-11T21:33:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 70,
            "deletions": 33,
            "body": "minor tweaks to downloads",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix memory check",
            "url": "https://github.com/ollama/ollama/pull/768",
            "state": "MERGED",
            "createdAt": "2023-10-12T16:34:57Z",
            "mergedAt": "2023-10-16T19:42:41Z",
            "closedAt": "2023-10-16T19:42:41Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 50,
            "deletions": 50,
            "body": "only do a system memory check on macos which has unified memory. on other platforms, rely on the vram offloading",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix download",
            "url": "https://github.com/ollama/ollama/pull/770",
            "state": "MERGED",
            "createdAt": "2023-10-12T19:54:13Z",
            "mergedAt": "2023-10-12T19:56:43Z",
            "closedAt": "2023-10-12T19:56:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "`inner` is a context that needs to be passed to functions inside the goroutine otherwise `g.Wait` may wait indefinitely even on error",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "linux: add user to the `ollama` group on install",
            "url": "https://github.com/ollama/ollama/pull/772",
            "state": "MERGED",
            "createdAt": "2023-10-12T21:22:16Z",
            "mergedAt": "2023-10-23T21:06:31Z",
            "closedAt": "2023-10-23T21:06:31Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "- Run the ollama system service as the current user\r\n \r\nresolves #613 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add how to quantize doc",
            "url": "https://github.com/ollama/ollama/pull/773",
            "state": "MERGED",
            "createdAt": "2023-10-12T22:35:56Z",
            "mergedAt": "2023-10-14T15:29:39Z",
            "closedAt": "2023-10-14T15:29:39Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 2
            },
            "additions": 98,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add version api and show server version in cli",
            "url": "https://github.com/ollama/ollama/pull/774",
            "state": "MERGED",
            "createdAt": "2023-10-12T23:54:56Z",
            "mergedAt": "2023-12-06T21:22:56Z",
            "closedAt": "2023-12-06T21:22:56Z",
            "reviews": {
                "totalCount": 21
            },
            "files": {
                "totalCount": 3
            },
            "additions": 71,
            "deletions": 30,
            "body": "some minor refactor of the cmd package\r\n\r\nExample: server and client are the same version\r\n```\r\n$ ollama --version\r\nYour ollama version 0.0.0\r\n```\r\n\r\nExample: server and client have different versions\r\n```\r\n$ ollama --version\r\nYour ollama version 99.99.99999\r\nWarning: Your client version is 0.0.0\r\n```\r\n\r\nExample: server is not accessible\r\n```\r\n$ ollama --version\r\nWarning: Your server is not accessible\r\nYour client version is 0.0.0\r\n```\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "show request to server rather than local check",
            "url": "https://github.com/ollama/ollama/pull/778",
            "state": "MERGED",
            "createdAt": "2023-10-13T15:14:41Z",
            "mergedAt": "2023-10-16T21:27:25Z",
            "closedAt": "2023-10-16T21:27:25Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 1,
            "body": "The show command should send a request to the server, rather than making a direct call to the function locally.\r\n\r\nresolces #776 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "improve api error handling",
            "url": "https://github.com/ollama/ollama/pull/781",
            "state": "MERGED",
            "createdAt": "2023-10-13T17:54:56Z",
            "mergedAt": "2023-10-13T20:57:10Z",
            "closedAt": "2023-10-13T20:57:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 16,
            "body": "- remove new lines from llama.cpp error messages relayed to client\r\n- check api option types and return error on wrong type\r\n- change num layers from 95% VRAM to 92% VRAM",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix link to api docs",
            "url": "https://github.com/ollama/ollama/pull/782",
            "state": "CLOSED",
            "createdAt": "2023-10-13T18:06:34Z",
            "mergedAt": null,
            "closedAt": "2023-10-21T16:00:36Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "It seems that the link to the API docs doesn't work for some reason. After some investigation, I think the GitHub markdown fails to match the relative path for `docs/api.md` because of the quote block.\r\n\r\nI submitted this PR in case you want a quick fix until you find out what exactly is wrong.\r\n\r\nThanks!",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: offloading on low end GPUs",
            "url": "https://github.com/ollama/ollama/pull/783",
            "state": "MERGED",
            "createdAt": "2023-10-13T20:08:46Z",
            "mergedAt": "2023-10-13T21:36:44Z",
            "closedAt": "2023-10-13T21:36:44Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 33,
            "deletions": 15,
            "body": "Fixes two issues when using low end GPUs:\r\n\r\nGPUs with low VRAM are disproportionately affected by overhead when offloading so any device that has less than 2GB VRAM will be exclusively CPU unless overwritten by num_gpu.\r\n\r\nA CUDA-enabled runner will still offload to GPU even if num_gpu is 0. This is problematic when the GPU doesn't support a compatible version of CUDA. In this case, select the CPU runner instead.\r\n\r\nCaveat: for MacOS (darwin) `go generate` only builds Metal on ARM so it shouldn't be marked as `Accelerated` since there's no fallback",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "check for newer updates",
            "url": "https://github.com/ollama/ollama/pull/784",
            "state": "MERGED",
            "createdAt": "2023-10-13T21:13:04Z",
            "mergedAt": "2023-10-13T21:29:46Z",
            "closedAt": "2023-10-13T21:29:46Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 1
            },
            "additions": 34,
            "deletions": 4,
            "body": "Check if there are newer version of ollama updates available. This prevents multiple updates in a row.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "check update response",
            "url": "https://github.com/ollama/ollama/pull/785",
            "state": "MERGED",
            "createdAt": "2023-10-13T22:04:48Z",
            "mergedAt": "2023-10-13T22:05:46Z",
            "closedAt": "2023-10-13T22:05:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: print version on start",
            "url": "https://github.com/ollama/ollama/pull/787",
            "state": "MERGED",
            "createdAt": "2023-10-13T23:11:39Z",
            "mergedAt": "2023-10-16T16:59:30Z",
            "closedAt": "2023-10-16T16:59:30Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add oterm to community integrations",
            "url": "https://github.com/ollama/ollama/pull/794",
            "state": "MERGED",
            "createdAt": "2023-10-15T21:23:55Z",
            "mergedAt": "2023-10-16T22:51:55Z",
            "closedAt": "2023-10-16T22:51:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Hey there!\r\nI just published  [oterm](https://github.com/ggozad/ollama) a text-based terminal client for Ollama. \r\nIt features:\r\n* intuitive and simple terminal UI, no need to run servers, frontends, just type oterm in your terminal.\r\n* multiple persistent chat sessions, stored together with the context embeddings in sqlite.\r\n* can use any of the models you have pulled in Ollama, or your own custom models.\r\n\r\nThis PR adds it to the community integrations",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix JSON Marshal Escaping for Special Characters",
            "url": "https://github.com/ollama/ollama/pull/799",
            "state": "MERGED",
            "createdAt": "2023-10-16T09:27:58Z",
            "mergedAt": "2023-10-17T15:46:02Z",
            "closedAt": "2023-10-17T15:46:02Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 4,
            "body": "Fixed the json.Marshal() behavior in llama.go to prevent automatic escaping of special characters like < and >. This ensures templates with these characters are correctly represented in the JSON output. Addresses issue https://github.com/jmorganca/ollama/issues/798",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "API docs link fix",
            "url": "https://github.com/ollama/ollama/pull/800",
            "state": "CLOSED",
            "createdAt": "2023-10-16T09:28:16Z",
            "mergedAt": null,
            "closedAt": "2023-10-21T16:00:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "For some reason, the relative API docs link is broken (api is a particular path in Github). \r\nReplaced the API docs link in README.md with the absolute path.\r\n\r\nFixes issue #802.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add ellama community integration",
            "url": "https://github.com/ollama/ollama/pull/801",
            "state": "MERGED",
            "createdAt": "2023-10-16T09:44:09Z",
            "mergedAt": "2023-10-16T22:51:25Z",
            "closedAt": "2023-10-16T22:51:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi.\r\nThank you for this cool project.\r\nIn this PR new emacs package added to community integrations. [ellama](https://github.com/s-kostyaev/ellama) supports streaming output and can be installed from [MELPA](https://melpa.org/#/getting-started)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: regression unsupported metal types",
            "url": "https://github.com/ollama/ollama/pull/809",
            "state": "MERGED",
            "createdAt": "2023-10-16T21:40:27Z",
            "mergedAt": "2023-10-17T15:40:40Z",
            "closedAt": "2023-10-17T15:40:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 4,
            "body": "omitting `--n-gpu-layers` means use metal on macos which isn't correct since ollama uses `num_gpu=0` to explicitly disable gpu for file types that are not implemented in metal",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update install.sh",
            "url": "https://github.com/ollama/ollama/pull/810",
            "state": "MERGED",
            "createdAt": "2023-10-16T21:44:08Z",
            "mergedAt": "2023-10-16T22:50:57Z",
            "closedAt": "2023-10-16T22:50:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "otherwise, the `ARCH` variable is unbound in `*)` ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add OllamaSharp for .NET community integration",
            "url": "https://github.com/ollama/ollama/pull/811",
            "state": "MERGED",
            "createdAt": "2023-10-16T21:48:38Z",
            "mergedAt": "2023-10-17T15:31:48Z",
            "closedAt": "2023-10-17T15:31:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Thank you so much for your efforts to build such an amazing piece of software.\r\nI love Ollama and use it every day and I also plan to integrate it into further .NET applications. That's why I published [OllamaSharp](https://github.com/awaescher/OllamaSharp) as nuget package so everyone in .NET land can talk to the Ollama API easily.\r\n\r\nThanks again and keep up the great work.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: wrong format string type",
            "url": "https://github.com/ollama/ollama/pull/812",
            "state": "MERGED",
            "createdAt": "2023-10-16T23:14:51Z",
            "mergedAt": "2023-10-17T15:40:49Z",
            "closedAt": "2023-10-17T15:40:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor llm/llama.go",
            "url": "https://github.com/ollama/ollama/pull/813",
            "state": "MERGED",
            "createdAt": "2023-10-16T23:41:07Z",
            "mergedAt": "2023-10-17T21:05:58Z",
            "closedAt": "2023-10-17T21:05:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 38,
            "deletions": 90,
            "body": "- remove unused struct GenerationSettings\r\n- remove a layer of indirection for prediction request. it's only used in one place so it's nicer to have a `map[string]any` instead of a struct\r\n- nest `Timings` for similar reasons\r\n- use `CutPrefix` to both check for `data:` and trimming",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ROCm support",
            "url": "https://github.com/ollama/ollama/pull/814",
            "state": "CLOSED",
            "createdAt": "2023-10-17T00:51:27Z",
            "mergedAt": null,
            "closedAt": "2023-12-24T02:18:17Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 10
            },
            "additions": 264,
            "deletions": 59,
            "body": "#667 got closed during a bad rebase attempt. This should be just about the minimum I can come up with to use build tags to switch between ROCm and CUDA, as well as docs for how to build it. The existing dockerfiles are updated so they do not break.\r\n\r\nPlease let me know @jmorganca @mxyng @BruceMacD if you'd like this in a different approach or something, or if you don't want to do this. Closes #738. Will post test results for GGML and GGUF files.",
            "participants": {
                "totalCount": 20
            },
            "comments": {
                "totalCount": 73
            }
        }
    },
    {
        "node": {
            "title": "Fix a typo",
            "url": "https://github.com/ollama/ollama/pull/818",
            "state": "MERGED",
            "createdAt": "2023-10-17T12:58:34Z",
            "mergedAt": "2023-10-17T13:00:16Z",
            "closedAt": "2023-10-17T13:00:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The word in the JSON response is `embedding` not `embeddings`:\r\n\r\n```sh\r\ncurl -X POST http://localhost:11434/api/embeddings -d '{\r\n  \"model\": \"codeup:latest\",\r\n  \"prompt\": \"Here is an article about llamas...\"\r\n}'\r\n```\r\n```json\r\n{\"embedding\":[-1.3911274671554565,0.045920971781015396,1.0808414220809937,0.058245059102773666,-0.27932560443878174,-0.1968495100736618,1.1102352142333984,0.9859555959701538,0.9562729597091675,-0.19171573221683502,0.16944187879562378,-0.5829504132270813,0.19427405 ...\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix /api/tags for no models.",
            "url": "https://github.com/ollama/ollama/pull/822",
            "state": "MERGED",
            "createdAt": "2023-10-17T17:20:38Z",
            "mergedAt": "2023-10-18T16:34:01Z",
            "closedAt": "2023-10-18T16:34:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "When the `.ollama` folder is broken, or no models exist, `/api/tags` returns `{models: null}` instead of the expected `{models: []}`.\r\n\r\nPlease review the change as minor as it is, as my experience with Go is close to nil.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix MB VRAM log output",
            "url": "https://github.com/ollama/ollama/pull/824",
            "state": "MERGED",
            "createdAt": "2023-10-17T18:48:25Z",
            "mergedAt": "2023-10-17T19:35:16Z",
            "closedAt": "2023-10-17T19:35:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This was logging the bytes:\r\n```\r\n15651045376 MiB VRAM available, loading up to 29 GPU layers\r\n```\r\n\r\nFix:\r\n```\r\n14926 MB VRAM available, loading up to 29 GPU layers\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "relay CUDA errors to the client",
            "url": "https://github.com/ollama/ollama/pull/825",
            "state": "MERGED",
            "createdAt": "2023-10-17T20:13:05Z",
            "mergedAt": "2023-10-18T19:36:57Z",
            "closedAt": "2023-10-18T19:36:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 35,
            "deletions": 12,
            "body": "When the llama.cpp runner failed with CUDA error the error message was not relayed to the client. Instead the client would only see an EOF error. Update the llama.cpp subprocess log monitor to capture CUDA errors and relay them to the client.\r\n\r\nBefore:\r\n```\r\nError: error reading llm response: unexpected EOF\r\n```\r\n\r\nAfter:\r\n```\r\nError: llama runner exited, you may not have enough available memory to run this model\r\n```\r\nor the actual error is available",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "show: no template system if empty",
            "url": "https://github.com/ollama/ollama/pull/826",
            "state": "MERGED",
            "createdAt": "2023-10-17T22:26:55Z",
            "mergedAt": "2023-10-18T20:11:10Z",
            "closedAt": "2023-10-18T20:11:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "This prevents show outputs like this:\r\n\r\n```\r\nollama run mistral\r\n>>> /show modelfile\r\n# Modelfile generated by \"ollama show\"\r\n# To build a new Modelfile based on this one, replace the FROM line with:\r\n# FROM mistral:latest\r\n\r\nFROM registry.ollama.ai/library/mistral:latest\r\nTEMPLATE \"\"\"[INST] {{ .Prompt }} [/INST]\r\n\"\"\"\r\nSYSTEM \"\"\"\"\"\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "model: native gotemplate adapter template",
            "url": "https://github.com/ollama/ollama/pull/827",
            "state": "MERGED",
            "createdAt": "2023-10-17T22:29:11Z",
            "mergedAt": "2023-10-18T20:11:25Z",
            "closedAt": "2023-10-18T20:11:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 3,
            "body": "Use gotemplate range instead of string concatenation ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "image: show parameters",
            "url": "https://github.com/ollama/ollama/pull/828",
            "state": "MERGED",
            "createdAt": "2023-10-17T22:45:17Z",
            "mergedAt": "2023-10-19T16:31:31Z",
            "closedAt": "2023-10-19T16:31:31Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 19,
            "deletions": 39,
            "body": "map options back into an any slice so the template can do the work of stringify the values",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "added python rag news summary",
            "url": "https://github.com/ollama/ollama/pull/829",
            "state": "MERGED",
            "createdAt": "2023-10-17T23:42:28Z",
            "mergedAt": "2023-10-21T04:03:16Z",
            "closedAt": "2023-10-21T04:03:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 225,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix Issue with Leading Whitespaces in Decoded Context",
            "url": "https://github.com/ollama/ollama/pull/833",
            "state": "MERGED",
            "createdAt": "2023-10-18T06:41:47Z",
            "mergedAt": "2023-10-18T20:52:48Z",
            "closedAt": "2023-10-18T20:52:48Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "This Pull Request addresses a bug in the Decode function where leading whitespaces are not properly removed from the decoded context.\r\n\r\nI have tested the behavior both before and after making this change:\r\n\r\n- Before: The leading whitespaces were present when using the context.\r\n- After: The leading whitespaces were properly removed.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "do not reload the running llm when runtime params change",
            "url": "https://github.com/ollama/ollama/pull/840",
            "state": "MERGED",
            "createdAt": "2023-10-18T18:10:23Z",
            "mergedAt": "2023-10-19T14:39:59Z",
            "closedAt": "2023-10-19T14:39:59Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 66,
            "deletions": 86,
            "body": "- only reload the running llm if the model has changed, or the options for loading the running model have changed\r\n- rename loaded llm to runner to differentiate from loaded model image\r\n- remove logic which keeps the first system prompt in the generation context\r\n\r\nSay I have 2 models, both are based on llama2, but they have different prompts and runtime parameters.\r\n```\r\nFROM llama2\r\nTEMPLATE \"\"\"\r\nyou are a dog\r\n\"\"\"\r\nPARAMETER stop \"human:\"\r\n```\r\nand\r\n```\r\nFROM llama2\r\nTEMPLATE \"\"\"\r\nyou are a cat\r\n\"\"\"\r\nPARAMETER stop \"bob:\"\r\n```\r\nIf I am building something that swaps requests between these models a lot our current logic will re-load the models every time, even though the only thing changing is the prompt template or runtime option is changing.\r\n\r\nThis change compares only model fields which require the model to be re-loaded before swapping the loaded model.\r\n\r\nResolves #337 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cleanup: command args",
            "url": "https://github.com/ollama/ollama/pull/841",
            "state": "MERGED",
            "createdAt": "2023-10-18T19:05:33Z",
            "mergedAt": "2023-10-19T18:22:40Z",
            "closedAt": "2023-10-19T18:22:40Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 7,
            "body": "A number of subcommands incorrectly set `MinimumNArgs` instead of `ExactArgs` which leads to confusion.\r\n\r\nRelated #803 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "#790 improve readme",
            "url": "https://github.com/ollama/ollama/pull/842",
            "state": "CLOSED",
            "createdAt": "2023-10-18T19:18:44Z",
            "mergedAt": null,
            "closedAt": "2023-11-29T21:30:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 60,
            "deletions": 1,
            "body": "As promised, an updated README that explains how to force lower memory usage.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "basic request validation",
            "url": "https://github.com/ollama/ollama/pull/843",
            "state": "MERGED",
            "createdAt": "2023-10-18T23:13:14Z",
            "mergedAt": "2023-10-19T16:30:45Z",
            "closedAt": "2023-10-19T16:30:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 109,
            "deletions": 30,
            "body": "- API returns `{\"error\": \"EOF\"}` when request is empty\r\n- Most handlers pass request fields without checking if they're empty produces bad errors\r\n- `created_at` on an empty generate request isn't set so it incorrectly shows `0001-01-01T00:00:00Z`\r\n- Create's `workDir` isn't used after #759 \r\n\r\nNote: there's an inconsistency in naming the key for model names. Some requests (Generate, Embedding) use `model` while others (Pull, Push, Create) use `name`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add error for `falcon` and `starcoder` vocab compatibility",
            "url": "https://github.com/ollama/ollama/pull/844",
            "state": "MERGED",
            "createdAt": "2023-10-19T16:14:03Z",
            "mergedAt": "2023-10-19T16:18:31Z",
            "closedAt": "2023-10-19T16:18:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 4,
            "body": "A recent update to llama.cpp introduced a compatibility issue with `starcoder` and `falcon` model families, this adds a friendly error message hint when these models fail to load",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "new readline library",
            "url": "https://github.com/ollama/ollama/pull/847",
            "state": "MERGED",
            "createdAt": "2023-10-19T21:09:43Z",
            "mergedAt": "2023-10-25T23:41:18Z",
            "closedAt": "2023-10-25T23:41:19Z",
            "reviews": {
                "totalCount": 20
            },
            "files": {
                "totalCount": 11
            },
            "additions": 972,
            "deletions": 86,
            "body": "This is simplified version of the readline library which cuts out a lot of the complexity of the version that we were using. There's still a few things to add like \"history\" and getting the multi-line prompts working correctly, but most (many?) things should be more or less working, including:\r\n\r\n* Each of the Ctrl-? chars (Ctrl-d, Ctrl-k, Ctrl-c, Ctrl-u, Ctrl-a, Ctrl-e, Ctrl-l, etc.)\r\n* Line wrap with backspace/arrow keys\r\n* Entering/exiting raw mode\r\n\r\nWould love some feedback if people could try it out.",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update golang.org/x/net fixes CVE-2023-3978,CVE-2023-39325,CVE-2023-4\u2026",
            "url": "https://github.com/ollama/ollama/pull/855",
            "state": "MERGED",
            "createdAt": "2023-10-20T16:32:34Z",
            "mergedAt": "2023-10-25T23:17:25Z",
            "closedAt": "2023-10-25T23:17:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 15,
            "body": "update golang.org/x/net fixes CVE-2023-3978,CVE-2023-39325,CVE-2023-44487",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix: ollama host for hostname",
            "url": "https://github.com/ollama/ollama/pull/859",
            "state": "MERGED",
            "createdAt": "2023-10-20T18:34:08Z",
            "mergedAt": "2023-10-20T18:40:56Z",
            "closedAt": "2023-10-20T18:40:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "This fixes the client when setting `OLLAMA_HOST` to just the hostname, e.g. `OLLAMA_HOST=myhost`. `OLLAMA_HOST=myhost:11434` currently works as does `OLLAMA_HOST=127.0.0.1` and `OLLAMA_HOST=127.0.0.1:11434`",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: nil pointer dereference",
            "url": "https://github.com/ollama/ollama/pull/863",
            "state": "MERGED",
            "createdAt": "2023-10-20T23:55:51Z",
            "mergedAt": "2023-10-21T00:23:38Z",
            "closedAt": "2023-10-21T00:23:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "if the token request returns an error, e.g. the token server isn't running, resp is dereferenced and panics",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update runtime options",
            "url": "https://github.com/ollama/ollama/pull/864",
            "state": "MERGED",
            "createdAt": "2023-10-21T00:24:42Z",
            "mergedAt": "2023-10-21T01:17:14Z",
            "closedAt": "2023-10-21T01:17:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "SetOptions doesn't differentiate between boot or runtime options but that's not relevant. By the time SetOptions is called, the LLM is already running so any boot option updates will be ignored",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added a minimalist React UI for Ollama models to ReadME.md",
            "url": "https://github.com/ollama/ollama/pull/870",
            "state": "MERGED",
            "createdAt": "2023-10-21T10:50:25Z",
            "mergedAt": "2023-10-23T14:44:39Z",
            "closedAt": "2023-10-23T14:44:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I discussed it a few times in the discord, and a few people seem to be using it, so it would be good to add.\r\n\r\nDemo video:\r\nhttps://github.com/jmorganca/ollama/assets/35015261/d50f7036-cdf2-44ed-9bb0-fdbed6a4ec66\r\n\r\nI'll be maintaining/improving it a lot over the coming weeks, and some contributors reached out to get involved.\r\n\r\nIt can now handle markdown, etc. And continues to improve beyond the initial demo.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "fix: Add support for legacy CPU (no AVX2/FMA) on Linux",
            "url": "https://github.com/ollama/ollama/pull/871",
            "state": "CLOSED",
            "createdAt": "2023-10-21T17:47:40Z",
            "mergedAt": null,
            "closedAt": "2023-10-27T19:31:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 27,
            "deletions": 0,
            "body": "Fixes the illegal instruction error when running with CPU without AVX2 or FMA, by building another set of ollama runner with `-DLLAMA_AVX2=off -DLLAMA_FMA=off`.\r\n\r\nBy default, upon running the cmake for ggml/gguf, it will have these arguments set to ON. Setting it to OFF, allows older CPU that don't have these instruction to be able to run the llama.cpp.  \r\n\r\nfixes #644\r\n\r\nSome sources for the AVX2 and FMA compatibility:\r\n- [CPUs_with_AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2)\r\n- [CPUs_with_FMA3](https://en.wikipedia.org/wiki/FMA_instruction_set#CPUs_with_FMA3)",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "fix readme for linux : port address already in use",
            "url": "https://github.com/ollama/ollama/pull/872",
            "state": "CLOSED",
            "createdAt": "2023-10-21T19:32:37Z",
            "mergedAt": null,
            "closedAt": "2023-10-26T17:47:43Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "If user is installing Ollama for the first time/fresh install then Ollama server is started automatically. So when you try \r\n```\r\nollama serve\r\n```\r\nthen it throws error - 127.0.0.1:11434: bind: address already in use \r\nSo instead of running this command user can skip to running model\r\n\r\nThis PR patches the corresponding fixes in documentation for linux\r\nFixes: https://github.com/jmorganca/ollama/issues/707",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "ggufv3",
            "url": "https://github.com/ollama/ollama/pull/881",
            "state": "MERGED",
            "createdAt": "2023-10-23T16:39:33Z",
            "mergedAt": "2023-10-23T17:50:45Z",
            "closedAt": "2023-10-23T17:50:45Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 36,
            "deletions": 34,
            "body": "ggufv3 adds support for big endianness, mainly for s390x architecture. while that's not currently supported for ollama, the change is simple.\r\n\r\nloosen version check to be more forward compatible. unless specified, gguf versions other v1 will be decoded into v2.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update default log target",
            "url": "https://github.com/ollama/ollama/pull/883",
            "state": "MERGED",
            "createdAt": "2023-10-23T17:45:10Z",
            "mergedAt": "2023-10-23T17:50:30Z",
            "closedAt": "2023-10-23T17:50:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 29,
            "deletions": 29,
            "body": "The original default writes to a file in the process's working directory which isn't ideal",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "bump submodules",
            "url": "https://github.com/ollama/ollama/pull/884",
            "state": "MERGED",
            "createdAt": "2023-10-23T18:23:31Z",
            "mergedAt": "2023-10-23T18:27:38Z",
            "closedAt": "2023-10-23T18:27:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "pin to 9e70cc03229df19ca2d28ce23cc817198f897278 for now since 438c2ca83045a00ef244093d27e9ed41a8cb4ea9 is breaking",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "during linux install add the ollama service user to the current resolved user's group",
            "url": "https://github.com/ollama/ollama/pull/886",
            "state": "CLOSED",
            "createdAt": "2023-10-23T21:23:35Z",
            "mergedAt": null,
            "closedAt": "2023-10-24T17:52:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "document linux install inline with the script file",
            "url": "https://github.com/ollama/ollama/pull/890",
            "state": "MERGED",
            "createdAt": "2023-10-24T16:03:12Z",
            "mergedAt": "2023-10-25T14:58:17Z",
            "closedAt": "2023-10-25T14:58:17Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 56,
            "deletions": 3,
            "body": "Shell scripts are dense and hard to read. Document explicitly what the installation script is doing so that enquiring users can see exactly what changes are being made to their system.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update faq",
            "url": "https://github.com/ollama/ollama/pull/893",
            "state": "MERGED",
            "createdAt": "2023-10-24T17:54:52Z",
            "mergedAt": "2023-10-24T23:02:35Z",
            "closedAt": "2023-10-24T23:02:35Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 30,
            "deletions": 2,
            "body": "Resolves #885 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Linux uninstall instructions",
            "url": "https://github.com/ollama/ollama/pull/894",
            "state": "MERGED",
            "createdAt": "2023-10-24T18:04:51Z",
            "mergedAt": "2023-10-24T18:07:05Z",
            "closedAt": "2023-10-24T18:07:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 20,
            "deletions": 0,
            "body": "Document how to clean up the standard Linux installation.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow for a configurable ollama model storage directory",
            "url": "https://github.com/ollama/ollama/pull/897",
            "state": "MERGED",
            "createdAt": "2023-10-24T21:53:27Z",
            "mergedAt": "2023-10-27T14:19:59Z",
            "closedAt": "2023-10-27T14:19:59Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 4
            },
            "additions": 37,
            "deletions": 21,
            "body": "- set `OLLAMA_MODELS` in the environment that ollama is running in to change where models are stored\r\n- update docs\r\n\r\n```bash\r\n$ OLLAMA_MODELS=/Users/bruce/ollama_models ollama serve\r\n# store models in /Users/bruce/ollama_models\r\n```\r\n\r\nResolves ~#228~ #153\r\n\r\nI'll hold off on merging this until #847 is in to avoid causing that PR pain.",
            "participants": {
                "totalCount": 20
            },
            "comments": {
                "totalCount": 27
            }
        }
    },
    {
        "node": {
            "title": "create remote models",
            "url": "https://github.com/ollama/ollama/pull/898",
            "state": "MERGED",
            "createdAt": "2023-10-24T22:48:51Z",
            "mergedAt": "2023-11-16T00:41:13Z",
            "closedAt": "2023-11-16T00:41:13Z",
            "reviews": {
                "totalCount": 32
            },
            "files": {
                "totalCount": 6
            },
            "additions": 371,
            "deletions": 194,
            "body": "This PR changes the way `/api/create` works and addresses some of the current deficiencies:\r\n\r\n1. The API now takes the Modelfile contents. If the field is empty, it'll be populated by reading the file set in the `path` field\r\n2. Add two new APIs to facilitate checking the existence of layers. This is required for detecting which layers the server already has in its blob store\r\n3. Update the `create` command to use these server changes. If the CLI finds a layer isn't known to the server using `GET /api/layer/:digest/path`, it'll create it with `POST /api/layer/:digest`. It'll then update it to the location relative to the server before sending the commands over to the server\r\n\r\nResolves #891 \r\nResolves #892 \r\nResolves #613 \r\nResolves #1066 \r\nResolves #1113 \r\nResolves #315\r\nResolves #1143 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "restore runner build flags",
            "url": "https://github.com/ollama/ollama/pull/900",
            "state": "MERGED",
            "createdAt": "2023-10-25T00:13:11Z",
            "mergedAt": "2023-10-27T19:13:44Z",
            "closedAt": "2023-10-27T19:13:44Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 3,
            "body": "Fixes #899 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Documenting OpenAI compatibility (and other docs tweaks)",
            "url": "https://github.com/ollama/ollama/pull/906",
            "state": "MERGED",
            "createdAt": "2023-10-25T20:18:41Z",
            "mergedAt": "2023-10-27T07:10:23Z",
            "closedAt": "2023-10-27T07:10:23Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 6,
            "body": "Modernization of https://github.com/jmorganca/ollama/pull/661\r\n\r\n- ~Closes https://github.com/jmorganca/ollama/issues/538~\r\n- Upstreams more knowledge from https://github.com/jmorganca/ollama/issues/546\r\n- Simplifies `brew install` to one line",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update linux.md",
            "url": "https://github.com/ollama/ollama/pull/907",
            "state": "MERGED",
            "createdAt": "2023-10-25T21:48:16Z",
            "mergedAt": "2023-10-25T22:03:34Z",
            "closedAt": "2023-10-25T22:03:34Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 53,
            "deletions": 40,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(client): trim trailing slash",
            "url": "https://github.com/ollama/ollama/pull/916",
            "state": "MERGED",
            "createdAt": "2023-10-26T17:51:36Z",
            "mergedAt": "2023-10-26T19:24:12Z",
            "closedAt": "2023-10-26T19:24:12Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 55,
            "deletions": 2,
            "body": "Also sets different default ports when scheme is in OLLAMA_HOST and is either `http` or `https`.\r\n\r\ne.g. `OLLAMA_HOST=https://example.com` will now use port 443 instead of 11434.\r\n\r\nResolves #910 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix docker build annotations",
            "url": "https://github.com/ollama/ollama/pull/917",
            "state": "MERGED",
            "createdAt": "2023-10-26T18:04:41Z",
            "mergedAt": "2023-10-26T19:00:34Z",
            "closedAt": "2023-10-26T19:00:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(download): no retry when out of space",
            "url": "https://github.com/ollama/ollama/pull/918",
            "state": "MERGED",
            "createdAt": "2023-10-26T18:34:32Z",
            "mergedAt": "2023-10-26T19:24:21Z",
            "closedAt": "2023-10-26T19:24:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "Resolves #911 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "offload 75% of available vram to improve stability",
            "url": "https://github.com/ollama/ollama/pull/921",
            "state": "MERGED",
            "createdAt": "2023-10-26T22:24:24Z",
            "mergedAt": "2023-10-27T00:49:55Z",
            "closedAt": "2023-10-27T00:49:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 4,
            "body": "Reducing the amount of layers off-loaded to vram to prevent out of memory errors on larger models. In my tests 7B and 13B models with 4-bit quantization still maxed out the number of layers and ran fast. Meanwhile, 70B models can now run on a T4 when they would previously crash with out-of-memory errors, but its still slow in this case.\r\n\r\nOur heuristic of memory required per layer being roughly the file size divided by the number of layers seems somewhat reliable after additional testing. The calculation for the amount of memory needed for weights looks something like this:\r\n```\r\nsize of weights = number of parameters * number of bytes per parameter\r\n```\r\nThis roughly equates to file size.\r\n\r\nI also added some clarity in the comment around storing the kv cache in vram.\r\n\r\nresolves #790 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add bracketed paste mode",
            "url": "https://github.com/ollama/ollama/pull/922",
            "state": "MERGED",
            "createdAt": "2023-10-26T22:53:45Z",
            "mergedAt": "2023-10-26T22:57:00Z",
            "closedAt": "2023-10-26T22:57:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 36,
            "deletions": 18,
            "body": "This change allows you to cut/paste into the REPL without have to add the \"\"\" around a block of text.\r\n\r\nI've tested it out with:\r\n  * Terminal.app\r\n  * iTerm2\r\n  * Warp\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "catch insufficient permissions nvidia err",
            "url": "https://github.com/ollama/ollama/pull/934",
            "state": "MERGED",
            "createdAt": "2023-10-27T16:36:50Z",
            "mergedAt": "2023-10-27T16:42:40Z",
            "closedAt": "2023-10-27T16:42:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "If there is an insufficient permissions error on `nvidia-smi` execution if would be logged as a parsing error. Catch the error before this happens.\r\n\r\n#932 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "clean up: remove server functions from client",
            "url": "https://github.com/ollama/ollama/pull/937",
            "state": "MERGED",
            "createdAt": "2023-10-27T20:40:04Z",
            "mergedAt": "2023-10-30T15:10:19Z",
            "closedAt": "2023-10-30T15:10:19Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 28,
            "deletions": 27,
            "body": "We have had trouble with cross-account file permission when the Ollama client and server are running as different users. This change is a small clean up to remove all calls to server package code from the client (except for `server.Run()`), from now on we should not call anymore server package functions from `cmd` to prevent bugs.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't quit ioloop on `NUL` character",
            "url": "https://github.com/ollama/ollama/pull/940",
            "state": "MERGED",
            "createdAt": "2023-10-28T02:08:56Z",
            "mergedAt": "2023-10-28T03:01:49Z",
            "closedAt": "2023-10-28T03:01:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 38,
            "deletions": 37,
            "body": "The `stdin` read loop would stop on receiving a `NUL` character, triggered by ctrl+space or other key combos.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "doc: categorised community integrations + added ollama-webui",
            "url": "https://github.com/ollama/ollama/pull/943",
            "state": "MERGED",
            "createdAt": "2023-10-28T21:04:26Z",
            "mergedAt": "2023-11-06T19:35:39Z",
            "closedAt": "2023-11-06T19:35:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 22,
            "deletions": 13,
            "body": "Just found out there was a community integrations section in the README.md file.\r\n\r\nI categorised the integrations into separate groups for better legibility and also added the [ollama-webui](https://github.com/ollama-webui/ollama-webui) project to the GUI list.\r\n\r\nThanks!",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: add webi as install option in readme",
            "url": "https://github.com/ollama/ollama/pull/944",
            "state": "CLOSED",
            "createdAt": "2023-10-28T23:52:57Z",
            "mergedAt": null,
            "closedAt": "2024-02-22T19:11:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "This PR is purely for another install option in the readme. It will be available shortly\r\n\r\nI've been getting ollama added to Webi because I use that to install cli tools for as much as I can these days (I like simple portable ways and Webi works well as a flow for our CI/CD jobs which often have to run cross platform as well).\r\n\r\nThe PR for that at the time of this writing is https://github.com/webinstall/webi-installers/pull/712\r\n\r\nHappy to make adjustments to the wording or placement or whatever would be helpful.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Fix conversion command for gptneox",
            "url": "https://github.com/ollama/ollama/pull/948",
            "state": "MERGED",
            "createdAt": "2023-10-30T15:52:56Z",
            "mergedAt": "2023-10-30T18:34:29Z",
            "closedAt": "2023-10-30T18:34:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: private gpt example was broken due to changes in chroma",
            "url": "https://github.com/ollama/ollama/pull/949",
            "state": "MERGED",
            "createdAt": "2023-10-30T17:58:00Z",
            "mergedAt": "2023-10-31T00:17:01Z",
            "closedAt": "2023-10-31T00:17:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 19,
            "deletions": 2005,
            "body": "This resolves #928\r\n\r\nChroma updated its api and thus langchain updated and everything broke.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "readline windows terminal support",
            "url": "https://github.com/ollama/ollama/pull/950",
            "state": "MERGED",
            "createdAt": "2023-10-30T18:33:27Z",
            "mergedAt": "2023-10-30T20:18:12Z",
            "closedAt": "2023-10-30T20:18:12Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 68,
            "deletions": 3,
            "body": "- update the readline package to have basic support on windows, this is not full feature parity with the unix cli yet\r\n\r\nThis change adds a readline implementation that works on Windows. I dont expect this will work for all features (keyboard short-cuts probably need work). This change also fixes `go build .` on windows.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fly example",
            "url": "https://github.com/ollama/ollama/pull/951",
            "state": "MERGED",
            "createdAt": "2023-10-30T19:00:14Z",
            "mergedAt": "2024-05-07T17:46:25Z",
            "closedAt": "2024-05-07T17:46:25Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 68,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "support raw generation requests",
            "url": "https://github.com/ollama/ollama/pull/952",
            "state": "MERGED",
            "createdAt": "2023-10-30T22:01:26Z",
            "mergedAt": "2023-11-08T22:05:02Z",
            "closedAt": "2023-11-08T22:05:02Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 50,
            "deletions": 5,
            "body": "- add the optional `raw` generate request parameter to bypass prompt formatting and response context\r\n\r\nAdd a `raw` parameter to `/generate` requests that allow directly specifying the prompt without the Ollama server applying additional formatting.\r\n```bash\r\ncurl -X \"POST\" -d '{\"model\":\"mistral\", \"prompt\": \"[INST] hi [/INST]\", \"raw\": true, \"stream\": false}' 'http://127.0.0.1:11434/api/generate'\r\n```\r\n\r\nExample use case, few-shot prompting:\r\n```python\r\nimport requests\r\n\r\ndef call_generate_endpoint(prompt, model=\"mistral\", raw=True, stream=False):\r\n    url = \"http://127.0.0.1:11434/api/generate\"\r\n    formatted_prompt = f\"\"\"[INST] This is awesome! [/INST]\r\nPostive\r\n[INST] This is bad! [/INST]\r\nNegative\r\n[INST] I love this movie [/INST]\r\nPositive\r\n[INST] {prompt} [/INST]\r\n\"\"\"\r\n    \r\n    payload = {\r\n        \"model\": model,\r\n        \"prompt\": formatted_prompt,\r\n        \"raw\": raw,\r\n        \"stream\": stream\r\n    }\r\n    \r\n    response = requests.post(url, json=payload)\r\n    \r\n    return response.json()\n\nresp = call_generate_endpoint(\"I hate this book\")\nprint(resp.response) # Negative\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "notify that the ollama api is available after linux install",
            "url": "https://github.com/ollama/ollama/pull/954",
            "state": "MERGED",
            "createdAt": "2023-10-31T14:05:20Z",
            "mergedAt": "2023-11-01T15:28:26Z",
            "closedAt": "2023-11-01T15:28:26Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "Inform the user that the Ollama API is available locally after install.\r\n```\r\n>>> Downloading ollama...\r\n>>> Installing ollama to /usr/local/bin...\r\n>>> NVIDIA CUDA drivers installed.\r\n>>> The Ollama API is now available at 127.0.0.1:11434.\r\n>>> Install complete. Run \"ollama\" from the command line.\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: add examples using bash to compare models",
            "url": "https://github.com/ollama/ollama/pull/955",
            "state": "MERGED",
            "createdAt": "2023-10-31T16:14:16Z",
            "mergedAt": "2023-11-10T14:59:32Z",
            "closedAt": "2023-11-10T14:59:32Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 81,
            "deletions": 0,
            "body": "This includes two bash scripts. the first will run a bunch of questions in sourcequestions against llama2. The second lets you pick 4 models on your system and run the same questions against all of them, making it easier to compare.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "docs: clarify and clean up API docs",
            "url": "https://github.com/ollama/ollama/pull/956",
            "state": "MERGED",
            "createdAt": "2023-10-31T20:12:17Z",
            "mergedAt": "2023-11-01T04:43:11Z",
            "closedAt": "2023-11-01T04:43:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 82,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "append LD_LIBRARY_PATH",
            "url": "https://github.com/ollama/ollama/pull/958",
            "state": "MERGED",
            "createdAt": "2023-10-31T22:55:25Z",
            "mergedAt": "2023-11-01T15:30:38Z",
            "closedAt": "2023-11-01T15:30:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 1,
            "body": "only append LD_LIBRARY_PATH in case it's already set\r\n\r\nRelated #758 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "k8s example",
            "url": "https://github.com/ollama/ollama/pull/959",
            "state": "MERGED",
            "createdAt": "2023-10-31T22:57:09Z",
            "mergedAt": "2023-11-07T16:43:22Z",
            "closedAt": "2023-11-07T16:43:22Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 134,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix tautology",
            "url": "https://github.com/ollama/ollama/pull/960",
            "state": "MERGED",
            "createdAt": "2023-11-01T03:50:18Z",
            "mergedAt": "2023-11-01T15:30:50Z",
            "closedAt": "2023-11-01T15:30:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "go mod tidy",
            "url": "https://github.com/ollama/ollama/pull/965",
            "state": "MERGED",
            "createdAt": "2023-11-01T18:54:35Z",
            "mergedAt": "2023-11-01T18:55:43Z",
            "closedAt": "2023-11-01T18:55:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 2,
            "deletions": 3,
            "body": "```\r\ngo mod tidy\r\ngo fmt ./...\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix log",
            "url": "https://github.com/ollama/ollama/pull/966",
            "state": "MERGED",
            "createdAt": "2023-11-02T00:18:49Z",
            "mergedAt": "2023-11-02T00:49:10Z",
            "closedAt": "2023-11-02T00:49:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "if there's a remainder, the log line will show the remainder instead of the actual size",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use default RoPE params for new models",
            "url": "https://github.com/ollama/ollama/pull/968",
            "state": "MERGED",
            "createdAt": "2023-11-02T06:13:35Z",
            "mergedAt": "2023-11-02T15:41:30Z",
            "closedAt": "2023-11-02T15:41:30Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "reformat api docs for more examples",
            "url": "https://github.com/ollama/ollama/pull/972",
            "state": "MERGED",
            "createdAt": "2023-11-02T15:27:40Z",
            "mergedAt": "2023-11-03T14:57:00Z",
            "closedAt": "2023-11-03T14:57:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 51,
            "deletions": 21,
            "body": "I'd like to add an example for raw requests in #952 to the docs, but that requires formatting them in way that is more friendly to multiple request/responses. This change moves request/response under an \"examples\" header.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove modelfile context deprecated in v0.0.7",
            "url": "https://github.com/ollama/ollama/pull/974",
            "state": "MERGED",
            "createdAt": "2023-11-02T20:02:47Z",
            "mergedAt": "2023-11-03T00:52:56Z",
            "closedAt": "2023-11-03T00:52:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 4,
            "body": "This modelfile variable was deprecated in ollama v0.0.7, which was a very early stage of the project. It was also not documented anywhere, and no longer used in our library images. It should be ok to remove this now.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update downloads to use retry wrapper",
            "url": "https://github.com/ollama/ollama/pull/975",
            "state": "MERGED",
            "createdAt": "2023-11-02T20:28:21Z",
            "mergedAt": "2023-11-02T23:12:48Z",
            "closedAt": "2023-11-02T23:12:48Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 24,
            "deletions": 39,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Only insert system prompt if first request",
            "url": "https://github.com/ollama/ollama/pull/978",
            "state": "MERGED",
            "createdAt": "2023-11-02T22:46:32Z",
            "mergedAt": "2023-11-02T22:48:02Z",
            "closedAt": "2023-11-02T22:48:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update default NumKeep",
            "url": "https://github.com/ollama/ollama/pull/979",
            "state": "MERGED",
            "createdAt": "2023-11-02T22:47:53Z",
            "mergedAt": "2023-11-02T22:48:44Z",
            "closedAt": "2023-11-02T22:48:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Set `NumKeep` to `4` by default",
            "url": "https://github.com/ollama/ollama/pull/982",
            "state": "MERGED",
            "createdAt": "2023-11-03T00:14:31Z",
            "mergedAt": "2023-11-03T00:26:11Z",
            "closedAt": "2023-11-03T00:26:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add missing \"be\" to GPU support warning message",
            "url": "https://github.com/ollama/ollama/pull/983",
            "state": "MERGED",
            "createdAt": "2023-11-03T01:36:29Z",
            "mergedAt": "2023-11-03T01:37:12Z",
            "closedAt": "2023-11-03T01:37:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove grammar mistake: duplicate \"install\" in GPU support warning message",
            "url": "https://github.com/ollama/ollama/pull/984",
            "state": "MERGED",
            "createdAt": "2023-11-03T01:50:32Z",
            "mergedAt": "2023-11-03T07:45:14Z",
            "closedAt": "2023-11-03T07:45:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Just realised another grammar mistake in the exact same error I just \"fixed\" \ud83d\ude38",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add `/chat` API",
            "url": "https://github.com/ollama/ollama/pull/991",
            "state": "MERGED",
            "createdAt": "2023-11-03T22:01:57Z",
            "mergedAt": "2023-12-04T23:01:06Z",
            "closedAt": "2023-12-04T23:01:06Z",
            "reviews": {
                "totalCount": 39
            },
            "files": {
                "totalCount": 9
            },
            "additions": 665,
            "deletions": 254,
            "body": "- add a new `/api/chat` API endpoint that takes an array of `message` objects. This endpoint is an alternative to `/api/generate`.\r\n- deprecate generation context and template, but continue to support them\r\n- rebuild chat content from messages\r\n\r\nThis changes adds a `/api/chat` endpoint to the API which takes an array of messages. This makes modifying and tracking the history on the fly much simpler. It is an alternative to prompt/response.\r\n\r\n`context` will continue to work as expected for now, but at some point in the future we may want to replace it completely with `/api/chat`.\r\n\r\n```\r\ncurl -X POST http://localhost:11434/api/generate -d '{\r\n    \"model\": \"mistral\",\r\n    \"prompt\": \"hello, how are you?\"\r\n}'\r\n\r\nOR\r\n\r\n### Basic generate request with messages\r\ncurl -X POST http://localhost:11434/api/chat -d '{\r\n    \"model\": \"mistral\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"why is the sky blue?\"\r\n        }\r\n    ]\r\n}'\r\n```\r\n\r\nresolves #981 \r\nresolves #1203 ",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 11
            }
        }
    },
    {
        "node": {
            "title": "update langchainjs doc",
            "url": "https://github.com/ollama/ollama/pull/992",
            "state": "MERGED",
            "createdAt": "2023-11-03T23:47:11Z",
            "mergedAt": "2023-11-09T13:08:31Z",
            "closedAt": "2023-11-09T13:08:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 2,
            "body": "Updates docs/tutorials/langchainjs.md from issue #539 \r\n\r\nadds missing await in line 36 and adds instructions to install cheerio\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cleanup upload and download errors",
            "url": "https://github.com/ollama/ollama/pull/993",
            "state": "MERGED",
            "createdAt": "2023-11-04T00:03:11Z",
            "mergedAt": "2023-11-06T19:32:12Z",
            "closedAt": "2023-11-06T19:32:12Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 34,
            "deletions": 31,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add OllamaEmbeddings to python LangChain example",
            "url": "https://github.com/ollama/ollama/pull/994",
            "state": "MERGED",
            "createdAt": "2023-11-04T03:11:00Z",
            "mergedAt": "2023-11-29T21:25:39Z",
            "closedAt": "2023-11-29T21:25:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 3,
            "body": "The current docs use `GPT4AllEmbeddings()`. Here we simply swap that out for `OllamaEmbeddings`, which was [recently added](https://github.com/langchain-ai/langchain/pull/10124) to LangChain.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Added ollama-rs to community integrations",
            "url": "https://github.com/ollama/ollama/pull/995",
            "state": "MERGED",
            "createdAt": "2023-11-04T08:51:06Z",
            "mergedAt": "2023-11-04T21:51:29Z",
            "closedAt": "2023-11-04T21:51:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hey, I made Rust bindings for Ollama https://github.com/pepperoni21/ollama-rs",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add gen.nvim as community contribution",
            "url": "https://github.com/ollama/ollama/pull/996",
            "state": "MERGED",
            "createdAt": "2023-11-04T10:08:26Z",
            "mergedAt": "2023-11-06T18:51:41Z",
            "closedAt": "2023-11-06T18:51:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi,\r\n\r\n[gen.nvim](https://github.com/David-Kunz/gen.nvim) is a Neovim extension from which you can invoke Ollama.\r\n\r\nBest regards,\r\nDavid",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add hass-ollama-conversation to community integrations",
            "url": "https://github.com/ollama/ollama/pull/999",
            "state": "MERGED",
            "createdAt": "2023-11-04T13:01:46Z",
            "mergedAt": "2023-11-06T18:50:35Z",
            "closedAt": "2023-11-06T18:50:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add custom home assistant integration [hass-ollama-conversation](https://github.com/ej52/hass-ollama-conversation)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added clear command",
            "url": "https://github.com/ollama/ollama/pull/1000",
            "state": "CLOSED",
            "createdAt": "2023-11-04T14:05:13Z",
            "mergedAt": null,
            "closedAt": "2023-11-09T00:49:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "Added clear command for ease of use\r\n\r\nCloses #989 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add ModelFusion community integration",
            "url": "https://github.com/ollama/ollama/pull/1001",
            "state": "CLOSED",
            "createdAt": "2023-11-04T14:43:21Z",
            "mergedAt": null,
            "closedAt": "2023-11-06T17:53:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Added an example for Self Querying Retrieval",
            "url": "https://github.com/ollama/ollama/pull/1009",
            "state": "CLOSED",
            "createdAt": "2023-11-06T00:00:25Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T03:31:42Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 8
            },
            "additions": 2193,
            "deletions": 0,
            "body": "Self querying retrieval is a method to filter a vector database to show the most relevant documents for a query",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Update api.md",
            "url": "https://github.com/ollama/ollama/pull/1015",
            "state": "CLOSED",
            "createdAt": "2023-11-06T10:24:18Z",
            "mergedAt": null,
            "closedAt": "2023-11-29T21:21:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixed documentation, responds one token for streamed results",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add ModelFusion community integration",
            "url": "https://github.com/ollama/ollama/pull/1020",
            "state": "MERGED",
            "createdAt": "2023-11-06T17:54:56Z",
            "mergedAt": "2023-11-06T18:46:16Z",
            "closedAt": "2023-11-06T18:46:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "instead of static number of parameters for each model family, get the real number from the tensors",
            "url": "https://github.com/ollama/ollama/pull/1022",
            "state": "MERGED",
            "createdAt": "2023-11-06T22:05:55Z",
            "mergedAt": "2023-11-09T01:55:46Z",
            "closedAt": "2023-11-09T01:55:47Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 72,
            "deletions": 18,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: clarify where the models are stored in the faq",
            "url": "https://github.com/ollama/ollama/pull/1023",
            "state": "MERGED",
            "createdAt": "2023-11-06T22:40:07Z",
            "mergedAt": "2023-11-08T01:59:55Z",
            "closedAt": "2023-11-08T01:59:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 19,
            "deletions": 0,
            "body": "Where are the models is a common enough question in the Discord. This clarifies it a bit further helping the user understand the structure.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update client.py",
            "url": "https://github.com/ollama/ollama/pull/1026",
            "state": "MERGED",
            "createdAt": "2023-11-07T06:59:18Z",
            "mergedAt": "2023-11-07T17:55:47Z",
            "closedAt": "2023-11-07T17:55:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "recieve -> receive",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added logseq ollama plugin",
            "url": "https://github.com/ollama/ollama/pull/1029",
            "state": "MERGED",
            "createdAt": "2023-11-07T10:49:05Z",
            "mergedAt": "2023-11-07T17:58:13Z",
            "closedAt": "2023-11-07T17:58:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Adds a plugin I made to integrate ollama with [logseq](https://github.com/logseq/logseq)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix sudo variable in install.sh",
            "url": "https://github.com/ollama/ollama/pull/1034",
            "state": "MERGED",
            "createdAt": "2023-11-07T16:42:52Z",
            "mergedAt": "2023-11-07T17:59:57Z",
            "closedAt": "2023-11-07T17:59:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "It was forgotten to replace sudo at one place with the variable for sudo.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add a complete /generate options example",
            "url": "https://github.com/ollama/ollama/pull/1035",
            "state": "MERGED",
            "createdAt": "2023-11-07T19:01:44Z",
            "mergedAt": "2023-11-09T00:44:37Z",
            "closedAt": "2023-11-09T00:44:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 111,
            "deletions": 43,
            "body": "- Add an example to the api docs that shows how all generate runtime options can be specified\r\n- Move the `GenerateRequest` options closed to the struct declaration so its easier for readers to find\r\n\r\nresolves #1027 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "progressbar: make start and end seamless",
            "url": "https://github.com/ollama/ollama/pull/1042",
            "state": "MERGED",
            "createdAt": "2023-11-08T11:50:36Z",
            "mergedAt": "2023-11-09T00:42:40Z",
            "closedAt": "2023-11-09T00:42:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This just makes the bars limiting the progressbar width hug the progressbar because it looks nicer.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add Ollamac to community integrations",
            "url": "https://github.com/ollama/ollama/pull/1043",
            "state": "MERGED",
            "createdAt": "2023-11-08T13:10:08Z",
            "mergedAt": "2023-11-08T19:01:09Z",
            "closedAt": "2023-11-08T19:01:09Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi @jmorganca, I appreciate the time and effort you've put into developing Ollama\u2014it's a fantastic tool! I noticed there isn't a native macOS application listed under community integrations, so I went ahead and created one called [Ollamac](https://github.com/kevinhermawan/Ollamac). I'm looking forward to hearing your thoughts on it! \ud83d\ude03\r\n\r\n![Ollamac](https://github.com/jmorganca/ollama/assets/84965338/dcae2e9e-3f8c-4d19-9221-7567cf64ce93)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added Ollama4j (Java library) to community integrations",
            "url": "https://github.com/ollama/ollama/pull/1044",
            "state": "MERGED",
            "createdAt": "2023-11-08T16:18:12Z",
            "mergedAt": "2023-11-08T19:04:32Z",
            "closedAt": "2023-11-08T19:04:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi @jmorganca, thank you so much for your efforts in building and constantly updating such a cool piece of software. \r\nI really like Ollama and have been experimenting with it regularly. I also plan to integrate it into many more Java applications. \r\nI noticed there isn't a Java library listed under community integrations, that's why I felt the need to create a Java library for interacting with Ollama's REST APIs, and as a result, I've published Ollama4j as a package and I plan to constantly improve it. This way, others who work with Java can easily interact with the Ollama API. \r\n\r\nThanks again, and keep up the great work!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "JSON mode: add `\"format\": \"json\"` as an api parameter",
            "url": "https://github.com/ollama/ollama/pull/1051",
            "state": "MERGED",
            "createdAt": "2023-11-09T01:58:51Z",
            "mergedAt": "2023-11-10T00:44:02Z",
            "closedAt": "2023-11-10T00:44:02Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 97,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fixed incorrect base model name",
            "url": "https://github.com/ollama/ollama/pull/1055",
            "state": "MERGED",
            "createdAt": "2023-11-09T12:28:31Z",
            "mergedAt": "2023-11-13T16:42:55Z",
            "closedAt": "2023-11-13T16:42:55Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Added tag version to 'GetNamespaceRepository' method in order to set the correct model used model tag version.\r\n\r\n(This PR fixes bug/issue: #946 )",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "skip gpu if less than 2GB VRAM are available",
            "url": "https://github.com/ollama/ollama/pull/1059",
            "state": "MERGED",
            "createdAt": "2023-11-09T18:46:17Z",
            "mergedAt": "2023-11-09T21:16:16Z",
            "closedAt": "2023-11-09T21:16:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 5,
            "body": "This check for available VRAM that triggered a fallback to CPU if less than 2GB of VRAM was available wasn't handled correctly in the current ollama version. This resulted in a very low amount of layers being offloaded to GPU in the case that a GPU was available with a low amount of available VRAM, and this results in worse performance than just falling back to CPU.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "document specifying multiple stop params",
            "url": "https://github.com/ollama/ollama/pull/1061",
            "state": "MERGED",
            "createdAt": "2023-11-09T19:51:13Z",
            "mergedAt": "2023-11-09T21:16:26Z",
            "closedAt": "2023-11-09T21:16:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "resolves #572 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added gptel to list of integrations",
            "url": "https://github.com/ollama/ollama/pull/1062",
            "state": "MERGED",
            "createdAt": "2023-11-09T20:18:02Z",
            "mergedAt": "2023-11-09T20:52:36Z",
            "closedAt": "2023-11-09T20:52:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add custom ollama-runner",
            "url": "https://github.com/ollama/ollama/pull/1067",
            "state": "CLOSED",
            "createdAt": "2023-11-10T01:45:17Z",
            "mergedAt": null,
            "closedAt": "2023-11-21T20:14:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 34871,
            "deletions": 103,
            "body": "- update llama.cpp examples with custom ollama-runner\r\n- update llama.cpp gguf version to latest\r\n\r\nThis change adds a custom inference server to llama.cpp based on the server we use in the current version, but with excess features removed. This allows us to have a more stable interface to build on when llama.cpp updates.\r\n\r\nTo review this please pull down the changes run `go generate ./...` and review the contents of the `llm/llama.cpp/gguf/examples/ollama-runner`\r\n\r\nThis change may be superseded by packaging in llama.cpp directly in the near future.",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Log Analysis Example ",
            "url": "https://github.com/ollama/ollama/pull/1074",
            "state": "MERGED",
            "createdAt": "2023-11-10T14:57:55Z",
            "mergedAt": "2023-11-17T00:33:07Z",
            "closedAt": "2023-11-17T00:33:07Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 131,
            "deletions": 0,
            "body": "At kubecon and other events and on discord, we have been asked how to analyse logs using ollama. This is a simple example of one approach to this.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Resume chunk download on UnexpectedEOF errors",
            "url": "https://github.com/ollama/ollama/pull/1075",
            "state": "MERGED",
            "createdAt": "2023-11-10T16:30:42Z",
            "mergedAt": "2023-11-10T16:48:28Z",
            "closedAt": "2023-11-10T16:48:28Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 2,
            "body": "If the chunk download is interrupted, resume from where we left off",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "New big-AGI integration",
            "url": "https://github.com/ollama/ollama/pull/1078",
            "state": "MERGED",
            "createdAt": "2023-11-10T20:26:09Z",
            "mergedAt": "2023-11-13T21:59:00Z",
            "closedAt": "2023-11-13T21:59:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi @jmorganca, we added Ollama support to big-AGI, which makes it easy to fetch, list models, generate text, chat, compare models, and even voice call, etc.\r\n\r\n![image](https://github.com/jmorganca/ollama/assets/32999/66c0c2b8-2eea-4982-bf6c-ba1cd05cd18d)\r\n\r\nI've linked the configuration document directly.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add example using JSON format output",
            "url": "https://github.com/ollama/ollama/pull/1079",
            "state": "MERGED",
            "createdAt": "2023-11-10T22:42:19Z",
            "mergedAt": "2023-11-16T17:13:35Z",
            "closedAt": "2023-11-16T17:13:35Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 97,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add support for Multimodel models",
            "url": "https://github.com/ollama/ollama/pull/1082",
            "state": "CLOSED",
            "createdAt": "2023-11-11T02:57:28Z",
            "mergedAt": null,
            "closedAt": "2023-11-22T00:22:46Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 12
            },
            "additions": 278,
            "deletions": 144,
            "body": "Interactive cli usage:\r\n```bash\r\n/set image add <image id int> <path to image file>\r\nPlease tell me what text is in this photo [img-<image id int>]\r\n```\r\n\r\nFor the API I added support for the `image_data` prop  with the type of `[]{id: int, data: string(base64)}`for the generate endpoint. \r\n\r\nTo support this, modelfile now has a `MMPROJ` key that points to the mmproj file path.\r\n\r\nTested with the following modelfile:\r\n\r\n```\r\nFROM ./ggml-model-q4_k.gguf\r\n\r\nTEMPLATE \"\"\"\r\nUSER:{{ .Prompt }}\r\nASSISTANT:\r\n\"\"\"\r\n\r\nMMPROJ ./mmproj-model-f16.gguf\r\n```\r\n\r\nAnd using the following pre-quantatized model - https://huggingface.co/mys/ggml_llava-v1.5-13b.\r\n\r\n<img width=\"1226\" alt=\"Screenshot 2023-11-09 at 11 30 10\u202fPM\" src=\"https://github.com/jmorganca/ollama/assets/192433/ec62e5e7-999b-44f2-9a32-e6b85ca5b404\">\r\n<img width=\"567\" alt=\"screenshot\" src=\"https://github.com/jmorganca/ollama/assets/192433/681c5cd5-9140-43b1-8383-08823cdbb002\">\r\n\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 12
            }
        }
    },
    {
        "node": {
            "title": "Add OllamaKit to the community integrations",
            "url": "https://github.com/ollama/ollama/pull/1085",
            "state": "MERGED",
            "createdAt": "2023-11-11T11:34:33Z",
            "mergedAt": "2023-11-11T22:41:42Z",
            "closedAt": "2023-11-11T22:41:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hello folks, I'm excited to share that I've been working on `OllamaKit`. Originally created for `Ollamac`, I've realized its potential extends to assisting anyone looking to integrate the Ollama API with Swift. \ud83d\ude03",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add /clear command via the ANSI method",
            "url": "https://github.com/ollama/ollama/pull/1086",
            "state": "CLOSED",
            "createdAt": "2023-11-11T15:54:25Z",
            "mergedAt": null,
            "closedAt": "2024-01-03T02:06:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "Small, but welcome addition in my opinion. Works using the ANSI method from the bottom. \r\n```go\r\ncase \"/clear\":\r\n    fmt.Printf(\"\\x1bc\")\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add JSON mode to `ollama run`",
            "url": "https://github.com/ollama/ollama/pull/1095",
            "state": "MERGED",
            "createdAt": "2023-11-12T03:29:21Z",
            "mergedAt": "2023-11-14T02:54:02Z",
            "closedAt": "2023-11-14T02:54:02Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 25,
            "deletions": 6,
            "body": "Allow using JSON mode from the `ollama run` command line\r\n\r\n* `--format json`: a new command line flag\r\n* `/set format json`: in the interactive `ollama run` terminal",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Created tutorial for running Ollama on NVIDIA Jetson devices",
            "url": "https://github.com/ollama/ollama/pull/1098",
            "state": "MERGED",
            "createdAt": "2023-11-12T11:02:14Z",
            "mergedAt": "2023-11-15T17:32:37Z",
            "closedAt": "2023-11-15T17:32:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 40,
            "deletions": 1,
            "body": "This pull request provides guidance for people interested in enabling NVIDIA's AI edge computing devices to run Ollama at full power (i.e. on the integrated GPU). Several people (myself included) have expressed interest in this capability (please see issue #1071).\r\n\r\n@BruceMacD mentioned via Discord that the CLI will soon support passing `num_gpu` as a parameter when running `ollama serve`. I will update the tutorial when that becomes available.\r\n\r\nThanks!",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add jupyter notebook example",
            "url": "https://github.com/ollama/ollama/pull/1104",
            "state": "MERGED",
            "createdAt": "2023-11-12T21:37:13Z",
            "mergedAt": "2023-11-17T22:46:26Z",
            "closedAt": "2023-11-17T22:46:26Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 107,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add Dart library to README.md",
            "url": "https://github.com/ollama/ollama/pull/1106",
            "state": "MERGED",
            "createdAt": "2023-11-13T04:25:49Z",
            "mergedAt": "2023-11-13T19:50:42Z",
            "closedAt": "2023-11-13T19:50:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Good afternoon!\r\n\r\nI have completed the first version of the Ollama library for Dart, making it possible to integrate Ollama into Flutter applications. I thought it would be nice to mention it in the readme file.\r\n\r\n![](https://media.giphy.com/media/3oNMQtqpnse0dbFe06/giphy.gif)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add ollama.nvim to list of terminal links",
            "url": "https://github.com/ollama/ollama/pull/1115",
            "state": "MERGED",
            "createdAt": "2023-11-13T20:46:12Z",
            "mergedAt": "2023-11-13T22:00:18Z",
            "closedAt": "2023-11-13T22:00:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "`ollama.nvim` is a good plugin, uses the ollama API directly! ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Maid to Community Integrations",
            "url": "https://github.com/ollama/ollama/pull/1120",
            "state": "MERGED",
            "createdAt": "2023-11-14T06:08:24Z",
            "mergedAt": "2023-11-16T16:27:53Z",
            "closedAt": "2023-11-16T16:27:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Maid is a cross-platform Flutter app for interfacing with GGUF / Llama models locally via Llama.cpp and interfacing remotely via Ollama. Currently Android, Windows and Linux are supported with plans to support Mac and IOS in the future.\r\n\r\nMaid is working great with Ollama now and once #991 goes through it will work even better.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Cheshire Cat to community integrations",
            "url": "https://github.com/ollama/ollama/pull/1124",
            "state": "MERGED",
            "createdAt": "2023-11-14T12:45:21Z",
            "mergedAt": "2023-11-16T16:30:55Z",
            "closedAt": "2023-11-16T16:30:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "We have an Ollama adapter (subclassing langchain to make it both sync and async) and also created a [setup tutorial](https://cheshirecat.ai/local-models-with-ollama/)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Use `stdout` file descriptor to determine terminal size",
            "url": "https://github.com/ollama/ollama/pull/1125",
            "state": "MERGED",
            "createdAt": "2023-11-14T16:04:33Z",
            "mergedAt": "2023-11-14T21:09:09Z",
            "closedAt": "2023-11-14T21:09:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add ability to pass prompt in via standard input such as `ollama run model < file`",
            "url": "https://github.com/ollama/ollama/pull/1126",
            "state": "MERGED",
            "createdAt": "2023-11-14T16:29:15Z",
            "mergedAt": "2023-11-14T21:42:21Z",
            "closedAt": "2023-11-14T21:42:21Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 29,
            "deletions": 53,
            "body": "Originally opened by @sqs  in #416\r\n\r\nPreviously, `ollama run` treated a non-terminal stdin (such as `ollama run model < file`) as containing one prompt per line. To run inference on a multi-line prompt, the only non-API workaround was to run `ollama run` interactively and wrap the prompt in `\"\"\"...\"\"\"`.\r\n\r\nNow, `ollama run` treats a non-terminal stdin as containing a single prompt. For example, if `myprompt.txt` is a multi-line file, then `ollama run model < myprompt.txt` would treat `myprompt.txt`'s entire contents as the prompt.\r\n\r\nExamples:\r\n\r\n```\r\ncat mycode.py | ollama run codellama \"what does this code do?\"\r\ncat essay.txt | ollama run llama2 \"Summarize this story in 5 points. Respond in json.\" --format json | jq\r\n```\r\n\r\nReplacement for the current behavior is to create a bash script that reads in each line from `stdin` and calls `ollama run`:\r\n\r\n```\r\n#!/bin/bash\r\nwhile IFS= read -r line; do\r\n    echo \"$line\" | ollama run $1\r\ndone\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move /generate format to optional parameters",
            "url": "https://github.com/ollama/ollama/pull/1127",
            "state": "MERGED",
            "createdAt": "2023-11-14T18:44:30Z",
            "mergedAt": "2023-11-14T21:12:30Z",
            "closedAt": "2023-11-14T21:12:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This field is optional and should be under the `Advanced parameters` header",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "FAQ: answer a few faq questions",
            "url": "https://github.com/ollama/ollama/pull/1128",
            "state": "MERGED",
            "createdAt": "2023-11-14T19:43:51Z",
            "mergedAt": "2023-11-15T23:05:13Z",
            "closedAt": "2023-11-15T23:05:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "return failure details when unauthorized to push",
            "url": "https://github.com/ollama/ollama/pull/1131",
            "state": "MERGED",
            "createdAt": "2023-11-14T20:28:49Z",
            "mergedAt": "2023-11-16T21:44:18Z",
            "closedAt": "2023-11-16T21:44:18Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 3,
            "body": "Previous behavior:\r\nPushing to the default namespace or a namespace you don't have access to results in a vague error.\r\n```\r\n$ ollama push mario\r\nretrieving manifest\r\nError: max retries exceeded\r\n```\r\n\r\nNew behavior:\r\nPushing to the default namespace or a namespace you don't have access to results the reason for the error.\r\n```\r\n$ ollama push mario\r\nretrieving manifest\r\nError: unable to push library/mario, make sure this namespace exists and you are authorized to push to it\r\n\r\n$ ollama push bruxe/mario\r\nretrieving manifest\r\nError: unable to push bruxe/mario, make sure this namespace exists and you are authorized to push to it\r\n```\r\n\r\nResolves #1140 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "replace go-humanize with format.HumanBytes",
            "url": "https://github.com/ollama/ollama/pull/1132",
            "state": "MERGED",
            "createdAt": "2023-11-14T22:58:03Z",
            "mergedAt": "2023-11-15T17:46:22Z",
            "closedAt": "2023-11-15T17:46:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 4,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "progress bar",
            "url": "https://github.com/ollama/ollama/pull/1134",
            "state": "MERGED",
            "createdAt": "2023-11-15T01:06:58Z",
            "mergedAt": "2023-11-17T22:03:35Z",
            "closedAt": "2023-11-17T22:03:35Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 12
            },
            "additions": 423,
            "deletions": 1445,
            "body": "Example:\r\n\r\n```\r\n$ ollama pull mistral\r\npulling manifest                                                                                                                                                                                                  (1s)\r\ndownloading 6ae280299950 100.0% [=========================================================================================================================================================] (4.1 GB/4.1 GB, 0 B/s, 0s)\r\ndownloading 22e1b2e8dc2f 100.0% [=============================================================================================================================================================] (43 B/43 B, 0 B/s, 0s)\r\ndownloading e35ab70a78c7 100.0% [=============================================================================================================================================================] (90 B/90 B, 0 B/s, 0s)\r\ndownloading 1cb90d66f4d4 100.0% [===========================================================================================================================================================] (381 B/381 B, 0 B/s, 0s)\r\nverifying sha256 digest                                                                                                                                                                                           (2s)\r\nwriting manifest                                                                                                                                                                                                  (0s)\r\nremoving any unused layers                                                                                                                                                                                        (0s)\r\nsuccess                                                                                                                                                                                                           (0s)\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "Add cgo implementation for llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/1146",
            "state": "MERGED",
            "createdAt": "2023-11-16T00:18:06Z",
            "mergedAt": "2023-12-22T16:16:31Z",
            "closedAt": "2023-12-22T16:16:31Z",
            "reviews": {
                "totalCount": 45
            },
            "files": {
                "totalCount": 55
            },
            "additions": 3205,
            "deletions": 1179,
            "body": "This change revamps the way ollama wires up llama.cpp for gguf to link directly via cgo instead\r\nof running a subprocess.  Within llama.cpp, a thin facade has been added to server.cpp (via included patch) \r\nto enable extern \"C\" access to the main logic to minimize changes to the existing LLM interface.\r\n\r\nMac, Linux, and Windows are supported and manually tested.  \r\n\r\nCarries #1268 and #814 \r\n",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 18
            }
        }
    },
    {
        "node": {
            "title": "add faq for proxies",
            "url": "https://github.com/ollama/ollama/pull/1147",
            "state": "MERGED",
            "createdAt": "2023-11-16T01:16:03Z",
            "mergedAt": "2023-11-16T16:43:37Z",
            "closedAt": "2023-11-16T16:43:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "install: fix enable contrib on debian 12",
            "url": "https://github.com/ollama/ollama/pull/1151",
            "state": "MERGED",
            "createdAt": "2023-11-16T08:23:44Z",
            "mergedAt": "2023-11-16T20:53:07Z",
            "closedAt": "2023-11-16T20:53:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "On debian 12, sources definitions have moved from\r\n/etc/apt/sources.list to /etc/apt/sources.list.d/debian.sources",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix push for model inheriting from other models",
            "url": "https://github.com/ollama/ollama/pull/1156",
            "state": "MERGED",
            "createdAt": "2023-11-16T19:52:56Z",
            "mergedAt": "2023-11-16T21:33:30Z",
            "closedAt": "2023-11-16T21:33:30Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 2,
            "body": "- fix auth scope: side effect of #1055 which changed the value of the scope parameter in the auth challenge\r\n- fix cross repo mounts\r\n\r\nresolves #1154 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Example: Function Calling in Typescript",
            "url": "https://github.com/ollama/ollama/pull/1159",
            "state": "MERGED",
            "createdAt": "2023-11-17T00:32:33Z",
            "mergedAt": "2023-11-21T18:06:55Z",
            "closedAt": "2023-11-21T18:06:55Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 7
            },
            "additions": 912,
            "deletions": 0,
            "body": "Two examples here. One to list the characters in the first few pages of War and Peace. The other parses emails for events and addresses.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update faq",
            "url": "https://github.com/ollama/ollama/pull/1160",
            "state": "MERGED",
            "createdAt": "2023-11-17T00:46:53Z",
            "mergedAt": "2023-11-17T00:48:51Z",
            "closedAt": "2023-11-17T00:48:51Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "placeholder environment variables",
            "url": "https://github.com/ollama/ollama/pull/1161",
            "state": "MERGED",
            "createdAt": "2023-11-17T00:55:55Z",
            "mergedAt": "2023-11-17T22:45:39Z",
            "closedAt": "2023-11-17T22:45:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update faq",
            "url": "https://github.com/ollama/ollama/pull/1164",
            "state": "MERGED",
            "createdAt": "2023-11-17T01:10:37Z",
            "mergedAt": "2023-11-17T01:20:19Z",
            "closedAt": "2023-11-17T01:20:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor Request Retry",
            "url": "https://github.com/ollama/ollama/pull/1175",
            "state": "MERGED",
            "createdAt": "2023-11-17T16:16:06Z",
            "mergedAt": "2023-11-17T19:22:35Z",
            "closedAt": "2023-11-17T19:22:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 27,
            "deletions": 28,
            "body": "The request retry logic is mostly in `download.go` and `upload.go`. This function is only meant to retry on authentication failure, so doing that multiple times is not needed. \r\n\r\n- do not log `upload failure` on error, this function is called on download also\r\n- do not log on request cancellation, this causes a cancelled download to log 10+ times due to the chunked downloads\r\n- only retry on auth failure explicitly, and one time",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update faq",
            "url": "https://github.com/ollama/ollama/pull/1176",
            "state": "MERGED",
            "createdAt": "2023-11-17T16:41:59Z",
            "mergedAt": "2023-11-17T16:42:59Z",
            "closedAt": "2023-11-17T16:42:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "faq: fix heading and add more details",
            "url": "https://github.com/ollama/ollama/pull/1177",
            "state": "MERGED",
            "createdAt": "2023-11-17T17:05:09Z",
            "mergedAt": "2023-11-17T18:05:21Z",
            "closedAt": "2023-11-17T18:05:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Installation instructions for Archlinux",
            "url": "https://github.com/ollama/ollama/pull/1178",
            "state": "MERGED",
            "createdAt": "2023-11-17T17:45:26Z",
            "mergedAt": "2023-11-21T14:33:22Z",
            "closedAt": "2023-11-21T14:33:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Pacman is the recommended installation method. And the package is in the official repository, so makes sense to mention it in the README.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Add Rivet to Community Integrations",
            "url": "https://github.com/ollama/ollama/pull/1183",
            "state": "MERGED",
            "createdAt": "2023-11-17T22:09:04Z",
            "mergedAt": "2023-11-20T15:36:47Z",
            "closedAt": "2023-11-20T15:36:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Adds the [Ollama plugin](https://github.com/abrenneke/rivet-plugin-ollama) for [Rivet](https://rivet.ironcladapp.com/) to the community integrations list (Extensions & Plugins section)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix potentially inaccurate error message",
            "url": "https://github.com/ollama/ollama/pull/1185",
            "state": "MERGED",
            "createdAt": "2023-11-17T23:07:37Z",
            "mergedAt": "2023-11-19T02:25:07Z",
            "closedAt": "2023-11-19T02:25:07Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "In the case of not enough VRAM being available this log made it seem as though there was an issue with cuda libraries. Move the error details to only be returned if `nvidia-smi` is not available specifically. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix cross device rename",
            "url": "https://github.com/ollama/ollama/pull/1186",
            "state": "MERGED",
            "createdAt": "2023-11-17T23:22:38Z",
            "mergedAt": "2023-11-18T05:54:54Z",
            "closedAt": "2023-11-18T05:54:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 7,
            "body": "`os.Rename` is only intended for files on the same filesystem. Instead of messing around with that, store the temporary file in the blobs directory\r\n\r\nresolves #1181 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adding `ogpt.nvim` into the list of plugins!",
            "url": "https://github.com/ollama/ollama/pull/1190",
            "state": "MERGED",
            "createdAt": "2023-11-18T13:00:58Z",
            "mergedAt": "2023-11-20T15:39:14Z",
            "closedAt": "2023-11-20T15:39:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "ChatGPT.nvim is a well built plugin. `ogpt.nvim` is a fork that supports Ollama",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "main_gpu argument is not getting set for llamacpp",
            "url": "https://github.com/ollama/ollama/pull/1192",
            "state": "MERGED",
            "createdAt": "2023-11-18T23:06:12Z",
            "mergedAt": "2023-11-20T15:52:52Z",
            "closedAt": "2023-11-20T15:52:52Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "In a multi-GPU platform I observed I cannot set the main GPU to be used to llamacpp though llamacpp itself support this through `main_gpu` argument.\r\n\r\n\r\nThis PR fixes just that.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "progress: fix bar rate",
            "url": "https://github.com/ollama/ollama/pull/1195",
            "state": "MERGED",
            "createdAt": "2023-11-19T02:37:58Z",
            "mergedAt": "2023-11-28T19:55:23Z",
            "closedAt": "2023-11-28T19:55:23Z",
            "reviews": {
                "totalCount": 17
            },
            "files": {
                "totalCount": 3
            },
            "additions": 126,
            "deletions": 82,
            "body": "implement rate as a rolling average over the last n updates. \r\n\r\nthe current issue is rate is calculated as an average rate over the lifetime of the progress bar. this reflects the actual progress well if the progress is smooth and flat but that's rarely the case.\r\n\r\nthe rolling average is a better way to represent changing rates. if more progress is made in the window than before, the rate will go up. if less progress is made, the rate will go down",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "README: link to LangChainGo for talking to ollama, with an example",
            "url": "https://github.com/ollama/ollama/pull/1206",
            "state": "MERGED",
            "createdAt": "2023-11-20T14:33:01Z",
            "mergedAt": "2023-11-20T15:35:07Z",
            "closedAt": "2023-11-20T15:35:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Laravel package to README.md",
            "url": "https://github.com/ollama/ollama/pull/1208",
            "state": "MERGED",
            "createdAt": "2023-11-20T15:46:16Z",
            "mergedAt": "2023-11-20T15:48:35Z",
            "closedAt": "2023-11-20T15:48:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: allow specifying relative files in modelfile",
            "url": "https://github.com/ollama/ollama/pull/1211",
            "state": "MERGED",
            "createdAt": "2023-11-20T20:24:21Z",
            "mergedAt": "2023-11-20T21:43:48Z",
            "closedAt": "2023-11-20T21:43:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Small regression here from remote models. Previously you could specify files relative to a modelfile without their path, this is my normal workflow. This change restores this behaviour to match v0.1.9.\r\n\r\nExample modelfile:\r\n```\r\nFROM nous-capybara-34b.Q4_0.gguf\r\nTEMPLATE \"USER: { .Prompt } ASSISTANT: \"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "enable metal for fp32, q5_0, q5_1",
            "url": "https://github.com/ollama/ollama/pull/1212",
            "state": "MERGED",
            "createdAt": "2023-11-20T21:48:29Z",
            "mergedAt": "2023-11-20T21:56:40Z",
            "closedAt": "2023-11-20T21:56:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 8,
            "body": "recent llama.cpp update added kernels for fp32, q5_0, and q5_1\r\n\r\nresolves #1200 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove redundant filename parameter in api request for `ollama create`",
            "url": "https://github.com/ollama/ollama/pull/1213",
            "state": "MERGED",
            "createdAt": "2023-11-20T21:54:26Z",
            "mergedAt": "2023-11-20T22:05:37Z",
            "closedAt": "2023-11-20T22:05:37Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use a pulsating spinner",
            "url": "https://github.com/ollama/ollama/pull/1215",
            "state": "CLOSED",
            "createdAt": "2023-11-21T01:28:55Z",
            "mergedAt": null,
            "closedAt": "2023-11-30T21:35:14Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 26,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Multimodal support",
            "url": "https://github.com/ollama/ollama/pull/1216",
            "state": "MERGED",
            "createdAt": "2023-11-21T02:55:55Z",
            "mergedAt": "2023-12-11T21:56:22Z",
            "closedAt": "2023-12-11T21:56:22Z",
            "reviews": {
                "totalCount": 43
            },
            "files": {
                "totalCount": 6
            },
            "additions": 235,
            "deletions": 28,
            "body": "This PR builds off of @mattapperson's work, but with a more ollama-like UX + API.",
            "participants": {
                "totalCount": 8
            },
            "comments": {
                "totalCount": 8
            }
        }
    },
    {
        "node": {
            "title": "Update Maid repo",
            "url": "https://github.com/ollama/ollama/pull/1218",
            "state": "MERGED",
            "createdAt": "2023-11-21T10:03:47Z",
            "mergedAt": "2023-11-21T14:30:34Z",
            "closedAt": "2023-11-21T14:30:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Sorry for the extra PR but i noticed i accidently linked my personal repo instead of the main repo",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: Add Oatmeal to terminal integrations",
            "url": "https://github.com/ollama/ollama/pull/1219",
            "state": "MERGED",
            "createdAt": "2023-11-21T11:51:01Z",
            "mergedAt": "2023-11-21T14:28:13Z",
            "closedAt": "2023-11-21T14:28:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "## Overview\r\n\r\nI love Ollama (amazing work on it!), it's what really got me in to trying LLMs. What was missing for my workflow was both a terminal application and Neovim plugin that actually *felt* like a chat app. [Oatmeal](https://github.com/dustinblackman/oatmeal) looks to deliver that with it's Ollama integration. \r\n\r\nHere's a fast demo to show it off using the Neovim editor integration. I hope it's something your users would find interesting as well.\r\n\r\n![oatmeal-demo](https://github.com/dustinblackman/oatmeal/assets/5246169/9ee5e910-4eff-4deb-8065-aeab8bfe6b00)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add installation packages category to community",
            "url": "https://github.com/ollama/ollama/pull/1221",
            "state": "MERGED",
            "createdAt": "2023-11-21T14:41:55Z",
            "mergedAt": "2023-11-21T20:12:23Z",
            "closedAt": "2023-11-21T20:12:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "Moved the arch package and someone has added a pr for brew. that needs to get updated to be a link.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix relative path on create",
            "url": "https://github.com/ollama/ollama/pull/1222",
            "state": "MERGED",
            "createdAt": "2023-11-21T17:06:12Z",
            "mergedAt": "2023-11-21T20:43:18Z",
            "closedAt": "2023-11-21T20:43:18Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 18,
            "deletions": 13,
            "body": "This fixes a regression in the API. Previously calling the API directly with a modelfile that has a relative file would work.\r\n\r\nEx:\r\n```\r\nFROM nous-capybara-34b.Q4_0.gguf\r\nTEMPLATE \"USER: { .Prompt } ASSISTANT: \"\r\n```\r\n\r\n```\r\ncurl -X POST http://localhost:11434/api/create -d '{\r\n    \"name\": \"bruce/nous-capybara\",\r\n    \"path\": \"/Users/bruce/models/nous-capybara/Modelfile\"\r\n}'\r\n```\r\n\r\npart of #1217",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Make alt+backspace delete word",
            "url": "https://github.com/ollama/ollama/pull/1223",
            "state": "MERGED",
            "createdAt": "2023-11-21T17:29:44Z",
            "mergedAt": "2023-11-21T20:26:47Z",
            "closedAt": "2023-11-21T20:26:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "In GNU Readline you can press alt+backspace to delete word. I'm used to this behavior and so it's jarring not to be able to do it. This commit adds the feature.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/1224",
            "state": "MERGED",
            "createdAt": "2023-11-21T17:50:16Z",
            "mergedAt": "2023-11-21T20:21:59Z",
            "closedAt": "2023-11-21T20:21:59Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 1,
            "deletions": 94,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add Llama Coder",
            "url": "https://github.com/ollama/ollama/pull/1225",
            "state": "MERGED",
            "createdAt": "2023-11-21T18:39:02Z",
            "mergedAt": "2023-11-21T19:08:20Z",
            "closedAt": "2023-11-21T19:08:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hey, i have created a plugin for VS Code that integrates with ollama!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update python client create example",
            "url": "https://github.com/ollama/ollama/pull/1227",
            "state": "MERGED",
            "createdAt": "2023-11-21T20:01:19Z",
            "mergedAt": "2023-11-27T20:36:20Z",
            "closedAt": "2023-11-27T20:36:20Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 67,
            "deletions": 9,
            "body": "When we updated our CLI to upload modelfile contents directly to the ollama server we missed updating the python example client. This change brings the logic in the python client in-line with our Go client",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use sha:256 prefix to indicate blob rather than @",
            "url": "https://github.com/ollama/ollama/pull/1228",
            "state": "CLOSED",
            "createdAt": "2023-11-21T20:04:30Z",
            "mergedAt": null,
            "closedAt": "2024-05-07T17:48:10Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "When uploading a remote blob we add an `@` sign in front of the `sha-256:` tag, this PR removes the `@` to achieve the same effect without requiring the additional step of adding the `@` to the digest",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "revert checksum calculation to calculate-as-you-go",
            "url": "https://github.com/ollama/ollama/pull/1229",
            "state": "MERGED",
            "createdAt": "2023-11-21T20:12:48Z",
            "mergedAt": "2023-11-30T18:54:38Z",
            "closedAt": "2023-11-30T18:54:38Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 33,
            "body": "calculating the checksum as it's being transferred is faster overall since the file doesn't need to be reread",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Community Integrations - Obsidian BMO Chatbot plugin",
            "url": "https://github.com/ollama/ollama/pull/1239",
            "state": "MERGED",
            "createdAt": "2023-11-22T12:42:36Z",
            "mergedAt": "2023-11-22T19:32:30Z",
            "closedAt": "2023-11-22T19:32:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "The simplicity and speed of Ollama is amazing!\r\n\r\nI would like to add Obsidian's \"BMO Chatbot\" plugin to the 'Community Integrations' section :)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "do not fail on unsupported template variables",
            "url": "https://github.com/ollama/ollama/pull/1244",
            "state": "MERGED",
            "createdAt": "2023-11-22T18:11:04Z",
            "mergedAt": "2023-12-06T21:23:04Z",
            "closedAt": "2023-12-06T21:23:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 5,
            "body": "- do not fail on unsupported parameters in model template\r\n\r\nresolves #1242",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: gguf int type",
            "url": "https://github.com/ollama/ollama/pull/1245",
            "state": "MERGED",
            "createdAt": "2023-11-22T19:40:43Z",
            "mergedAt": "2023-11-22T19:42:56Z",
            "closedAt": "2023-11-22T19:42:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor layer creation",
            "url": "https://github.com/ollama/ollama/pull/1250",
            "state": "MERGED",
            "createdAt": "2023-11-22T22:56:02Z",
            "mergedAt": "2023-12-05T22:32:52Z",
            "closedAt": "2023-12-05T22:32:52Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 7
            },
            "additions": 197,
            "deletions": 223,
            "body": "refactor layer creation\r\n\r\nprevious layer creation was not ideal because:\r\n\r\n1. it required reading the input file multiple times, once to calculate the sha256 checksum, another to write it to disk, and potentially one more to decode the underlying gguf\r\n2. used io.ReadSeeker which is prone to user error. if the file isn't reset correctly or in the right place, it could end up reading an empty file\r\n\r\nthere are also some brittleness when reading existing layers else\r\nwriting the inherited layers will error reading an already closed file\r\n\r\nthis commit aims to fix these issues by restructuring layer creation.\r\n\r\n1. it will now write the layer to a temporary file as well as the hash function and move it to the final location on Commit\r\n2. layers are read once when copied to the destination. exception is raw model files which still requires a second read to decode the model metadata\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "env variable to configure defaultSessionDuration",
            "url": "https://github.com/ollama/ollama/pull/1257",
            "state": "CLOSED",
            "createdAt": "2023-11-23T20:26:27Z",
            "mergedAt": null,
            "closedAt": "2024-05-07T23:47:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "This adds a simple environment variable to configure the defautSessionDuration that currently is hardcoded as 5 minutes.\r\n\r\nFixes issues #1048 and #931 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "warn if running a ggml model file",
            "url": "https://github.com/ollama/ollama/pull/1258",
            "state": "CLOSED",
            "createdAt": "2023-11-23T23:29:53Z",
            "mergedAt": null,
            "closedAt": "2023-11-24T19:02:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 72,
            "deletions": 10,
            "body": "If the model a user is running will the use ggml runtime log a warning that prompts them to check for update to try and pull the gguf version of the model.\r\n\r\n```\r\nollama run orca-mini\r\nThis model requires an update to work in future versions of Ollama. Check for update now? (y/n) y\r\npulling manifest\r\npulling 4de14feaabf8... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u258f(903 MB/903 MB)\r\npulling 8971eb8e89ce... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u258f(107 B/107 B)\r\npulling e7731c6d6962... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u258f(34 B/34 B)\r\npulling 905da7e7adc2... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u258f(76 B/76 B)\r\npulling 1bb164b05eb4... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u258f(460 B/460 B)\r\nverifying sha256 digest\r\nwriting manifest\r\nremoving any unused layers\r\nsuccess\r\n>>>\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Disable CUDA peer access as a workaround for multi-gpu inference bug",
            "url": "https://github.com/ollama/ollama/pull/1261",
            "state": "MERGED",
            "createdAt": "2023-11-24T06:20:51Z",
            "mergedAt": "2023-11-24T19:05:57Z",
            "closedAt": "2023-11-24T19:05:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "When CUDA peer access is enabled, multi-gpu inference will produce garbage output. This is a known bug of llama.cpp (or nvidia). Until the upstream bug https://github.com/ggerganov/llama.cpp/issues/3772 is fixed, we can disable CUDA peer access temporarily to ensure correct output.\r\n\r\nSee #961.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 7
            }
        }
    },
    {
        "node": {
            "title": "windows CUDA support",
            "url": "https://github.com/ollama/ollama/pull/1262",
            "state": "MERGED",
            "createdAt": "2023-11-24T06:26:31Z",
            "mergedAt": "2023-11-24T22:16:36Z",
            "closedAt": "2023-11-24T22:16:36Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 11,
            "deletions": 1,
            "body": "Fix #403 \r\n- Support cuda build in Windows\r\n- Import \"containerd/console\" lib to support colorful output in Windows terminal\r\n\r\n![image](https://github.com/jmorganca/ollama/assets/558657/0018234e-e61c-4c55-a627-dd667ffbbbdf)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "complete gguf upgrade",
            "url": "https://github.com/ollama/ollama/pull/1268",
            "state": "CLOSED",
            "createdAt": "2023-11-24T18:59:51Z",
            "mergedAt": null,
            "closedAt": "2023-12-15T19:39:08Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 19
            },
            "additions": 78,
            "deletions": 397,
            "body": "- remove ggml runner\r\n- automatically pull gguf models when ggml detected\r\n- tell users to update to gguf in the case automatic pull fails\r\n\r\nOn running a ggml model, a gguf model will be automatically pulled before running:\r\n```\r\nollama run orca-mini\r\nThis model is no longer compatible with Ollama. Pulling a new version...\r\npulling manifest \r\npulling 66002b78c70a... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(2.0 GB/2.0 GB)                             \r\npulling dd90d0f2b7ee... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(95 B/95 B)                                 \r\npulling 93ca9b3d83dc... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(89 B/89 B)                                 \r\npulling 33eb43a1488d... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(52 B/52 B)                                 \r\npulling fd52b10ee3ee... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(455 B/455 B)                               \r\nverifying sha256 digest \r\nwriting manifest \r\nremoving any unused layers \r\nsuccess \r\n>>> hello\r\n```\r\n\r\nWhen a gguf model is not available:\r\n```\r\nollama run custom-ggml\r\npulling manifest \r\nError: pull model manifest: file does not exist\r\nError: unsupported model, please update this model to gguf format\r\n```\r\n\r\nCreate from a GGML library model:\r\n```\r\nollama create mario -f ~/models/mario/Modelfile\r\ntransferring model data\r\nreading model metadata\r\nupdating base model\r\npulling manifest\r\npulling 22f7f8ef5f4c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(3.8 GB/3.8 GB)\r\npulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(7.0 KB/7.0 KB)\r\npulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(4.8 KB/4.8 KB)\r\npulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(59 B/59 B)\r\npulling 2759286baa87... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(105 B/105 B)\r\npulling 5407e3188df9... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f(529 B/529 B)\r\nverifying sha256 digest\r\nwriting manifest\r\nremoving any unused layers\r\nsuccess\r\n... etc\r\n```\r\n\r\nCreate from a custom ggml model:\r\n```\r\nollama create orca-ggml -f ~/models/orca-ggml/Modelfile\r\ntransferring model data\r\ncreating model layer\r\nError: model binary specified in FROM field is not a valid gguf format model, unsupported model format\r\n```\r\n\r\nAPI request from a GGML file error (same for embeddings and generate):\r\n```\r\n{\r\n  \"error\": \"unsupported model format: this model may be incompatible with your version of ollama. If you previously pulled this model, try updating it by running `ollama pull orca-ggml:latest`\"\r\n}\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix: disable ':' in tag names",
            "url": "https://github.com/ollama/ollama/pull/1280",
            "state": "MERGED",
            "createdAt": "2023-11-26T21:14:26Z",
            "mergedAt": "2023-11-29T18:33:45Z",
            "closedAt": "2023-11-29T18:33:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "Resolves #1247",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Amica to community integrations",
            "url": "https://github.com/ollama/ollama/pull/1281",
            "state": "MERGED",
            "createdAt": "2023-11-27T02:41:48Z",
            "mergedAt": "2023-11-27T15:44:37Z",
            "closedAt": "2023-11-27T15:44:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi!\r\n\r\nThis adds Amica (https://github.com/semperai/amica) to community integrations. \r\n\r\nUsing ollama is very simple with Amica:\r\n\r\n`settings -> chatbot -> chatbot backend -> select ollama`\r\n\r\nThanks for ollama it is a great project!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ignore jetbrain ides",
            "url": "https://github.com/ollama/ollama/pull/1287",
            "state": "MERGED",
            "createdAt": "2023-11-27T18:18:07Z",
            "mergedAt": "2023-11-27T20:57:45Z",
            "closedAt": "2023-11-27T20:57:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "ignore jetbrain ides",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Allow setting parameters in the REPL",
            "url": "https://github.com/ollama/ollama/pull/1294",
            "state": "MERGED",
            "createdAt": "2023-11-28T00:02:40Z",
            "mergedAt": "2023-11-29T17:56:42Z",
            "closedAt": "2023-11-29T17:56:42Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 153,
            "deletions": 86,
            "body": "This change adds a new `/set parameter` command inside the repl so that you can change parameters without having to recreate a modelfile.\r\n\r\nI have changed the `/show parameters` command to also reflect any parameters that have been set, however I haven't yet changed `/show modelfile` which should spit out a new modelfile which reflects the changes. That can come in a followup PR. Also not included in this PR are `/set template` and `/set system` which will come in a different PR.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add verbose request logs to server.",
            "url": "https://github.com/ollama/ollama/pull/1295",
            "state": "CLOSED",
            "createdAt": "2023-11-28T01:28:16Z",
            "mergedAt": null,
            "closedAt": "2024-05-07T23:46:51Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 52,
            "deletions": 0,
            "body": "Add verbose request logs to server.  Completes https://github.com/jmorganca/ollama/issues/1118\r\n\r\nexample output\r\n```\r\n2023/11/27 12:30:18 routes.go:736: Request POST - /api/generate; QueryParams: map[]; URLParams: []; Body: {\"model\":\"orca-mini\",\"prompt\":\"word up\",\"system\":\"\",\"template\":\"\",\"context\":[31822,13,8458,31922,3244,31871,13,3838,397,363,7421,8825,342,5243,10389,5164,828,31843,9530,362,988,362,365,473,31843,13,13,8458,31922,9779,31871,13,5521,397,365,13,13,8458,31922,13166,31871,13,312,705,363,7421,8825,29328,289,2803,365,288,2470,8931,291,1673,1132,31843,1035,473,312,955,365,1703,31902,13,8458,31922,3244,31871,13,3838,397,363,7421,8825,342,5243,10389,5164,828,31843,9530,362,988,362,365,473,31843,13,13,8458,31922,9779,31871,13,5521,397,365,13,13,8458,31922,13166,31871,13,312,705,363,7421,8825,29328,289,2803,365,288,2470,8931,291,1673,1132,31843,1035,473,312,955,365,1703,31902,13,8458,31922,3244,31871,13,3838,397,363,7421,8825,342,5243,10389,5164,828,31843,9530,362,988,362,365,473,31843,13,13,8458,31922,9779,31871,13,31824,3106,322,260,259,6285,313,13,13,8458,31922,13166,31871,13,312,25122,31844,504,312,705,432,1796,674,365,397,12065,289,31843,9410,365,3281,1673,541,3846,405,1132,562,266,5149,365,764,955,351,31902,13,8458,31922,3244,31871,13,3838,397,363,7421,8825,342,5243,10389,5164,828,31843,9530,362,988,362,365,473,31843,13,13,8458,31922,9779,31871,13,26755,13,13,8458,31922,13166,31871,13,312,31876,31836,10157,31844,504,362,363,7421,8825,31844,312,705,432,29328,289,1313,8931,342,6175,5000,31843,1725,4199,1717,322,289,2803,351,2470,8931,291,1673,1132,31843,1053,635,2492,2128,312,473,955,365,351,31902],\"format\":\"\",\"options\":null}; ClientIP: 127.0.0.1; Status: 200; UserAgent: ollama/0.0.0 (arm64 darwin) Go/go1.21.4; Duration: 5.217641542s\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update faq.md",
            "url": "https://github.com/ollama/ollama/pull/1299",
            "state": "MERGED",
            "createdAt": "2023-11-28T11:03:48Z",
            "mergedAt": "2023-11-28T14:54:42Z",
            "closedAt": "2023-11-28T14:54:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fix a typo in the CA update command",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct MacOS Host Port in FAQ",
            "url": "https://github.com/ollama/ollama/pull/1301",
            "state": "MERGED",
            "createdAt": "2023-11-28T12:11:17Z",
            "mergedAt": "2023-11-29T16:44:04Z",
            "closedAt": "2023-11-29T16:44:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "For some reason, the port for MacOS in this how-to was different then the one mentioned before and the one used after in the linux example. Skimming over this and copy pasting this as a Mac user, would result in the ollama program running on a different port and making it unreachable unless the port is changed in all other settings of all things using ollama.\r\n\r\nIf this was meant to show that you can use other ports, and wasnt just a typo, this would be a very bad way to do it. One single digit being different is nothing a normal person catches and then understands as \"You can do this as well\"",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "split from into one or more models",
            "url": "https://github.com/ollama/ollama/pull/1308",
            "state": "MERGED",
            "createdAt": "2023-11-28T21:35:43Z",
            "mergedAt": "2023-12-05T22:33:03Z",
            "closedAt": "2023-12-05T22:33:03Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 3
            },
            "additions": 233,
            "deletions": 83,
            "body": "this allows a single from line to contain multiple models, e.g. 2 in the case of llava models\r\n\r\nthis change adds a field `model_families` which is a replacement for the singular `model_family`. `model_family` will continue to exist for the time being",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Do no overwrite systemd service file",
            "url": "https://github.com/ollama/ollama/pull/1320",
            "state": "CLOSED",
            "createdAt": "2023-11-29T18:54:10Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T03:27:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 2,
            "body": "Currently during upgrade systemd file is lost, this fix avoid overwriting a file",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Fixed cuda repo location for rhel os",
            "url": "https://github.com/ollama/ollama/pull/1321",
            "state": "MERGED",
            "createdAt": "2023-11-29T19:28:42Z",
            "mergedAt": "2023-11-29T19:55:15Z",
            "closedAt": "2023-11-29T19:55:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "validate model tags on copy",
            "url": "https://github.com/ollama/ollama/pull/1323",
            "state": "MERGED",
            "createdAt": "2023-11-29T20:34:54Z",
            "mergedAt": "2023-11-29T20:54:29Z",
            "closedAt": "2023-11-29T20:54:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 21,
            "deletions": 2,
            "body": "Validate model tags and name on copy/create.\r\n\r\nResolves #1247\r\n\r\nExample errors:\r\n```\r\nollama create mario:test:1 -f ~/models/mario/Modelfile\r\ntransferring model data\r\nError: invalid model path: ':' (colon) is not allowed in tag names\r\n\r\nollama create :test -f ~/models/mario/Modelfile\r\ntransferring model data\r\nError: invalid model path: model repository name is required\r\n\r\nollama cp brxce/nous-capybara brxce/capybara:1:2\r\nError: invalid model path: ':' (colon) is not allowed in tag names\r\n\r\nollama cp brxce/nous-capybara :1:2\r\nError: invalid model path: model repository name is required\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Corrected transposed 129 to 192 for OLLAMA_ORIGINS example",
            "url": "https://github.com/ollama/ollama/pull/1325",
            "state": "MERGED",
            "createdAt": "2023-11-30T03:28:04Z",
            "mergedAt": "2023-11-30T03:44:18Z",
            "closedAt": "2023-11-30T03:44:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Issue:  \r\nDoc contained\r\n```\r\necho 'Environment=\"OLLAMA_ORIGINS=http://129.168.1.1:*,https://example.com\"' >>/etc/systemd/system/ollama.service.d/environment.conf\r\n```\r\nWhich, based on all other examples, should be 192.168.1.1\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "load projectors",
            "url": "https://github.com/ollama/ollama/pull/1334",
            "state": "MERGED",
            "createdAt": "2023-11-30T18:51:22Z",
            "mergedAt": "2023-12-05T22:40:53Z",
            "closedAt": "2023-12-05T22:40:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 62,
            "deletions": 27,
            "body": "continuation of #1250 and #1308 to load additional models\r\n\r\nThis adds model configurations to generate response:\r\n\r\n```\r\n$ curl -s localhost:11434/api/generate -d '{\"model\":\"llava:7b-v1.5-q4_0\"}' | jq .\r\n{\r\n  \"model\": \"llava:7b-v1.5-q4_0\",\r\n  \"created_at\": \"2023-12-01T19:41:43.684471Z\",\r\n  \"response\": \"\",\r\n  \"model_configuration\": {\r\n    \"model_format\": \"gguf\",\r\n    \"model_family\": \"llama\",\r\n    \"model_families\": [\r\n      \"llama\",\r\n      \"clip\"\r\n    ],\r\n    \"model_type\": \"7B\",\r\n    \"file_type\": \"Q4_0\"\r\n  },\r\n  \"done\": true\r\n}\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "allow setting the system and template for prompts in the repl",
            "url": "https://github.com/ollama/ollama/pull/1335",
            "state": "MERGED",
            "createdAt": "2023-11-30T21:34:44Z",
            "mergedAt": "2023-12-01T17:28:36Z",
            "closedAt": "2023-12-01T17:28:36Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 87,
            "deletions": 21,
            "body": "This change allows setting the system prompt and the prompt template in the repl. It works with both single lines, and with multiline input.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docker: set PATH, LD_LIBRARY_PATH, and capabilities",
            "url": "https://github.com/ollama/ollama/pull/1336",
            "state": "MERGED",
            "createdAt": "2023-12-01T00:32:32Z",
            "mergedAt": "2023-12-01T05:16:56Z",
            "closedAt": "2023-12-01T05:16:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix adapter loading from SHA hash",
            "url": "https://github.com/ollama/ollama/pull/1347",
            "state": "MERGED",
            "createdAt": "2023-12-01T18:56:13Z",
            "mergedAt": "2023-12-01T19:08:26Z",
            "closedAt": "2023-12-01T19:08:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "When I tried to load an adapter from a local .bin file (by using `ollama create`), the command errored out with `Error: open /@sha256:ac5d2a80bc8bb4e9be08302373191c842a7edd3fcefd346d93e84941313b1c5a: no such file or directory`. I believe this is because the sha hash was misinterpreted as a filepath. I found where this is handled in `images.go` and saw that `model`s resolve hashes correctly but adapters do not. I don't have context on the past implementations of adapters, but maybe at one point the adapters were not hashed/versioned?\r\n\r\nI hope this addresses the issue! One small thought is that this block of code might be able to be placed before the switch to reduce duplicated logic for `model`. Not sure if that'd have an impact on the other cases.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "handle ctrl+z",
            "url": "https://github.com/ollama/ollama/pull/1349",
            "state": "MERGED",
            "createdAt": "2023-12-02T00:04:30Z",
            "mergedAt": "2023-12-02T00:21:49Z",
            "closedAt": "2023-12-02T00:21:49Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "resolves #1332",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "make linewrap still work when the terminal width has changed",
            "url": "https://github.com/ollama/ollama/pull/1350",
            "state": "MERGED",
            "createdAt": "2023-12-02T00:18:40Z",
            "mergedAt": "2023-12-04T22:14:56Z",
            "closedAt": "2023-12-04T22:14:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adding Cross-Origin Support for Web Apps",
            "url": "https://github.com/ollama/ollama/pull/1357",
            "state": "CLOSED",
            "createdAt": "2023-12-03T02:49:05Z",
            "mergedAt": null,
            "closedAt": "2024-07-17T07:22:11Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 293,
            "deletions": 9,
            "body": "This pull request introduces a new feature that allows users to grant cross-origin access to web applications. This will allow easier setup for web applications that would like to access the local Ollama API.\r\n\r\nChanges Made:\r\n- Added an endpoint that allows web applications to request using the local Ollama API\r\n  - The user is prompted for consent\r\n- Added an endpoint to view all authorizations\r\n- Added an endpoint to revoke an authorization\r\n- Updated documentation to provide clear instructions on configuring cross-origin access for web applications.\r\n\r\nAddress:  #433 #300\r\n\r\nScreenshot:\r\nOpenSuse - Gnome (Linux): \r\n![Screenshot from 2023-12-03 03-38-10](https://github.com/jmorganca/ollama/assets/2102243/a3b19a3b-3441-4931-9a41-3c8d7d2b561c)\r\n\r\nMacOS:\r\n<img width=\"954\" alt=\"image\" src=\"https://github.com/jmorganca/ollama/assets/2102243/910ea876-8bd5-45c9-9fcd-9ae81e90b232\">\r\n\r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Ollama Telegram Bot",
            "url": "https://github.com/ollama/ollama/pull/1364",
            "state": "MERGED",
            "createdAt": "2023-12-03T17:16:56Z",
            "mergedAt": "2023-12-03T19:19:55Z",
            "closedAt": "2023-12-03T19:19:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "This pull request adds [telegram-ollama](https://github.com/ruecat/ollama-telegram) to [Extensions & Plugins](https://github.com/jmorganca/ollama/commit/41f73433bbf607160f2356388463de42714f2d23) section\r\nI created a bot for telegram, it uses aiogram and can stream API requests in one message, without ratelimit.\r\nSoon it will get Docker support and other features.\r\nThanks!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "install: fix rocky kernel packages",
            "url": "https://github.com/ollama/ollama/pull/1376",
            "state": "MERGED",
            "createdAt": "2023-12-04T19:20:27Z",
            "mergedAt": "2023-12-04T22:23:43Z",
            "closedAt": "2023-12-04T22:23:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "package names for rocky-linux are slightly different",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update for qwen",
            "url": "https://github.com/ollama/ollama/pull/1377",
            "state": "MERGED",
            "createdAt": "2023-12-04T19:38:17Z",
            "mergedAt": "2023-12-06T20:31:51Z",
            "closedAt": "2023-12-06T20:31:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "revert cli to use /api/generate",
            "url": "https://github.com/ollama/ollama/pull/1383",
            "state": "MERGED",
            "createdAt": "2023-12-05T00:27:13Z",
            "mergedAt": "2023-12-05T00:35:30Z",
            "closedAt": "2023-12-05T00:35:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 119,
            "deletions": 115,
            "body": "This change reverts the CLI to use `/api/generate` instead of `/api/chat`.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chat api endpoint",
            "url": "https://github.com/ollama/ollama/pull/1392",
            "state": "MERGED",
            "createdAt": "2023-12-05T18:57:45Z",
            "mergedAt": "2023-12-05T19:57:33Z",
            "closedAt": "2023-12-05T19:57:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 9
            },
            "additions": 551,
            "deletions": 133,
            "body": "- add a new `/api/chat` API endpoint that takes an array of `message` objects. This endpoint is an alternative to `/api/generate`.\r\n- deprecate generation context and template, but continue to support them\r\n- rebuild chat content from messages\r\n\r\nThis changes adds a `/api/chat` endpoint to the API which takes an array of messages. This makes modifying and tracking the history on the fly much simpler. It is an alternative to prompt/response.\r\n\r\n`context` will continue to work as expected for now, but at some point in the future we may want to replace it completely with `/api/chat`.\r\n\r\n```\r\ncurl -X POST http://localhost:11434/api/generate -d '{\r\n    \"model\": \"mistral\",\r\n    \"prompt\": \"hello, how are you?\"\r\n}'\r\n\r\nOR\r\n\r\n### Basic generate request with messages\r\ncurl -X POST http://localhost:11434/api/chat -d '{\r\n    \"model\": \"mistral\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"why is the sky blue?\"\r\n        }\r\n    ]\r\n}'\r\n```\r\n\r\nresolves #981 \r\nresolves #1203 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "fix: trim space in modelfile fields",
            "url": "https://github.com/ollama/ollama/pull/1393",
            "state": "MERGED",
            "createdAt": "2023-12-05T19:58:30Z",
            "mergedAt": "2023-12-05T20:18:01Z",
            "closedAt": "2023-12-05T20:18:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 3,
            "body": "only trim whitespace for FROM, ADAPTER, and PARAMETER since whitespace in LICENSE, TEMPLATE, SYSTEM might be significant\r\n\r\nresolves #1390\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Simple chat example",
            "url": "https://github.com/ollama/ollama/pull/1409",
            "state": "MERGED",
            "createdAt": "2023-12-06T22:36:33Z",
            "mergedAt": "2023-12-06T23:49:46Z",
            "closedAt": "2023-12-06T23:49:46Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 70,
            "deletions": 0,
            "body": "Simple example using Bruce's chat endpoint",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/1412",
            "state": "MERGED",
            "createdAt": "2023-12-07T02:24:20Z",
            "mergedAt": "2023-12-11T23:05:10Z",
            "closedAt": "2023-12-11T23:05:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Adding integration to databases section",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Simple chat example for typescript",
            "url": "https://github.com/ollama/ollama/pull/1419",
            "state": "MERGED",
            "createdAt": "2023-12-07T19:49:25Z",
            "mergedAt": "2023-12-07T22:42:24Z",
            "closedAt": "2023-12-07T22:42:24Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 117,
            "deletions": 0,
            "body": "A simple example of the chat endpoint",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "os specific ctrl-z",
            "url": "https://github.com/ollama/ollama/pull/1420",
            "state": "MERGED",
            "createdAt": "2023-12-07T20:03:08Z",
            "mergedAt": "2023-12-11T15:48:15Z",
            "closedAt": "2023-12-11T15:48:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 25,
            "deletions": 8,
            "body": "Add OS specific readline functions. Windows does not support these suspend system calls, so make ctrl-z a no-op on windows. This fixes development windows native builds.\r\n\r\nresolves #1414",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "fix redundant newline",
            "url": "https://github.com/ollama/ollama/pull/1421",
            "state": "MERGED",
            "createdAt": "2023-12-07T21:44:54Z",
            "mergedAt": "2023-12-07T21:47:23Z",
            "closedAt": "2023-12-07T21:47:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: encode full previous prompt in context",
            "url": "https://github.com/ollama/ollama/pull/1424",
            "state": "MERGED",
            "createdAt": "2023-12-08T00:15:16Z",
            "mergedAt": "2023-12-08T21:53:52Z",
            "closedAt": "2023-12-08T21:53:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "I believe this is a bug from the chat API changes, only the most recent request/response was added to the context history. It should be the full prompt after rebuilding the history. This shouldn't be in any released version.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: restore modelfile system in prompt template",
            "url": "https://github.com/ollama/ollama/pull/1425",
            "state": "MERGED",
            "createdAt": "2023-12-08T00:28:05Z",
            "mergedAt": "2023-12-08T19:20:19Z",
            "closedAt": "2023-12-08T19:20:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "In #1244 this line which sets the modelfile system variable in the template got removed. It must still be there to apply the system template from the modelfile. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: only flush template in chat when current role encountered",
            "url": "https://github.com/ollama/ollama/pull/1426",
            "state": "MERGED",
            "createdAt": "2023-12-08T00:51:04Z",
            "mergedAt": "2023-12-08T21:44:24Z",
            "closedAt": "2023-12-08T21:44:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 92,
            "deletions": 15,
            "body": "There was a bug in the /chat endpoint here during templating that resulted in the prompt template being written incorrectly. \r\n\r\nIf a `user` was encountered when the `system` was already set the template would be written before the `user` content was set. This was not correct. The template should only be written when the exact role has been encountered before.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "post-response templating",
            "url": "https://github.com/ollama/ollama/pull/1427",
            "state": "MERGED",
            "createdAt": "2023-12-08T00:56:14Z",
            "mergedAt": "2023-12-22T22:07:05Z",
            "closedAt": "2023-12-22T22:07:05Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 3
            },
            "additions": 333,
            "deletions": 15,
            "body": "- add post-response templating to /generate\r\n- add post-response templating to /chat\r\n- add templating tests\r\n\r\nA common format for LLM templating may include post-response templating. Our current template format kind of supported this by checking `{{ if not .First }}` but it is confusing to read. This change allows post-response templating to be applied.\r\n\r\nHere is an example of a format that is now supported:\r\n```\r\n<|im_start|>user\r\n{{ .Prompt }}<|im_end|>\r\n<|im_start|>assistant\r\n{{ .Response }}<|im_end|>\r\n```\r\n\r\nCurrent templates are not effected.\r\n\r\nFollow-up: docs\r\n\r\nResolves: #1423 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "document response in modelfile template variables",
            "url": "https://github.com/ollama/ollama/pull/1428",
            "state": "MERGED",
            "createdAt": "2023-12-08T01:02:21Z",
            "mergedAt": "2024-01-08T19:38:51Z",
            "closedAt": "2024-01-08T19:38:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 5,
            "body": "Document #1427, to be merged on next release",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "\ud83d\udee0\ufe0f Add service activation prompt",
            "url": "https://github.com/ollama/ollama/pull/1440",
            "state": "CLOSED",
            "createdAt": "2023-12-08T20:51:25Z",
            "mergedAt": null,
            "closedAt": "2024-06-09T18:07:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 2,
            "body": "Closes #1352 \r\n\r\n### Key Changes:\r\n- Added `ask_to_activate_service` function to prompt users for service activation post-installation.\r\n- Integrated the prompt in the script's flow, allowing conditional execution of systemd service configuration.\r\n\r\n\r\n### Impact:\r\n- Improves user experience by providing a choice to activate the service.\r\n- Ensures clearer installation process, aligning with user expectations.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Added mention of the NOPRUNE env var",
            "url": "https://github.com/ollama/ollama/pull/1444",
            "state": "CLOSED",
            "createdAt": "2023-12-09T01:38:51Z",
            "mergedAt": null,
            "closedAt": "2023-12-12T01:15:00Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "OLLAMA_NOPRUNE will prevent the pruning process from running, but it isn't mentioned anywhere outside of the code and a merged PR.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: parallel queueing race condition caused silent failure",
            "url": "https://github.com/ollama/ollama/pull/1445",
            "state": "MERGED",
            "createdAt": "2023-12-09T01:58:05Z",
            "mergedAt": "2023-12-09T19:14:02Z",
            "closedAt": "2023-12-09T19:14:02Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 30,
            "deletions": 26,
            "body": "As of the most recent llama.cpp update concurrent requests had a race condition that would result in an empty response.\r\n\r\nThis was not easy to observe since the response from the llm runner subprocess was a 200 with the error {\"content\":\"slot unavailable\"} in the response stream, which just silently closed the channel.\r\n\r\nThis change resolves this by allowing multiple slots in the llm runner subprocess. We manage the queueing ourselves so this should be ok. @dhiltgen this may be a case we need to account for in the cgo changes.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update readmes, requirements, packagejsons, etc for all examples",
            "url": "https://github.com/ollama/ollama/pull/1452",
            "state": "MERGED",
            "createdAt": "2023-12-10T06:34:15Z",
            "mergedAt": "2023-12-22T17:10:41Z",
            "closedAt": "2023-12-22T17:10:41Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 26
            },
            "additions": 345,
            "deletions": 100,
            "body": "Most of the examples needed updates of Readmes to show how to run them. Some of the requirements.txt files had extra content that wasn't needed, or missing altogether. Apparently some folks like to run npm start to run typescript, so a script was added to all typescript examples which hadn't been done before.\r\n\r\nBasically just a lot of cleanup.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Added Bionic GPT as a front end.",
            "url": "https://github.com/ollama/ollama/pull/1463",
            "state": "MERGED",
            "createdAt": "2023-12-11T08:01:18Z",
            "mergedAt": "2023-12-15T19:33:04Z",
            "closedAt": "2023-12-15T19:33:04Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove per-model types",
            "url": "https://github.com/ollama/ollama/pull/1469",
            "state": "MERGED",
            "createdAt": "2023-12-11T17:40:49Z",
            "mergedAt": "2023-12-12T20:27:03Z",
            "closedAt": "2023-12-12T20:27:03Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 0,
            "deletions": 64,
            "body": "mostly replaced by decoding tensors except ggml models which only support llama",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix Readme \"Database -> MindsDB\" link",
            "url": "https://github.com/ollama/ollama/pull/1479",
            "state": "MERGED",
            "createdAt": "2023-12-12T10:20:25Z",
            "mergedAt": "2023-12-12T15:26:13Z",
            "closedAt": "2023-12-12T15:26:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This pull request fixes markdown (\"MindsDB\" link in Readme)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "retry on concurrent request failure",
            "url": "https://github.com/ollama/ollama/pull/1483",
            "state": "MERGED",
            "createdAt": "2023-12-12T17:10:54Z",
            "mergedAt": "2023-12-12T17:14:35Z",
            "closedAt": "2023-12-12T17:14:35Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 98,
            "deletions": 77,
            "body": "- remove parallel\r\n- retry concurrent requests on failure",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "exponential back-off",
            "url": "https://github.com/ollama/ollama/pull/1484",
            "state": "MERGED",
            "createdAt": "2023-12-12T17:27:55Z",
            "mergedAt": "2023-12-12T17:33:03Z",
            "closedAt": "2023-12-12T17:33:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "Exponential back-off on concurrent slot failure.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix issues with `/set template` and `/set system`",
            "url": "https://github.com/ollama/ollama/pull/1486",
            "state": "MERGED",
            "createdAt": "2023-12-12T19:23:05Z",
            "mergedAt": "2023-12-12T19:43:20Z",
            "closedAt": "2023-12-12T19:43:20Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 38,
            "deletions": 37,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added message format for chat api",
            "url": "https://github.com/ollama/ollama/pull/1488",
            "state": "MERGED",
            "createdAt": "2023-12-12T20:34:01Z",
            "mergedAt": "2023-12-13T16:21:23Z",
            "closedAt": "2023-12-13T16:21:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add image support to the chat api",
            "url": "https://github.com/ollama/ollama/pull/1490",
            "state": "MERGED",
            "createdAt": "2023-12-12T20:37:22Z",
            "mergedAt": "2023-12-12T21:28:58Z",
            "closedAt": "2023-12-12T21:28:58Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 14,
            "deletions": 10,
            "body": "This change allows multimodal vision models to be able to be used in the chat API.\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Clean up documentation",
            "url": "https://github.com/ollama/ollama/pull/1506",
            "state": "MERGED",
            "createdAt": "2023-12-13T17:49:24Z",
            "mergedAt": "2023-12-22T17:10:01Z",
            "closedAt": "2023-12-22T17:10:01Z",
            "reviews": {
                "totalCount": 62
            },
            "files": {
                "totalCount": 7
            },
            "additions": 367,
            "deletions": 236,
            "body": "Will probably need to update with PRs for new release.\r\n\r\nThis accomplishes a few things. \r\n\r\n- First it looks at the api docs, makes them a bit more consistent, and fixes the requests and responses to reflect how they actually work today. \r\n- It creates a better README for the docs folder\r\n- Creates a DeepDive for understanding the files and layers using the api (inspired by the gdev doc Jeff shared)\r\n- Creates a troubleshooting guide that starts to share the common solutions to some error messages that pop up.\r\n- Moves some of the FAQ questions into more appropriate places in the docs.\r\n\r\nI think that\u2019s it.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "restore model load duration on generate response",
            "url": "https://github.com/ollama/ollama/pull/1524",
            "state": "MERGED",
            "createdAt": "2023-12-14T17:01:29Z",
            "mergedAt": "2023-12-14T17:15:50Z",
            "closedAt": "2023-12-14T17:15:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 27,
            "deletions": 36,
            "body": "- set model load duration on generate and chat done response\r\n- calculate createAt time when response created\r\n\r\nresolves #1523 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove sample_count from docs",
            "url": "https://github.com/ollama/ollama/pull/1527",
            "state": "MERGED",
            "createdAt": "2023-12-14T17:47:56Z",
            "mergedAt": "2023-12-14T22:49:01Z",
            "closedAt": "2023-12-14T22:49:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 12,
            "body": "this info has not been returned from these endpoints in some time",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add unit test of API routes",
            "url": "https://github.com/ollama/ollama/pull/1528",
            "state": "MERGED",
            "createdAt": "2023-12-14T22:35:10Z",
            "mergedAt": "2023-12-15T00:47:40Z",
            "closedAt": "2023-12-15T00:47:40Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 122,
            "deletions": 30,
            "body": "This change modifies the base server to allow it to be more easily unit tested. It also adds in a simple unit test to \"/api/version\" to demonstrate how to add unit tests in the future.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README with Enchanted iOS App",
            "url": "https://github.com/ollama/ollama/pull/1529",
            "state": "MERGED",
            "createdAt": "2023-12-14T22:35:25Z",
            "mergedAt": "2023-12-15T19:37:29Z",
            "closedAt": "2023-12-15T19:37:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "I have just released iOS mobile App for Ollama and wanted to share with the community. A lot of improvements are coming up soon.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "send empty messages on last chat response",
            "url": "https://github.com/ollama/ollama/pull/1530",
            "state": "MERGED",
            "createdAt": "2023-12-14T22:44:19Z",
            "mergedAt": "2023-12-18T19:23:38Z",
            "closedAt": "2023-12-18T19:23:38Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 10,
            "body": "Send an empty message on the last chat response rather than omitting it. This makes the chat API match the generate API.\r\n\r\nAs of this change...\r\n\r\n`/generate`\r\n```\r\ncurl http://localhost:11434/api/generate -d '{\r\n    \"model\": \"mistral\"\r\n}'\r\n\r\nHTTP/1.1 200 OK\r\nContent-Type: application/json; charset=utf-8\r\nDate: Thu, 14 Dec 2023 22:42:13 GMT\r\nContent-Length: 88\r\nConnection: close\r\n\r\n{\r\n  \"model\": \"mistral\",\r\n  \"created_at\": \"2023-12-14T22:42:13.365069Z\",\r\n  \"response\": \"\",\r\n  \"done\": true\r\n}\r\n```\r\n```\r\ncurl http://localhost:11434/api/generate -d '{\r\n    \"model\": \"mistral\",\r\n    \"prompt\": \"reply with a single word\"\r\n}'\r\n\r\nHTTP/1.1 200 OK\r\nContent-Type: application/x-ndjson\r\nDate: Thu, 14 Dec 2023 22:43:05 GMT\r\nConnection: close\r\nTransfer-Encoding: chunked\r\n\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:43:05.60168Z\",\"response\":\" Okay\",\"done\":false}\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:43:05.616406Z\",\"response\":\".\",\"done\":false}\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:43:05.631163Z\",\"response\":\"\",\"done\":true,\"context\":[733,16289,28793,28705,10071,395,264,2692,1707,733,28748,16289,28793,19811,28723],\"total_duration\":414743625,\"load_duration\":760500,\"prompt_eval_count\":14,\"prompt_eval_duration\":393924000,\"eval_count\":2,\"eval_duration\":14727000}\r\n```\r\n\r\n`/chat`\r\n```\r\ncurl http://localhost:11434/api/chat -d '{\r\n    \"model\": \"mistral\"\r\n}'\r\n\r\nHTTP/1.1 200 OK\r\nContent-Type: application/json; charset=utf-8\r\nDate: Thu, 14 Dec 2023 22:41:56 GMT\r\nContent-Length: 132\r\nConnection: close\r\n\r\n{\r\n  \"model\": \"mistral\",\r\n  \"created_at\": \"2023-12-14T22:41:56.540246Z\",\r\n  \"message\": {\r\n    \"role\": \"assistant\",\r\n    \"content\": \"\",\r\n    \"images\": null\r\n  },\r\n  \"done\": true\r\n}\r\n```\r\n```\r\ncurl http://localhost:11434/api/chat -d '{\r\n    \"model\": \"mistral\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"reply with one word\"\r\n        }\r\n    ]\r\n}'\r\nHTTP/1.1 200 OK\r\nContent-Type: application/x-ndjson\r\nDate: Thu, 14 Dec 2023 22:38:36 GMT\r\nConnection: close\r\nTransfer-Encoding: chunked\r\n\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:38:36.625168Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Okay\",\"images\":null},\"done\":false}\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:38:36.639622Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\",\"images\":null},\"done\":false}\r\n{\"model\":\"mistral\",\"created_at\":\"2023-12-14T22:38:36.654161Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\",\"images\":null},\"done\":true,\"total_duration\":417861167,\"load_duration\":927584,\"prompt_eval_count\":13,\"prompt_eval_duration\":396099000,\"eval_count\":2,\"eval_duration\":14502000}\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add API tests for list handler",
            "url": "https://github.com/ollama/ollama/pull/1535",
            "state": "MERGED",
            "createdAt": "2023-12-15T02:16:09Z",
            "mergedAt": "2023-12-15T02:18:25Z",
            "closedAt": "2023-12-15T02:18:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 61,
            "deletions": 0,
            "body": "This change adds some tests for the `GET /api/list` endpoint. It includes a test that gets no models, and one that returns a single entry.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add API create/copy handlers",
            "url": "https://github.com/ollama/ollama/pull/1541",
            "state": "MERGED",
            "createdAt": "2023-12-15T06:20:11Z",
            "mergedAt": "2023-12-15T19:59:18Z",
            "closedAt": "2023-12-15T19:59:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 75,
            "deletions": 12,
            "body": "This change adds a test for calling `POST /api/create` which creates a new model.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update required go version",
            "url": "https://github.com/ollama/ollama/pull/1544",
            "state": "MERGED",
            "createdAt": "2023-12-15T16:38:44Z",
            "mergedAt": "2023-12-15T19:15:57Z",
            "closedAt": "2023-12-15T19:15:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "go 1.21 is required to build ollama, update the go.mod to reflect this\r\n\r\nresolves #1515 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "allow for starting llava queries with filepath",
            "url": "https://github.com/ollama/ollama/pull/1549",
            "state": "MERGED",
            "createdAt": "2023-12-15T18:02:18Z",
            "mergedAt": "2023-12-21T18:21:00Z",
            "closedAt": "2023-12-21T18:21:00Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 5,
            "body": "- shared cmd file pattern regex\r\n- do not error when multimodal has starting file\r\n\r\nThere was a bug that a multimodal query starting with an unescaped file path resulted in an \"Unknown command\" error.\r\n\r\nThis change allows for multimodal models to have queries starting with the filepath.\r\n\r\n```\r\n$ ollama run llava\r\n>>> /test\r\nUnknown command '/test'. Type /? for help\r\n>>> /Users/bruce/Downloads/Ollama_christmas_background.png what is this\r\nAdded image '/Users/bruce/Downloads/Ollama_christmas_background.png'\r\n This is a cartoon of cats, and it appears to be an illustration or drawing of the animals. The scene depicts them gathered around a Christmas tree. In total, there are nine cats in various positions, with some standing close together while others are more spread out. One cat can be seen sitting underneath the \r\ntree, perhaps enjoying the holiday festivities. The overall atmosphere seems to be joyful and playful as these cats interact and pose around their Christmas tree setting.\r\n\r\n$ ollama run llama2\r\n>>> /Users/bruce/Downloads/Ollama_christmas_background.png what is this\r\nUnknown command '/Users/bruce/Downloads/Ollama_christmas_background.png'. Type /? for help\r\n```\r\n\r\nresolves #1511",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add lint and test on pull_request",
            "url": "https://github.com/ollama/ollama/pull/1552",
            "state": "MERGED",
            "createdAt": "2023-12-15T19:34:07Z",
            "mergedAt": "2024-01-11T17:37:46Z",
            "closedAt": "2024-01-11T17:37:46Z",
            "reviews": {
                "totalCount": 19
            },
            "files": {
                "totalCount": 17
            },
            "additions": 141,
            "deletions": 82,
            "body": "fixes a bug with generate where `get_flags` errors on ubuntu (and likely windows) when building cuda on a cuda-less system\r\n\r\nfixes a bug in windows where `/api/list` does not return models correctly\r\n\r\nboth lint and test requires go generate results so do it once then propagate the artifacts to the rest of the pipeline\r\n\r\nthis enables linting with golangci-lint but doesn't go overboard with linters. it enables the default linters and only a few extra linters to catch the most egregious bugs\r\n\r\nlinting can run locally with [golangci-lint](https://golangci-lint.run/usage/install/)\r\n\r\n```\r\ngo install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.55.2\r\ngolangci-lint run -v\r\n```\r\n\r\n```\r\ndocker run --rm -v $(pwd):/app -w /app golangci/golangci-lint:v1.55.2 golangci-lint run -v\r\n```\r\n\r\nresolves #1539 ",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add magic header for unit tests",
            "url": "https://github.com/ollama/ollama/pull/1558",
            "state": "MERGED",
            "createdAt": "2023-12-16T02:01:01Z",
            "mergedAt": "2023-12-18T18:41:02Z",
            "closedAt": "2023-12-18T18:41:02Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 11,
            "body": "This change adds in the magic GGUF header for the temporary model image layer.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Langchain Dart library",
            "url": "https://github.com/ollama/ollama/pull/1564",
            "state": "MERGED",
            "createdAt": "2023-12-16T11:04:46Z",
            "mergedAt": "2023-12-19T19:04:53Z",
            "closedAt": "2023-12-19T19:04:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Lets get rid of these old modelfile examples",
            "url": "https://github.com/ollama/ollama/pull/1592",
            "state": "MERGED",
            "createdAt": "2023-12-19T01:48:00Z",
            "mergedAt": "2023-12-19T04:29:49Z",
            "closedAt": "2023-12-19T04:29:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 11
            },
            "additions": 0,
            "deletions": 186,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added cmdh to community section in README",
            "url": "https://github.com/ollama/ollama/pull/1595",
            "state": "MERGED",
            "createdAt": "2023-12-19T03:09:13Z",
            "mergedAt": "2023-12-19T04:55:18Z",
            "closedAt": "2023-12-19T04:55:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Added a link to my terminal application cmdh, which lets you request linux commands using an LLM (https://github.com/pgibler/cmdh). I just added ollama support today. The tl;dr is it sends your command request to the LLM which outputs a terminal command that matches the request. You can then use a hotkey to run the command. It's saved me hours of looking through documentation and can push out pretty complex results.\r\n\r\nThank you for creating & maintaining ollama!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "First take at a community resources page of blogs, tutorials, videos",
            "url": "https://github.com/ollama/ollama/pull/1596",
            "state": "CLOSED",
            "createdAt": "2023-12-19T04:46:48Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T01:55:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 41,
            "deletions": 0,
            "body": "we need a community page in the docs for blogs, videos, and tutorials. Tools that use Ollama will still go on the front readme.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "updating development document for clarity and legibility",
            "url": "https://github.com/ollama/ollama/pull/1598",
            "state": "CLOSED",
            "createdAt": "2023-12-19T05:48:59Z",
            "mergedAt": null,
            "closedAt": "2024-03-07T22:03:27Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 74,
            "deletions": 80,
            "body": "Updating the development docs with:\r\n* organization based on OS\r\n* adding helpful links for linux based installs",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix: set template without triple quotes",
            "url": "https://github.com/ollama/ollama/pull/1614",
            "state": "MERGED",
            "createdAt": "2023-12-19T19:37:53Z",
            "mergedAt": "2024-01-09T17:36:25Z",
            "closedAt": "2024-01-09T17:36:25Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 67,
            "deletions": 55,
            "body": "this changes updates `/set` to better handle multiline strings. `/set` now correctly sets template or system without using triple quotes\r\n\r\n```\r\n>>> /set template {{ .Prompt }}\r\nSet prompt template.\r\n```\r\n\r\nadditionally, use a strings.Builder instead of concatenating string values for prompt building\r\n\r\n```\r\n>>> \"\"\"hello\r\n... world\"\"\"\r\n```\r\n\r\n```\r\n>>> \"\"\"\r\n... hello\r\n... world\r\n... \"\"\"\r\n```\r\n\r\n```\r\n>>> /set system \"\"\"\r\n... you are a llama\r\n... \"\"\"\r\n```\r\n\r\n\r\n```\r\n>>> /set template \"\"\"\r\n... {{.System}}\r\n... User: {{.Prompt}}\r\n... Assistant: {{.Response}}\r\n... \"\"\"\r\n```\r\n\r\nresolves #1609\r\nresolves #1607",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix(test): use real version string for comparison",
            "url": "https://github.com/ollama/ollama/pull/1619",
            "state": "MERGED",
            "createdAt": "2023-12-19T23:05:48Z",
            "mergedAt": "2023-12-19T23:48:53Z",
            "closedAt": "2023-12-19T23:48:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "nix passes in the any configured build flags to test which means it's being run with, among other things, `ldflags`, e.g.:\r\n\r\n```\r\ngo test -v -ldflags='-X github.com/jmorganca/ollama/version.Version=x.y.z' ./...\r\n```\r\n\r\nresolves #1613 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update Readme Quickstart Ollama with Docker",
            "url": "https://github.com/ollama/ollama/pull/1622",
            "state": "CLOSED",
            "createdAt": "2023-12-20T06:04:12Z",
            "mergedAt": null,
            "closedAt": "2024-06-09T18:06:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 23,
            "deletions": 0,
            "body": "# Update Readme Quickstart Ollama with Docker\r\n\r\nUpon initial exploration of the repository, leveraging Docker for getting started appears to be the most straightforward approach. Following the [Ollama documentation for initiating with Docker](https://github.com/jmorganca/ollama?tab=readme-ov-file#docker) led me to the [Ollama Docker Image](https://hub.docker.com/r/ollama/ollama).\r\n\r\nSubsequently, I launched Ollama using the command:\r\n\r\n```shell\r\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\r\n```\r\n\r\nHowever, beyond this point, there lacks guidance on the subsequent steps.\r\n\r\nThis pull request aims to address this gap by incorporating instructions to guide users from spinning up the Docker image to utilizing the chat API.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "adds ooo to Community Integrations in README",
            "url": "https://github.com/ollama/ollama/pull/1623",
            "state": "MERGED",
            "createdAt": "2023-12-20T08:14:18Z",
            "mergedAt": "2024-03-25T19:08:33Z",
            "closedAt": "2024-03-25T19:08:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Adds a link to a terminal command (https://github.com/npahlfer/ooo) that lets you pipe in outputs from other terminal commands \"into\" Ollama and parse them through your prompt.\r\nThis way you can parse command outputs in an easy way!\r\nYou can also just prompt Ollama like you normally would eg. `$ ooo how long is a rope`.\r\n\r\nThanks, I love your work!\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Cache option #1573",
            "url": "https://github.com/ollama/ollama/pull/1642",
            "state": "MERGED",
            "createdAt": "2023-12-20T20:24:14Z",
            "mergedAt": "2023-12-22T22:16:20Z",
            "closedAt": "2023-12-22T22:16:20Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 6,
            "deletions": 2,
            "body": "This PR, adds the API option \"cache\", that allows the llama.cpp server to cache our prompt Eval and the response.\r\nThis speed-up follow-up calls immensely for some models, if you use it over the API, with the same prompt (or even partial ones), it will speed up subsequent calls, since it skips the evaluation of the prompt.\r\n\r\nAlso, this PR includes commands /set cache and /set nocache to give users the ability to enable prompt caching in the official CLI.\r\n\r\n\r\n* Add a new entry \"cache\" to the options object that is passed to the worker\r\n* Add commands /set cache and /set nocache to allow this in the repl cli\r\n* Update docs\r\n\r\n\r\nThis is a partial fix for, Enable prompt cache #1573, we might need to patch llama.cpp at some point to allow us full flexibility.",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 11
            }
        }
    },
    {
        "node": {
            "title": "add FAQ for slow networking in WSL2",
            "url": "https://github.com/ollama/ollama/pull/1646",
            "state": "MERGED",
            "createdAt": "2023-12-20T23:03:32Z",
            "mergedAt": "2023-12-21T00:27:24Z",
            "closedAt": "2023-12-21T00:27:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix `template` api doc description",
            "url": "https://github.com/ollama/ollama/pull/1661",
            "state": "MERGED",
            "createdAt": "2023-12-21T18:12:50Z",
            "mergedAt": "2024-01-03T16:00:59Z",
            "closedAt": "2024-01-03T16:00:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "The API docs specify that `template` overrides the prompt which isn't the case (verified back to v0.1.13), this is the functionality that `raw` mode enables. This change fixes the description.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Community Integrations - Obsidian Local GPT plugin",
            "url": "https://github.com/ollama/ollama/pull/1662",
            "state": "CLOSED",
            "createdAt": "2023-12-21T19:13:07Z",
            "mergedAt": null,
            "closedAt": "2024-01-22T17:04:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "Local GPT plugin for Obsidian mainly relies on Ollama provider\r\n![image](https://github.com/pfrankov/obsidian-local-gpt/assets/584632/724d4399-cb6c-4531-9f04-a1e5df2e3dad)\r\n![image](https://github.com/jmorganca/ollama/assets/584632/199b11c2-dc2a-4168-8466-247af40b572c)\r\nBut it's also possible to use OpenAI-like local server.\r\n\r\nI'd say that Local GPT plugin is enhanced version of [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama) in every way.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Quiet down llama.cpp logging by default",
            "url": "https://github.com/ollama/ollama/pull/1675",
            "state": "MERGED",
            "createdAt": "2023-12-22T16:48:08Z",
            "mergedAt": "2023-12-22T16:57:18Z",
            "closedAt": "2023-12-22T16:57:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 25,
            "deletions": 14,
            "body": "By default builds will now produce non-debug and non-verbose binaries. To enable verbose logs in llama.cpp and debug symbols in the native code, set `CGO_CFLAGS=-g`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update where are models stored q",
            "url": "https://github.com/ollama/ollama/pull/1677",
            "state": "MERGED",
            "createdAt": "2023-12-22T17:49:12Z",
            "mergedAt": "2023-12-22T17:56:28Z",
            "closedAt": "2023-12-22T17:56:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "build cuda and rocm",
            "url": "https://github.com/ollama/ollama/pull/1679",
            "state": "MERGED",
            "createdAt": "2023-12-22T20:22:28Z",
            "mergedAt": "2024-01-26T00:38:14Z",
            "closedAt": "2024-01-26T00:38:14Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 1
            },
            "additions": 64,
            "deletions": 16,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor how we augment llama.cpp and refine windows native build",
            "url": "https://github.com/ollama/ollama/pull/1680",
            "state": "MERGED",
            "createdAt": "2023-12-22T21:05:13Z",
            "mergedAt": "2024-01-03T16:10:17Z",
            "closedAt": "2024-01-03T16:10:17Z",
            "reviews": {
                "totalCount": 22
            },
            "files": {
                "totalCount": 22
            },
            "additions": 816,
            "deletions": 717,
            "body": "This changes the model for llama.cpp inclusion so we're not applying a patch, but instead have the C++ code directly in the ollama tree, which should make it easier to refine and update over time.\r\n\r\nThis also includes a change to refactor the dynamic loading logic to support variants that are purely dynamic, and leverages this on Windows.  In the windows build now, the base executable has only standard system dependencies, which means no special PATH setup is required.  That binary caries 2 payloads - one for CPU build and one for CUDA, and will load the appropriate one at runtime.  The dependencies for those are extracted into a temporary directory, and the PATH is updated automatically to ensure the deps are loaded.  We should be able to follow this same model to add ROCm support for windows as well in a follow up.\r\n\r\nAs a potential follow up, we could drop the sed of `main` and switch to a pure dynamic load strategy so the symbol isn't a conflict.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix windows system memory lookup",
            "url": "https://github.com/ollama/ollama/pull/1683",
            "state": "MERGED",
            "createdAt": "2023-12-23T00:05:02Z",
            "mergedAt": "2024-01-03T17:00:39Z",
            "closedAt": "2024-01-03T17:00:40Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 8
            },
            "additions": 68,
            "deletions": 20,
            "body": "This refines the gpu package error handling and fixes a bug with the system memory lookup on windows.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Guard integration tests with a tag",
            "url": "https://github.com/ollama/ollama/pull/1684",
            "state": "MERGED",
            "createdAt": "2023-12-23T00:33:48Z",
            "mergedAt": "2023-12-23T00:43:41Z",
            "closedAt": "2023-12-23T00:43:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 15,
            "deletions": 5,
            "body": "This should help CI avoid running the integration test logic in a container where it's not currently possible.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add windows native build instructions",
            "url": "https://github.com/ollama/ollama/pull/1697",
            "state": "MERGED",
            "createdAt": "2023-12-24T17:05:29Z",
            "mergedAt": "2024-01-06T03:34:21Z",
            "closedAt": "2024-01-06T03:34:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 39,
            "deletions": 5,
            "body": "Fixes #1694\r\n\r\nNote: the resulting native windows binary isn't particularly user friendly right now as it requires setting your PATH deep into the source tree to pick up the dependent DLLs.  I'm working on another change that will address this.  I'll keep this PR as a draft until that's ready.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor builder dockerfile",
            "url": "https://github.com/ollama/ollama/pull/1700",
            "state": "CLOSED",
            "createdAt": "2023-12-24T19:09:04Z",
            "mergedAt": null,
            "closedAt": "2024-01-03T00:57:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 47,
            "deletions": 32,
            "body": "Reorganize the x86/arm components to be more DRY, and remove the cuda driver\r\n\r\nNote: to build locally on arm mac, I need to remove the `--cache-from` and `--cache-to` flags in the script to be able to build without a builder defined.  It seems with a builder, qemu is being used instead of rosetta, and the rocm post-install packaging scripts have some binaries that wont run with qemu resulting in \r\n```\r\n...\r\n#10 864.1 Error while loading /var/lib/dpkg/info/rocrand.postinst: Exec format error\r\n#10 864.1 dpkg: error processing package rocrand (--configure):\r\n#10 864.1  installed rocrand package post-installation script subprocess returned error exit status 1\r\n```\r\n\r\nIf I omit creating a buildx builder, the default Docker Desktop build with rosetta works. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: Chatbox",
            "url": "https://github.com/ollama/ollama/pull/1706",
            "state": "MERGED",
            "createdAt": "2023-12-25T09:46:38Z",
            "mergedAt": "2024-02-23T12:17:28Z",
            "closedAt": "2024-02-23T12:17:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Thank you so much for developing Ollama; it has made running llama2 on my Mac incredibly simple. I've completely forgotten how I used to handle all the dependencies myself.\r\n\r\nRecently, I've added support for Ollama's locally deployed models to my project [Chatbox](https://github.com/Bin-Huang/chatbox) (in the [latest release](https://github.com/Bin-Huang/chatbox/releases)), and now Chatbox + Ollama is just fantastic.\ud83c\udf7b\r\n\r\n![Dec-25-2023 17-38-17](https://github.com/jmorganca/ollama/assets/20723142/75791fb3-5fc0-48ea-a7d9-3fbaf3ca0a3e)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "follow best practices by adding resp.Body.Close()",
            "url": "https://github.com/ollama/ollama/pull/1708",
            "state": "MERGED",
            "createdAt": "2023-12-25T13:31:10Z",
            "mergedAt": "2023-12-25T14:01:37Z",
            "closedAt": "2023-12-25T14:01:37Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added Ollama-SwiftUI to integrations",
            "url": "https://github.com/ollama/ollama/pull/1747",
            "state": "MERGED",
            "createdAt": "2023-12-30T18:42:25Z",
            "mergedAt": "2024-01-02T14:47:50Z",
            "closedAt": "2024-01-02T14:47:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update maid to use the right repo",
            "url": "https://github.com/ollama/ollama/pull/1757",
            "state": "MERGED",
            "createdAt": "2024-01-01T23:00:44Z",
            "mergedAt": "2024-01-02T14:47:08Z",
            "closedAt": "2024-01-02T14:47:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: relay request opts to loaded llm prediction",
            "url": "https://github.com/ollama/ollama/pull/1761",
            "state": "MERGED",
            "createdAt": "2024-01-02T15:35:08Z",
            "mergedAt": "2024-01-03T17:01:42Z",
            "closedAt": "2024-01-03T17:01:42Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 5
            },
            "additions": 103,
            "deletions": 68,
            "body": "- options from the loaded llm were being used regardless of the requested options\r\n\r\nOnly the options set from the request that initially loaded the model were being used as of the most recent llama.cpp update. Fix this by relaying the resolved options when options are checked during load time.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "keyboard shortcut help",
            "url": "https://github.com/ollama/ollama/pull/1764",
            "state": "MERGED",
            "createdAt": "2024-01-03T01:59:41Z",
            "mergedAt": "2024-01-03T02:04:13Z",
            "closedAt": "2024-01-03T02:04:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 5,
            "body": "This change adds some help in the REPL for using the keyboard shortcut commands.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/1766",
            "state": "MERGED",
            "createdAt": "2024-01-03T15:10:12Z",
            "mergedAt": "2024-01-03T15:44:22Z",
            "closedAt": "2024-01-03T15:44:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "fix quickstart spelling",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Terminal Integration - ShellOracle",
            "url": "https://github.com/ollama/ollama/pull/1767",
            "state": "MERGED",
            "createdAt": "2024-01-03T17:51:14Z",
            "mergedAt": "2024-02-20T03:18:05Z",
            "closedAt": "2024-02-20T03:18:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "ShellOracle is a new ZSH Line Editor widget that uses Ollama for intelligent shell command generation! Ollama rocks!\r\n\r\n![ShellOracle](https://i.imgur.com/QM2LkAf.gif)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "set `num_gpu` to 1 only by default on darwin arm64",
            "url": "https://github.com/ollama/ollama/pull/1771",
            "state": "MERGED",
            "createdAt": "2024-01-03T19:08:20Z",
            "mergedAt": "2024-01-03T19:10:29Z",
            "closedAt": "2024-01-03T19:10:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add embeddings to API",
            "url": "https://github.com/ollama/ollama/pull/1773",
            "state": "MERGED",
            "createdAt": "2024-01-03T19:52:52Z",
            "mergedAt": "2024-01-04T20:00:52Z",
            "closedAt": "2024-01-04T20:00:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "The API is missing the embeddings point, so this adds it. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: pull either original model or from model on create",
            "url": "https://github.com/ollama/ollama/pull/1774",
            "state": "MERGED",
            "createdAt": "2024-01-03T20:48:36Z",
            "mergedAt": "2024-01-04T06:34:38Z",
            "closedAt": "2024-01-04T06:34:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 2,
            "body": "I created a bug here when accounting for the \"pull parent model\" case when pull gguf models on deprecated ggml models. In the case of a Modelfile like this:\r\n```\r\nFROM orca-mini\r\nSYSTEM \"you are mario\"\r\n```\r\nwhere orca-mini is a `ggml` library model there is no `ParentModel` and the model itself should be pulled.\r\n\r\nThis handles both:\r\n```\r\nFROM orca-mini\r\nSYSTEM \"you are mario\"\r\n```\r\nand a child:\r\n```\r\nFROM mario\r\nPARAMETER temperature 0\r\n```\r\ncorrectly and will pull the root model for both cases.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: add Msty app in readme",
            "url": "https://github.com/ollama/ollama/pull/1775",
            "state": "MERGED",
            "createdAt": "2024-01-03T20:53:26Z",
            "mergedAt": "2024-02-20T19:03:33Z",
            "closedAt": "2024-02-20T19:03:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "\r\nhttps://github.com/jmorganca/ollama/assets/47485043/d402e724-5aa4-4d60-92b0-fecc30143c9f\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add ollama user to render group for Radeon support",
            "url": "https://github.com/ollama/ollama/pull/1776",
            "state": "MERGED",
            "createdAt": "2024-01-03T21:03:32Z",
            "mergedAt": "2024-01-03T21:07:54Z",
            "closedAt": "2024-01-03T21:07:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "For the ROCm libraries to access the driver, we need to add the ollama user to the render group.\r\n\r\nNote: the script will need more work to fully support radeon cards, but this will at least make the installation functional.  Without this change, the server sits in a crash-loop due to lack of permissions to access the driver.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fail fast on WSL1 while allowing on WSL2",
            "url": "https://github.com/ollama/ollama/pull/1778",
            "state": "MERGED",
            "createdAt": "2024-01-03T23:16:12Z",
            "mergedAt": "2024-01-04T00:18:41Z",
            "closedAt": "2024-01-04T00:18:41Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 0,
            "body": "This prevents users from accidentally installing on WSL1 with instructions guiding how to upgrade their WSL instance to version 2.  Once running WSL2 if you have an NVIDIA card, you can follow their instructions to set up GPU passthrough and run models on the GPU.  This is not possible on WSL1.\r\n\r\n\r\nExample output.\r\n\r\nWSL1\r\n```\r\ndaniel@DESKTOP-PUNI632:/mnt/c/Users/Daniel$ ./install.sh\r\nERROR WSL1 is not currently supported - please upgrade to WSL2 with 'wsl --set-version <distro> 2'\r\ndaniel@DESKTOP-PUNI632:/mnt/c/Users/Daniel$ uname -r\r\n4.4.0-19041-Microsoft\r\n```\r\n\r\nWSL2\r\n```\r\nroot@DESKTOP-PUNI632:/mnt/c/Users/Daniel# ./install.sh\r\n>>> Downloading ollama...\r\n################################################################################################################# 100.0% \r\n...\r\nroot@DESKTOP-PUNI632:/mnt/c/Users/Daniel# uname -r\r\n5.15.133.1-microsoft-standard-WSL2\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Improve maintainability of Radeon card list",
            "url": "https://github.com/ollama/ollama/pull/1779",
            "state": "MERGED",
            "createdAt": "2024-01-03T23:18:04Z",
            "mergedAt": "2024-01-04T00:18:57Z",
            "closedAt": "2024-01-04T00:18:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 1,
            "body": "This moves the list of AMD GPUs to an easier to maintain list which should make it easier to update over time.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update cmake flags for `amd64` macOS",
            "url": "https://github.com/ollama/ollama/pull/1780",
            "state": "MERGED",
            "createdAt": "2024-01-04T00:06:12Z",
            "mergedAt": "2024-01-04T00:22:15Z",
            "closedAt": "2024-01-04T00:22:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 7,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix CPU only builds",
            "url": "https://github.com/ollama/ollama/pull/1781",
            "state": "MERGED",
            "createdAt": "2024-01-04T00:09:15Z",
            "mergedAt": "2024-01-04T00:18:25Z",
            "closedAt": "2024-01-04T00:18:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 3,
            "body": "Go embed doesn't like when there's no matching files, so put a dummy placeholder in to allow building without any GPU support If no \"server\" library is found, it's safely ignored at runtime.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Load dynamic cpu lib on windows",
            "url": "https://github.com/ollama/ollama/pull/1785",
            "state": "MERGED",
            "createdAt": "2024-01-04T16:51:13Z",
            "mergedAt": "2024-01-04T16:55:02Z",
            "closedAt": "2024-01-04T16:55:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 8,
            "body": "On linux, we link the CPU library in to the Go app and fall back to it when no GPU match is found. On windows we do not link in the CPU library so that we can better control our dependencies for the CLI.  This fixes the logic so we correctly fallback to the dynamic CPU library on windows.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add faq about quant and context",
            "url": "https://github.com/ollama/ollama/pull/1786",
            "state": "CLOSED",
            "createdAt": "2024-01-04T17:46:13Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T03:17:13Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 0,
            "body": "This adds a short faq to describe quantization and context.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add faq about components of ollama. What is the serve command?",
            "url": "https://github.com/ollama/ollama/pull/1787",
            "state": "CLOSED",
            "createdAt": "2024-01-04T18:12:04Z",
            "mergedAt": null,
            "closedAt": "2024-02-20T04:16:09Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Revamp code layout for the llm directory and llama.cpp submodule",
            "url": "https://github.com/ollama/ollama/pull/1788",
            "state": "MERGED",
            "createdAt": "2024-01-04T20:18:44Z",
            "mergedAt": "2024-01-04T21:14:28Z",
            "closedAt": "2024-01-04T21:14:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 19
            },
            "additions": 57,
            "deletions": 51,
            "body": "Now that we only submodule llama.cpp once, we can tidy up the paths a bit.\r\n\r\nThis also moves the ext_server c++ code to a distinct directory to solve the go test glitch.\r\n\r\n```\r\n% go test ./...\r\n# github.com/jmorganca/ollama/llm\r\ncgo-gcc-prolog:153:33: warning: unused variable '_cgo_a' [-Wunused-variable]\r\ncgo-gcc-prolog:165:33: warning: unused variable '_cgo_a' [-Wunused-variable]\r\n?       github.com/jmorganca/ollama     [no test files]\r\n?       github.com/jmorganca/ollama/cmd [no test files]\r\n?       github.com/jmorganca/ollama/examples/golang-simplegenerate      [no test files]\r\n?       github.com/jmorganca/ollama/llm [no test files]\r\n?       github.com/jmorganca/ollama/llm/generate        [no test files]\r\n?       github.com/jmorganca/ollama/parser      [no test files]\r\n?       github.com/jmorganca/ollama/progress    [no test files]\r\n?       github.com/jmorganca/ollama/readline    [no test files]\r\nok      github.com/jmorganca/ollama/api 0.317s\r\nok      github.com/jmorganca/ollama/format      0.212s\r\n?       github.com/jmorganca/ollama/version     [no test files]\r\nok      github.com/jmorganca/ollama/gpu 0.211s\r\nok      github.com/jmorganca/ollama/server      0.203s\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Cleaup stale submodule",
            "url": "https://github.com/ollama/ollama/pull/1790",
            "state": "MERGED",
            "createdAt": "2024-01-04T21:44:08Z",
            "mergedAt": "2024-01-04T21:47:25Z",
            "closedAt": "2024-01-04T21:47:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "If the tree has a stale submodule, make sure we clean it up first",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update Dockerfile.build",
            "url": "https://github.com/ollama/ollama/pull/1791",
            "state": "MERGED",
            "createdAt": "2024-01-04T21:55:46Z",
            "mergedAt": "2024-01-05T03:13:44Z",
            "closedAt": "2024-01-05T03:13:44Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 94,
            "deletions": 67,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor interactive mode and fix the show command",
            "url": "https://github.com/ollama/ollama/pull/1793",
            "state": "MERGED",
            "createdAt": "2024-01-05T01:31:19Z",
            "mergedAt": "2024-01-05T20:20:05Z",
            "closedAt": "2024-01-05T20:20:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 572,
            "deletions": 515,
            "body": "This change splits up the interactive generation part of the CLI into its own file, and also fixes some issues with the way the `show` command works.\r\n\r\nThere is a new `/show info` command in the REPL which can show details about the model, and also `/show modelfile` will combine any parameters set in the REPL so that it creates the correct modelfile.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: allow extension origins (still needs explicit listing), fixes #1686",
            "url": "https://github.com/ollama/ollama/pull/1797",
            "state": "MERGED",
            "createdAt": "2024-01-05T02:03:54Z",
            "mergedAt": "2024-01-06T01:20:09Z",
            "closedAt": "2024-01-06T01:20:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 11
            }
        }
    },
    {
        "node": {
            "title": "fix docker doc to point to hub",
            "url": "https://github.com/ollama/ollama/pull/1801",
            "state": "MERGED",
            "createdAt": "2024-01-05T02:43:54Z",
            "mergedAt": "2024-01-05T03:20:45Z",
            "closedAt": "2024-01-05T03:20:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "there is a link that should point to dockerhub which is the best place to understand docker",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "gpu: read memory info from all cuda devices",
            "url": "https://github.com/ollama/ollama/pull/1802",
            "state": "MERGED",
            "createdAt": "2024-01-05T05:14:35Z",
            "mergedAt": "2024-01-05T16:25:58Z",
            "closedAt": "2024-01-05T16:25:58Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 29,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove unused generate patches",
            "url": "https://github.com/ollama/ollama/pull/1810",
            "state": "MERGED",
            "createdAt": "2024-01-05T16:02:58Z",
            "mergedAt": "2024-01-05T16:25:45Z",
            "closedAt": "2024-01-05T16:25:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 2,
            "body": "This patch file no longer exists.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: simplify ggml update logic",
            "url": "https://github.com/ollama/ollama/pull/1814",
            "state": "MERGED",
            "createdAt": "2024-01-05T19:14:26Z",
            "mergedAt": "2024-01-05T20:22:32Z",
            "closedAt": "2024-01-05T20:22:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 24,
            "body": "- allow cancelling gguf model update pull\r\n- additional information is now available in show response, use this to pull gguf before running",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add unit tests for Parser",
            "url": "https://github.com/ollama/ollama/pull/1815",
            "state": "MERGED",
            "createdAt": "2024-01-05T21:10:42Z",
            "mergedAt": "2024-01-05T22:04:32Z",
            "closedAt": "2024-01-05T22:04:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 63,
            "deletions": 0,
            "body": "This adds a couple of basic unit tests for parsing.\r\n\r\nSuggested in #1809 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "switch api for ShowRequest to use the name field",
            "url": "https://github.com/ollama/ollama/pull/1816",
            "state": "MERGED",
            "createdAt": "2024-01-05T23:02:10Z",
            "mergedAt": "2024-01-05T23:06:43Z",
            "closedAt": "2024-01-05T23:06:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "only pull gguf model if already exists",
            "url": "https://github.com/ollama/ollama/pull/1817",
            "state": "MERGED",
            "createdAt": "2024-01-05T23:17:13Z",
            "mergedAt": "2024-01-05T23:50:00Z",
            "closedAt": "2024-01-05T23:50:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 12,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(cmd): history in alt prompt",
            "url": "https://github.com/ollama/ollama/pull/1818",
            "state": "MERGED",
            "createdAt": "2024-01-05T23:58:04Z",
            "mergedAt": "2024-01-08T21:48:35Z",
            "closedAt": "2024-01-08T21:48:35Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 28,
            "deletions": 26,
            "body": "using up/down arrows (for history) messes up multiline string inputs by replacing the alt prefix `...` with the default prefix `>>>`",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Support multiple LLM libs; ROCm v5 and v6; Rosetta, AVX, and AVX2 compatible CPU builds",
            "url": "https://github.com/ollama/ollama/pull/1819",
            "state": "MERGED",
            "createdAt": "2024-01-06T03:52:13Z",
            "mergedAt": "2024-01-11T22:00:48Z",
            "closedAt": "2024-01-11T22:00:48Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 33
            },
            "additions": 821,
            "deletions": 625,
            "body": "In some cases we may want multiple variants for a given GPU type or CPU. This adds logic to have an optional Variant which we can use to select an optimal library, but also allows us to try multiple variants in case some fail to load.\r\n\r\nThis change includes updates to the Dockerfile.build to compile 2 variants for ROCm so we can support v5 and v6.\r\n\r\nI've also added multiple CPU variants and runtime detection logic so we can support both lowest-common-denominator for really old CPUs (and rosetta emulation on macos) as well as more modern CPUs.  At present, llama.cpp does not verify CPU features, so loading the wrong cpu variant will panic the whole process with illegal instruction.  Ollama should autodetect the optimal llm library variant for the given system, but I've also added a fail-safe mechanism for users to be able to force a specific library to workaround problems should they arise.\r\n\r\nThis also converges the LLM library model to use dynamic loading for all scenarios instead of having a built-in static link for macos and linux.  Windows was always fully dynamic, and now linux and macos follow the same pattern, so I was able to clean up the implementation and reduce some unnecessary complexity.\r\n\r\nFixes #1868\r\nFixes #1821 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "feat: add support for min_p (resolve #1142)",
            "url": "https://github.com/ollama/ollama/pull/1825",
            "state": "MERGED",
            "createdAt": "2024-01-06T15:28:07Z",
            "mergedAt": "2024-07-27T21:37:40Z",
            "closedAt": "2024-07-27T21:37:40Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 6
            },
            "additions": 6,
            "deletions": 0,
            "body": "Heavy lifting was done by https://github.com/ggerganov/llama.cpp/pull/3841 this PR just makes the option accessible.",
            "participants": {
                "totalCount": 14
            },
            "comments": {
                "totalCount": 35
            }
        }
    },
    {
        "node": {
            "title": "Accept windows paths for image processing",
            "url": "https://github.com/ollama/ollama/pull/1828",
            "state": "MERGED",
            "createdAt": "2024-01-06T18:52:31Z",
            "mergedAt": "2024-01-07T17:05:46Z",
            "closedAt": "2024-01-07T17:05:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 54,
            "deletions": 2,
            "body": "This enhances our regex to support windows style paths.  The regex will match invalid path specifications, but we'll still validate file existence and filter out mismatches\r\n\r\nFixes #1794",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Community Integrations - Ollama for Ruby",
            "url": "https://github.com/ollama/ollama/pull/1830",
            "state": "MERGED",
            "createdAt": "2024-01-06T21:26:22Z",
            "mergedAt": "2024-01-07T03:31:39Z",
            "closedAt": "2024-01-07T03:31:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hi!\r\n\r\nAdding a new library for the Ruby language:\r\n- [Ollama for Ruby](https://github.com/gbaptista/ollama-ai)\r\n  - [Ruby Gem](https://rubygems.org/gems/ollama-ai)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add `-DCMAKE_SYSTEM_NAME=Darwin` cmake flag",
            "url": "https://github.com/ollama/ollama/pull/1832",
            "state": "MERGED",
            "createdAt": "2024-01-07T05:33:14Z",
            "mergedAt": "2024-01-07T05:46:18Z",
            "closedAt": "2024-01-07T05:46:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 1,
            "body": "`-DCMAKE_SYSTEM_NAME=Darwin` is required for the `-DCMAKE_SYSTEM_PROCESSOR=x86_64 -DCMAKE_OSX_ARCHITECTURES=x86_64` flags to take effect\r\n\r\nThis also adds system info logging in verbose mode\r\n\r\nFixes #1827",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Dont use `-Wall` in static build",
            "url": "https://github.com/ollama/ollama/pull/1833",
            "state": "MERGED",
            "createdAt": "2024-01-07T05:47:53Z",
            "mergedAt": "2024-01-07T15:39:19Z",
            "closedAt": "2024-01-07T15:39:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Fixes this warning:\r\n\r\n```\r\n% go build .\r\n# github.com/jmorganca/ollama/llm\r\ncgo-gcc-prolog:153:33: warning: unused variable '_cgo_a' [-Wunused-variable]\r\ncgo-gcc-prolog:165:33: warning: unused variable '_cgo_a' [-Wunused-variable]\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect very old CUDA GPUs and fall back to CPU",
            "url": "https://github.com/ollama/ollama/pull/1834",
            "state": "MERGED",
            "createdAt": "2024-01-07T05:56:34Z",
            "mergedAt": "2024-01-07T18:39:49Z",
            "closedAt": "2024-01-07T18:39:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 74,
            "deletions": 2,
            "body": "If we try to load the CUDA library on an old GPU, it panics and crashes the server.  This checks the compute capability before we load the library so we can gracefully fall back to CPU mode.\r\n\r\nPrior to version 0.1.18, the fallback behavior resulted from the subprocess runner crashing.  Example from an old GPU:\r\n```\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce GTX 765M, compute capability 3.0\r\n\r\ncuBLAS error 3 at /go/src/github.com/jmorganca/ollama/llm/llama.cpp/gguf/ggml-cuda.cu:5552\r\ncurrent device: 0\r\n2024/01/06 21:48:54 llama.go:320: llama runner exited with error: exit status 1\r\n```\r\nIn 0.1.18 without this fix, the server crashes with a panic due to the assert in llama.cpp.\r\n\r\nWith this fix on the same system we now detect and fallback to CPU mode:\r\n```\r\n2024/01/06 21:52:17 shim_ext_server.go:142: Dynamic LLM variants [cuda rocm]\r\n2024/01/06 21:52:17 gpu.go:37: Detecting GPU type\r\n2024/01/06 21:52:17 gpu.go:56: Nvidia GPU detected\r\n2024/01/06 21:52:17 gpu.go:89: CUDA GPU is too old. Falling back to CPU mode. Compute Capability detected: 3.0\r\n2024/01/06 21:52:17 routes.go:953: no GPU detected\r\n...\r\n```\r\n\r\nExample output on a newer supported GPU which correctly runs on the GPU:\r\n```\r\n2024/01/06 21:55:11 shim_ext_server.go:142: Dynamic LLM variants [cuda rocm]\r\n2024/01/06 21:55:11 gpu.go:37: Detecting GPU type\r\n2024/01/06 21:55:11 gpu.go:56: Nvidia GPU detected\r\n2024/01/06 21:55:11 gpu.go:86: CUDA Compute Capability detected: 7.5\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Community Integrations - vscode, Sublime Text, CLI\u2026",
            "url": "https://github.com/ollama/ollama/pull/1841",
            "state": "CLOSED",
            "createdAt": "2024-01-07T13:39:42Z",
            "mergedAt": null,
            "closedAt": "2024-09-05T20:24:10Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": ":wave: I have added new integrations for CLI, Ruby, Visual Studio Code, Sublime Text, and Obsidian.\r\n\r\n*VSCode Demonstration:\r\n\r\nhttps://github.com/jmorganca/ollama/assets/113217272/e6ba9c62-56d5-401f-8b63-51407d9154bd\r\n\r\n*CLI Demonstration:\r\n\r\nhttps://github.com/jmorganca/ollama/assets/113217272/5612653b-c279-4fe7-910f-f734e26f4489\r\n\r\n> _* The videos were edited: Typing speed accelerated by 1.5x, the delay before streaming was cut out, and the answers were accelerated by 4x._\r\n\r\n- [Nano Bots CLI](https://github.com/icebaker/ruby-nano-bots)\r\n- [Nano Bots for Ruby](https://github.com/icebaker/ruby-nano-bots)\r\n- [Visual Studio Code](https://github.com/icebaker/vscode-nano-bots)\r\n- [Sublime Text](https://github.com/icebaker/sublime-nano-bots)\r\n- [Obsidian](https://github.com/icebaker/obsidian-nano-bots)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Accomodate split cuda lib dir",
            "url": "https://github.com/ollama/ollama/pull/1849",
            "state": "MERGED",
            "createdAt": "2024-01-08T00:24:45Z",
            "mergedAt": "2024-02-06T00:01:17Z",
            "closedAt": "2024-02-06T00:01:17Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "Makes it a little easier to compile when cuda lib dir is split up as in nixos.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "Offload layers to GPU based on new model size estimates",
            "url": "https://github.com/ollama/ollama/pull/1850",
            "state": "MERGED",
            "createdAt": "2024-01-08T04:52:49Z",
            "mergedAt": "2024-01-08T21:42:00Z",
            "closedAt": "2024-01-08T21:42:00Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 10
            },
            "additions": 161,
            "deletions": 154,
            "body": "This PR fixes a large number of crashes and \"out of memory\" errors related to VRAM allocation, by using a more accurate estimation of how much memory is required to run a model with a given context size.\r\n\r\nModels such as `mixtral` will now run on lower end hardware that would previously before, even if defaulting to the CPU is required. Also, more layers are loaded to Nvidia GPUs which should result in a speedup on Linux.\r\n\r\nDetails:\r\n- VRAM estimation now accounts for the kv cache and tensor graph (which can grow to GiBs for large context sizes)\r\n- On macOS, Ollama will now run in CPU mode, even on Apple Silicon (`arm64`) if the GPU doesn't have enough VRAM. Models such as `mixtral`, `llama2:70b`, etc will now work (perhaps slowly) instead of crashing\r\n- On Linux, the number of layers to be offloaded to the GPU now accounts for the kv cache which is also partially offloaded\r\n\r\nTodo in a follow up:\r\n- Handle smaller batch sizes as mention in #1812\r\n- Still seeing some errors with very large context sizes (64k, 128k)\r\n- Limit `num_ctx` to what the model is trained on\r\n\r\nFixes #1838\r\nFixes #1812\r\nFixes #1516 \r\nFixes #1674\r\nFixes #1374\r\nFixes #1534\r\nFixes #1303\r\nFixes #1413\r\nFixes #1636\r\nFixes #1837\r\nFixes #1627\r\nFixes #1566\r\nFixes #1576\r\nFixes #1703 \r\n\r\n\r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "remove ggml automatic re-pull",
            "url": "https://github.com/ollama/ollama/pull/1856",
            "state": "MERGED",
            "createdAt": "2024-01-08T17:09:55Z",
            "mergedAt": "2024-01-08T19:41:01Z",
            "closedAt": "2024-01-08T19:41:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 1,
            "deletions": 41,
            "body": "Remove ggml automatic re-pull now that ggml removal has been rolled out. This prevents a possible future bug where non-ggml models always get pulled on run.\r\n\r\nWhen an unsupported model format is run the error message is displayed to the user:\r\n```\r\n$ ollama run orca-mini\r\nError: unsupported model format: this model may be incompatible with your version of Ollama. If you previously pulled this model, try updating it by running `ollama pull orca-mini:latest`\r\n\r\n$ ollama create mario -f ~/models/mario/Modelfile\r\ntransferring model data\r\nreading model metadata\r\nError: orca-mini is not in gguf format, this base model is not compatible with this version of ollama\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add cliobot to ollama supported list",
            "url": "https://github.com/ollama/ollama/pull/1873",
            "state": "MERGED",
            "createdAt": "2024-01-09T19:17:55Z",
            "mergedAt": "2024-03-25T19:07:19Z",
            "closedAt": "2024-03-25T19:07:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "\ud83e\udd17",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Set corret CUDA minimum compute capability version",
            "url": "https://github.com/ollama/ollama/pull/1874",
            "state": "MERGED",
            "createdAt": "2024-01-09T19:29:52Z",
            "mergedAt": "2024-01-09T19:37:22Z",
            "closedAt": "2024-01-09T19:37:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "If you attempt to run the current CUDA build on compute capability 5.2 cards, you'll hit the following failure:\r\ncuBLAS error 15 at ggml-cuda.cu:7956: the requested functionality is not supported",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Calculate overhead based number of gpu devices",
            "url": "https://github.com/ollama/ollama/pull/1875",
            "state": "MERGED",
            "createdAt": "2024-01-09T20:24:26Z",
            "mergedAt": "2024-01-09T20:53:33Z",
            "closedAt": "2024-01-09T20:53:33Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 8
            },
            "additions": 13,
            "deletions": 6,
            "body": "The CUDA memory allocated for overhead is placed on a single GPU ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Typo Correction in API Documentation",
            "url": "https://github.com/ollama/ollama/pull/1878",
            "state": "MERGED",
            "createdAt": "2024-01-09T21:05:47Z",
            "mergedAt": "2024-01-09T21:21:18Z",
            "closedAt": "2024-01-09T21:21:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "I've noticed a small typo in the API documentation and have submitted a fix for it. The \"role\" value was written as \"assisant\" which I've updated to \"assistant\". ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update submodule to `6efb8eb30e7025b168f3fda3ff83b9b386428ad6`",
            "url": "https://github.com/ollama/ollama/pull/1885",
            "state": "MERGED",
            "createdAt": "2024-01-10T06:19:01Z",
            "mergedAt": "2024-01-10T21:48:38Z",
            "closedAt": "2024-01-10T21:48:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: load ~/.ollama/.env using godotenv",
            "url": "https://github.com/ollama/ollama/pull/1886",
            "state": "CLOSED",
            "createdAt": "2024-01-10T06:46:40Z",
            "mergedAt": null,
            "closedAt": "2024-01-22T21:52:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 43,
            "deletions": 0,
            "body": "- More generic than https://github.com/jmorganca/ollama/pull/1846\r\n- Slots in simply with the existing environment variable configuration\r\n  - Can be used to set environment variables on MacOS for e.g. OLLAMA_ORIGINS without needing to fiddle around with plist/SIP\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Increase minimum CUDA memory allocation overhead and fix minimum overhead for multi-gpu",
            "url": "https://github.com/ollama/ollama/pull/1896",
            "state": "MERGED",
            "createdAt": "2024-01-10T13:55:02Z",
            "mergedAt": "2024-01-11T00:08:51Z",
            "closedAt": "2024-01-11T00:08:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 12,
            "body": "Fixes https://github.com/jmorganca/ollama/issues/1887",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add more search paths for cuda libs",
            "url": "https://github.com/ollama/ollama/pull/1899",
            "state": "CLOSED",
            "createdAt": "2024-01-10T14:36:10Z",
            "mergedAt": null,
            "closedAt": "2024-01-11T01:16:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 1,
            "body": "Fixes https://github.com/jmorganca/ollama/issues/1898, although more as a stop gap and we may want to search through paths like the author's [gist](https://gist.github.com/Zenopheus/ba4632ec6dcbd6737b6f9b180d897d1d)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Smarter GPU Management library detection",
            "url": "https://github.com/ollama/ollama/pull/1914",
            "state": "MERGED",
            "createdAt": "2024-01-10T23:07:34Z",
            "mergedAt": "2024-01-10T23:21:57Z",
            "closedAt": "2024-01-10T23:21:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 169,
            "deletions": 65,
            "body": "When there are multiple management libraries installed on a system\r\nnot every one will be compatible with the current driver.  This change\r\nimproves our management library algorithm to build up a set of discovered\r\nlibraries based on glob patterns, and then try all of them until we're able to\r\nload one without error.\r\n\r\nFixes #1903 \r\nFixes #1898 \r\nFixes #1888 \r\nFixes #1879\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b1842 and add new cuda lib dep",
            "url": "https://github.com/ollama/ollama/pull/1915",
            "state": "MERGED",
            "createdAt": "2024-01-11T00:48:36Z",
            "mergedAt": "2024-01-16T21:36:49Z",
            "closedAt": "2024-01-16T21:36:49Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 1,
            "body": "Upstream llama.cpp has added a new dependency with the NVIDIA CUDA Driver Libraries (libcuda.so) which is part of the driver distribution, not the general cuda libraries, and is not available as an archive, so we can not statically link it.  This may introduce some additional compatibility challenges which we'll need to keep an eye on.\r\n\r\nMarking draft until we can test on more driver/cuda version combinations to ensure this doesn't cause compatibility problems.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "download: add inactivity monitor",
            "url": "https://github.com/ollama/ollama/pull/1916",
            "state": "MERGED",
            "createdAt": "2024-01-11T00:55:08Z",
            "mergedAt": "2024-01-26T18:56:01Z",
            "closedAt": "2024-01-26T18:56:01Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 74,
            "deletions": 43,
            "body": "If a download part is inactive for some time, restart it. From profiling, it's possible for one or more of the download parts to stall and receive no content from the storage backend for many consecutive seconds. \r\n\r\nThis generally causes the download to slow to a rate of near zero at the end as other, faster parts complete their download. \r\n\r\nThis change adds a monitor for each part. If the part doesn't receive data (0 bytes) for a given window (5 seconds), the monitor will trigger a stall error and the request is interrupted and retried. This retry does _not_ increment the retry counter.\r\n\r\nRelated to #1736 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix gpu_test.go Error (same type) uint64->uint32",
            "url": "https://github.com/ollama/ollama/pull/1921",
            "state": "MERGED",
            "createdAt": "2024-01-11T08:41:45Z",
            "mergedAt": "2024-01-11T13:22:23Z",
            "closedAt": "2024-01-11T13:22:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "When running the test suite on linux with a cuda build I get the following error without this commit:\r\n\r\n```log\r\n--- FAIL: TestBasicGetGPUInfo (0.06s)\r\n    gpu_test.go:21: \r\n                Error Trace:    /build/ollama-cuda/src/ollama/gpu/gpu_test.go:21\r\n                Error:          Elements should be the same type\r\n                Test:           TestBasicGetGPUInfo\r\nFAIL\r\nFAIL    github.com/jmorganca/ollama/gpu 0.078s\r\n```\r\n\r\nThis was due to a type mismatch between `GetGPUInfo()` and the corresponding `TestBasicGetGPUInfo()` test. This simple commit fixes it on the test side and now I get the following test output:\r\n\r\n```log\r\nok      github.com/jmorganca/ollama/gpu 0.090s\r\n```\r\n\r\n(my first line of go btw.)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add group delete to uninstall instructions",
            "url": "https://github.com/ollama/ollama/pull/1924",
            "state": "MERGED",
            "createdAt": "2024-01-11T10:24:57Z",
            "mergedAt": "2024-01-12T05:07:00Z",
            "closedAt": "2024-01-12T05:07:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "After executing the `userdel ollama` command, I saw this message:\r\n\r\n```sh\r\n$ sudo userdel ollama\r\nuserdel: group ollama not removed because it has other members.\r\n```\r\n\r\nWhich reminded me that I had to remove the dangling group too. For completeness, the uninstall instructions should do this too.\r\n\r\nThanks!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add semantic kernel to Readme",
            "url": "https://github.com/ollama/ollama/pull/1931",
            "state": "MERGED",
            "createdAt": "2024-01-11T19:37:14Z",
            "mergedAt": "2024-01-11T19:40:24Z",
            "closedAt": "2024-01-11T19:40:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "We just released support for Ollama in the Python version of Semantic Kernel, this links directly there. Would love to move this to a package approach instead of using a http request, but that can be done once your work on that is completed as mentioned here #1857.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "api: add model for all requests",
            "url": "https://github.com/ollama/ollama/pull/1932",
            "state": "MERGED",
            "createdAt": "2024-01-11T22:16:22Z",
            "mergedAt": "2024-01-18T22:56:51Z",
            "closedAt": "2024-01-18T22:56:51Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 61,
            "deletions": 26,
            "body": "Prefer using `req.Model` and fallback to `req.Name`. `req.Model` is already the field name for generate and chat which are by far the most popular endpoints. This change aligns the other requests.\r\n\r\nAlso update `CopyRequest.Destination` to `CopyRequest.Target` which better describe field",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "fix build and lint",
            "url": "https://github.com/ollama/ollama/pull/1934",
            "state": "MERGED",
            "createdAt": "2024-01-11T22:20:28Z",
            "mergedAt": "2024-01-11T22:36:21Z",
            "closedAt": "2024-01-11T22:36:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 2,
            "body": "x/exp/slices is compatible with 1.20 while slices is not\r\n\r\nalso fix llm/llm.go where fmt is used but not imported",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix up the CPU fallback selection",
            "url": "https://github.com/ollama/ollama/pull/1935",
            "state": "MERGED",
            "createdAt": "2024-01-11T22:53:46Z",
            "mergedAt": "2024-01-11T23:52:33Z",
            "closedAt": "2024-01-11T23:52:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 30,
            "deletions": 16,
            "body": "The memory changes and multi-variant change had some merge glitches I missed.  This fixes them so we actually get the cpu llm lib and best variant for the given system.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Convert the REPL to use /api/chat for interactive responses",
            "url": "https://github.com/ollama/ollama/pull/1936",
            "state": "MERGED",
            "createdAt": "2024-01-11T23:15:31Z",
            "mergedAt": "2024-01-12T20:05:52Z",
            "closedAt": "2024-01-12T20:05:52Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 155,
            "deletions": 72,
            "body": "This change switches the REPL to use `/api/chat` when running in interactive mode. It will still use `/api/generate` for non-interactive sessions. I've also attempted to DRY out the display response for calls to either end point to be able to properly do word wrapping.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove client.py",
            "url": "https://github.com/ollama/ollama/pull/1937",
            "state": "MERGED",
            "createdAt": "2024-01-11T23:52:08Z",
            "mergedAt": "2024-01-16T19:01:41Z",
            "closedAt": "2024-01-16T19:01:41Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 284,
            "body": "See [ollama-python](https://github.com/jmorganca/ollama-python)",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Add twinny vscode extension to Extensions and Plugins",
            "url": "https://github.com/ollama/ollama/pull/1950",
            "state": "MERGED",
            "createdAt": "2024-01-12T13:32:18Z",
            "mergedAt": "2024-01-31T14:25:06Z",
            "closedAt": "2024-01-31T14:25:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Hey, thanks for the awesome work you've been doing with Ollama.\r\n\r\nI was hoping that you would consider adding my extension for vscode to the list of extensions and plugins for Ollama.  \r\n\r\nIt's basically a Copilot alternative with FIM and Chat features.\r\n\r\nhttps://github.com/rjmacarthy/twinny\r\n\r\nMany thanks,\r\n\r\nRichard",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove double newlines in /set parameter",
            "url": "https://github.com/ollama/ollama/pull/1961",
            "state": "MERGED",
            "createdAt": "2024-01-12T19:57:46Z",
            "mergedAt": "2024-01-12T23:18:19Z",
            "closedAt": "2024-01-12T23:18:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 6,
            "body": "also change `fmt.Print(\"\\(.*\\)\\n\")` to `fmt.Println(\"\\1\")`\r\n\r\nBefore\r\n```\r\n>>> /set parameter temperature 0\r\nSet parameter 'temperature' to '0'\r\n\r\n>>> Send a message (/? for help)\r\n```\r\n\r\nNow\r\n```\r\n>>> /set parameter temperature 0\r\nSet parameter 'temperature' to '0'\r\n>>>\r\n```\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "trim chat prompt based on llm context size",
            "url": "https://github.com/ollama/ollama/pull/1963",
            "state": "MERGED",
            "createdAt": "2024-01-12T20:48:35Z",
            "mergedAt": "2024-01-30T20:59:29Z",
            "closedAt": "2024-01-30T20:59:29Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 4
            },
            "additions": 440,
            "deletions": 57,
            "body": "When trimming the input chat prompt we need to make sure we keep the prompt template in the expected format. Without this the prompt will be trimmed without accounting for the model template when the maximum context length is reached, which can result in unexpected behavior from the model.\r\n\r\n- update the `ChatPrompt` function to return a list of prompt variable, to allow the calling function to append them into the final prompt\r\n- create the final prompt based on the loaded LLM's context window size, while preserving the prompt template formatting and system message in the first message of the new context window",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "improve cuda detection (rel. issue #1704)",
            "url": "https://github.com/ollama/ollama/pull/1966",
            "state": "MERGED",
            "createdAt": "2024-01-12T21:12:54Z",
            "mergedAt": "2024-01-15T02:00:11Z",
            "closedAt": "2024-01-15T02:00:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 5,
            "body": "This pull request supersedes https://github.com/jmorganca/ollama/pull/1880",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: request retry with error",
            "url": "https://github.com/ollama/ollama/pull/1968",
            "state": "MERGED",
            "createdAt": "2024-01-12T21:34:32Z",
            "mergedAt": "2024-01-16T18:33:50Z",
            "closedAt": "2024-01-16T18:33:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 34,
            "body": "This fixes a subtle bug with makeRequestWithRetry where an HTTP status error on a retried request will potentially not return the right error.\r\n\r\nWhen a request is retried on Unauthorized, the second request does not go through the same error handling as the first request. For example, if the second request returns with status 404, it returns the request and a nil error while if the first request returns with the same status, it will return a nil request and os.ErrNotExist",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add max context length check",
            "url": "https://github.com/ollama/ollama/pull/1971",
            "state": "MERGED",
            "createdAt": "2024-01-12T22:55:03Z",
            "mergedAt": "2024-01-12T23:10:25Z",
            "closedAt": "2024-01-12T23:10:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 15,
            "deletions": 0,
            "body": "setting a context length greater than what the model is trained for has adverse effects. to prevent this, if the user requests a larger context length, log and set it to the model's max",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use g++ to build `libext_server.so` on linux",
            "url": "https://github.com/ollama/ollama/pull/1972",
            "state": "MERGED",
            "createdAt": "2024-01-13T08:12:36Z",
            "mergedAt": "2024-01-13T08:12:42Z",
            "closedAt": "2024-01-13T08:12:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes the build error:\r\n\r\n```\r\nError: Unable to load dynamic library: Unable to load dynamic server library: /tmp/ollama3730278603/cpu/libext_server.so: undefined symbol: _ZTVN10cxxabiv117class\r\n```\r\n\r\ncc @dhiltgen ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix intel mac build",
            "url": "https://github.com/ollama/ollama/pull/1982",
            "state": "MERGED",
            "createdAt": "2024-01-13T22:48:06Z",
            "mergedAt": "2024-01-14T16:26:47Z",
            "closedAt": "2024-01-14T16:26:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 1,
            "body": "Make sure we're building an x86 ext_server lib when cross-compiling\r\n\r\nPrior to this fix, running the cross-compiled binary on an intel mac produced the following error:\r\n```\r\n2024/01/13 14:38:47 llm.go:66: not enough vram available, falling back to CPU only\r\n2024/01/13 14:38:47 cpu_common.go:15: CPU has AVX\r\n2024/01/13 14:38:47 dyn_ext_server.go:384: Updating LD_LIBRARY_PATH to /var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal:\r\nloading /var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal/libext_server.so library\r\n2024/01/13 14:38:47 llm.go:151: Failed to load dynamic library /var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal/libext_server.so  Unable to load dynamic library: Unable to load dynamic server library: dlopen(/var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal/libext_server.so, 2): no suitable image found.  Did find:\r\n\t/var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal/libext_server.so: mach-o, but wrong architecture\r\n\t/var/folders/z8/jy4xc40953n1tfs96m6gnzkr0000gn/T/ollama2093980092/metal/libext_server.so: stat() failed with errno=4\r\n[GIN] 2024/01/13 - 14:38:47 | 500 |  416.860287ms |       127.0.0.1 | POST     \"/api/chat\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "use model defaults for `num_gqa`, `rope_frequency_base` and `rope_frequency_scale`",
            "url": "https://github.com/ollama/ollama/pull/1983",
            "state": "MERGED",
            "createdAt": "2024-01-13T23:18:45Z",
            "mergedAt": "2024-05-09T16:06:14Z",
            "closedAt": "2024-05-09T16:06:14Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 58,
            "deletions": 83,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Let gpu.go and gen_linux.sh also find CUDA on Arch Linux",
            "url": "https://github.com/ollama/ollama/pull/1987",
            "state": "MERGED",
            "createdAt": "2024-01-14T13:13:16Z",
            "mergedAt": "2024-01-18T21:32:10Z",
            "closedAt": "2024-01-18T21:32:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 8,
            "deletions": 2,
            "body": "* Let gpu.go and gen_linux.sh find CUDA on Arch Linux.\r\n* These changes were needed to let the [ollama-cuda](https://archlinux.org/packages/extra/x86_64/ollama-cuda/) package on Arch Linux find CUDA when building.\r\n* Also, use `find` instead of `ls` in `gen_linux.sh`.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add macos cross-compile CI coverage",
            "url": "https://github.com/ollama/ollama/pull/1990",
            "state": "MERGED",
            "createdAt": "2024-01-14T18:16:28Z",
            "mergedAt": "2024-01-16T20:31:37Z",
            "closedAt": "2024-01-16T20:31:37Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 28,
            "deletions": 3,
            "body": "Linux and Windows are not yet set up for cross-compilation like MacOS, so I've excluded those from the CI matrix.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix CPU-only build under Android Termux enviornment.",
            "url": "https://github.com/ollama/ollama/pull/1999",
            "state": "MERGED",
            "createdAt": "2024-01-15T10:13:07Z",
            "mergedAt": "2024-01-19T01:16:54Z",
            "closedAt": "2024-01-19T01:16:54Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Update gpu.go initGPUHandles() to declare gpuHandles variable before reading it. This resolves an \"invalid memory address or nil pointer dereference\" error.\r\n\r\nUpdate dyn_ext_server.c to avoid setting the RTLD_DEEPBIND flag under __TERMUX__ (Android).",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add multiple CPU variants for Intel Mac",
            "url": "https://github.com/ollama/ollama/pull/2007",
            "state": "MERGED",
            "createdAt": "2024-01-15T20:38:20Z",
            "mergedAt": "2024-01-18T19:32:29Z",
            "closedAt": "2024-01-18T19:32:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 18
            },
            "additions": 321,
            "deletions": 186,
            "body": "This also refines the build process for the ext_server build.\r\n\r\nI had initially aimed to get rid of the gcc/g++ library generation step and rely on cmake to build a shared library, but due to toolchain quirks, this model didn't work reliably. (e.g. linux worked since it's a consistent toolchain, and arm mac worked, but intel mac segfaults when calling the init function pointer). This may still be achievable in a follow up incremental PR, but for now I'll stick with g++ to create the main library we dlopen on all platforms except windows.\r\n\r\nAnother potential follow up is to consider splitting out the cuda shared libraries as a discrete download and handle it in the installer script if we don't detect cuda present on the host.  That would further reduce the footprint and resolve the slow initial startup due to decompressing large payloads.\r\n\r\n_Marking draft until I have a chance to more fully test, but so far happy path testing on mac (intel/arm), windows(cuda), and linux (rocm/cuda) looks good._\r\n\r\nExtracting the now compressed payloads takes some time - ~15s on my older laptop\r\n```\r\n2024/01/15 11:12:42 payload_common.go:106: Extracting dynamic libraries...\r\n2024/01/15 11:12:57 payload_common.go:145: Dynamic LLM libraries [rocm_v6 cpu cpu_avx2 cpu_avx cuda_v11 rocm_v5]\r\n```\r\n\r\nUncompressed sizes once on disk:\r\n```\r\n% du -sh /tmp/ollama3226276348/*\r\n36M\t/tmp/ollama3226276348/cpu\r\n36M\t/tmp/ollama3226276348/cpu_avx\r\n36M\t/tmp/ollama3226276348/cpu_avx2\r\n410M\t/tmp/ollama3226276348/cuda_v11\r\n30M\t/tmp/ollama3226276348/rocm_v5\r\n31M\t/tmp/ollama3226276348/rocm_v6\r\n```\r\n\r\nThe actual linux binary:\r\n```\r\n% ls -lh ollama-linux-amd64\r\n-rwxrwxr-x 1 daniel daniel 294M Jan 15 11:12 ollama-linux-amd64\r\n```\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add Open Interpreter to README",
            "url": "https://github.com/ollama/ollama/pull/2016",
            "state": "MERGED",
            "createdAt": "2024-01-16T15:30:38Z",
            "mergedAt": "2024-01-18T21:59:39Z",
            "closedAt": "2024-01-18T21:59:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Adding Open Interpreter to the list of Extensions & Plugins. Includes link to OI documentation explaining how to use Ollama to power OI",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix show parameters",
            "url": "https://github.com/ollama/ollama/pull/2017",
            "state": "MERGED",
            "createdAt": "2024-01-16T17:20:24Z",
            "mergedAt": "2024-01-16T18:34:44Z",
            "closedAt": "2024-01-16T18:34:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 41,
            "deletions": 20,
            "body": "The ShowParameters call was converting some floats into ints. This simplifies the code and adds a unit test.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: cache prompt causes kv cache to fill and not return after some time",
            "url": "https://github.com/ollama/ollama/pull/2018",
            "state": "MERGED",
            "createdAt": "2024-01-16T18:00:45Z",
            "mergedAt": "2024-01-16T18:48:05Z",
            "closedAt": "2024-01-16T18:48:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "- prompt cache causes inferance to hang after some time\r\n\r\nThis is a temporary fix to mitigate #1994 if I can't fix the root cause before the next release.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "install: pin fedora to max 37",
            "url": "https://github.com/ollama/ollama/pull/2020",
            "state": "MERGED",
            "createdAt": "2024-01-16T19:46:46Z",
            "mergedAt": "2024-01-18T22:23:42Z",
            "closedAt": "2024-01-18T22:23:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "repos for fedora 38 and newer do not exist as of this commit\r\n\r\n```\r\n$ dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/fedora37/x86_64/cuda-fedora37.repo\r\nAdding repo from: https://developer.download.nvidia.com/compute/cuda/repos/fedora37/x86_64/cuda-fedora37.repo\r\n```\r\n\r\n```\r\n$ dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/fedora38/x86_64/cuda-fedora38.repo\r\nAdding repo from: https://developer.download.nvidia.com/compute/cuda/repos/fedora38/x86_64/cuda-fedora38.repo\r\nStatus code: 404 for https://developer.download.nvidia.com/compute/cuda/repos/fedora38/x86_64/cuda-fedora38.repo (IP: 152.195.19.142)\r\nError: Configuration of repo failed\r\n```\r\n\r\nresolves #1993 \r\nresolves #1326 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Library - Haystack",
            "url": "https://github.com/ollama/ollama/pull/2021",
            "state": "MERGED",
            "createdAt": "2024-01-16T19:57:08Z",
            "mergedAt": "2024-01-18T21:38:32Z",
            "closedAt": "2024-01-18T21:38:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Updated readme with the web link for haystack ollama integration",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: normalize name path before splitting",
            "url": "https://github.com/ollama/ollama/pull/2026",
            "state": "MERGED",
            "createdAt": "2024-01-17T00:49:42Z",
            "mergedAt": "2024-01-17T00:58:42Z",
            "closedAt": "2024-01-17T00:58:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "During pruning input to ParseModelPath is a file path which on Windows will cause the split to not work as expected. It's still necessary to split on `/` because the most common case is a URL path which is platform agnostic",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs/update langchainjs.md",
            "url": "https://github.com/ollama/ollama/pull/2027",
            "state": "MERGED",
            "createdAt": "2024-01-17T07:06:47Z",
            "mergedAt": "2024-05-09T03:21:03Z",
            "closedAt": "2024-05-09T03:21:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Updated sample code as per warning notification from the package maintainers",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs/Update langchainjs.md",
            "url": "https://github.com/ollama/ollama/pull/2030",
            "state": "MERGED",
            "createdAt": "2024-01-17T10:31:27Z",
            "mergedAt": "2024-04-15T22:37:30Z",
            "closedAt": "2024-04-15T22:37:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Changed ollama.call() for ollama.invoke() as per deprecated documentation from langchain",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refine the linux cuda/rocm developer docs",
            "url": "https://github.com/ollama/ollama/pull/2055",
            "state": "MERGED",
            "createdAt": "2024-01-18T17:52:23Z",
            "mergedAt": "2024-01-18T20:07:31Z",
            "closedAt": "2024-01-18T20:07:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 3,
            "body": "With the recent improvements in the [gen_linux.sh](https://github.com/jmorganca/ollama/blob/main/llm/generate/gen_linux.sh) script and these doc updates, this should fix #1704 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Mechanical switch from log to slog",
            "url": "https://github.com/ollama/ollama/pull/2056",
            "state": "MERGED",
            "createdAt": "2024-01-18T19:45:25Z",
            "mergedAt": "2024-01-18T22:27:24Z",
            "closedAt": "2024-01-18T22:27:24Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 14
            },
            "additions": 96,
            "deletions": 87,
            "body": "A few obvious levels were adjusted, but generally everything mapped to \"info\" level.\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Improve scratch buffer estimates",
            "url": "https://github.com/ollama/ollama/pull/2057",
            "state": "CLOSED",
            "createdAt": "2024-01-18T21:21:21Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T05:57:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 8,
            "deletions": 14,
            "body": "This tweaks the scratch buffer estimates to account for batch size and allocates a larger amount of overhead. This is a temporary fix \u2013 long term we want to inspect the model weights for proper tensor-by-tensor estimates.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix show handler",
            "url": "https://github.com/ollama/ollama/pull/2060",
            "state": "MERGED",
            "createdAt": "2024-01-18T23:37:30Z",
            "mergedAt": "2024-01-19T00:02:28Z",
            "closedAt": "2024-01-19T00:02:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 4,
            "body": "the show handler is unique in the sense that the request struct is passed directly to `GetModel` while every other handler deconstructs the request into its parameters. therefore the pattern of setting a local variable `model` and setting it to req.Model or req.Name doesn't work",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Save and load sessions",
            "url": "https://github.com/ollama/ollama/pull/2063",
            "state": "MERGED",
            "createdAt": "2024-01-19T01:54:02Z",
            "mergedAt": "2024-01-25T20:12:36Z",
            "closedAt": "2024-01-25T20:12:36Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 8
            },
            "additions": 312,
            "deletions": 39,
            "body": "This change allows users to interactively save a session from the REPL, and then load it back up again later.\r\n\r\nIt also adds a new `MESSAGE` command for Modelfiles so that users can build their own session which can be created with `ollama create`.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Use `gzip` for embedded files",
            "url": "https://github.com/ollama/ollama/pull/2067",
            "state": "MERGED",
            "createdAt": "2024-01-19T04:55:36Z",
            "mergedAt": "2024-01-19T18:23:04Z",
            "closedAt": "2024-01-19T18:23:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 18,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Switch to local dlopen symbols",
            "url": "https://github.com/ollama/ollama/pull/2099",
            "state": "MERGED",
            "createdAt": "2024-01-19T20:00:11Z",
            "mergedAt": "2024-01-19T20:22:04Z",
            "closedAt": "2024-01-19T20:22:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes #2066 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "More WSL paths",
            "url": "https://github.com/ollama/ollama/pull/2100",
            "state": "MERGED",
            "createdAt": "2024-01-19T21:25:04Z",
            "mergedAt": "2024-01-19T21:41:08Z",
            "closedAt": "2024-01-19T21:41:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Fixes #1939 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Sign dynamic libraries on macOS",
            "url": "https://github.com/ollama/ollama/pull/2101",
            "state": "MERGED",
            "createdAt": "2024-01-19T22:15:03Z",
            "mergedAt": "2024-01-20T00:24:11Z",
            "closedAt": "2024-01-20T00:24:11Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 13,
            "deletions": 2,
            "body": "Also fixes `gzip` from erroring if `.gz` files already exist",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: remove overwritten model layers",
            "url": "https://github.com/ollama/ollama/pull/2102",
            "state": "MERGED",
            "createdAt": "2024-01-19T23:00:15Z",
            "mergedAt": "2024-01-22T17:37:49Z",
            "closedAt": "2024-01-22T17:37:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 2,
            "body": "if create overrides a manifest, first add the older manifest's layers to the delete map so they can be cleaned up\r\n\r\nresolves #2097 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Increase minimum CUDA overhead to 1024MiB",
            "url": "https://github.com/ollama/ollama/pull/2114",
            "state": "MERGED",
            "createdAt": "2024-01-20T21:20:18Z",
            "mergedAt": "2024-01-20T22:11:38Z",
            "closedAt": "2024-01-20T22:11:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update submodule to `6f9939d119b2d004c264952eb510bd106455531e`",
            "url": "https://github.com/ollama/ollama/pull/2115",
            "state": "MERGED",
            "createdAt": "2024-01-20T22:19:52Z",
            "mergedAt": "2024-01-22T19:56:40Z",
            "closedAt": "2024-01-22T19:56:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add support for CUDA 5.0 cards",
            "url": "https://github.com/ollama/ollama/pull/2116",
            "state": "MERGED",
            "createdAt": "2024-01-20T22:28:09Z",
            "mergedAt": "2024-01-27T18:28:38Z",
            "closedAt": "2024-01-27T18:28:38Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 15,
            "deletions": 6,
            "body": "Building on #2112, this expands back to 5.0 cards, and also adds a few newer targets which theoretically should help performance on the more modern cards.  The resulting binary grows a little in size but not significantly\r\n\r\n* 0.1.21 => 263M\r\n* #2112 => 264M\r\n* This PR: => 266M\r\n\r\nFixes #1865\r\n\r\nI'll keep this draft until we can run more performance testing on modern cards to ensure no significant regression",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Unlock mutex when failing to load model",
            "url": "https://github.com/ollama/ollama/pull/2117",
            "state": "MERGED",
            "createdAt": "2024-01-21T01:06:59Z",
            "mergedAt": "2024-01-21T01:54:46Z",
            "closedAt": "2024-01-21T01:54:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Avoids `ollama serve` hanging with `concurrent llm servers not yet supported, waiting for prior server to complete` when a model fails to load\r\n\r\nI believe this also fixes https://github.com/jmorganca/ollama/issues/1641",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Combine the 2 Dockerfiles and add ROCm",
            "url": "https://github.com/ollama/ollama/pull/2127",
            "state": "MERGED",
            "createdAt": "2024-01-21T19:39:53Z",
            "mergedAt": "2024-01-21T19:49:01Z",
            "closedAt": "2024-01-21T19:49:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 111,
            "deletions": 116,
            "body": "This renames Dockerfile.build to replace the old Dockerfile, and adds some new stages to support 2 modes of building - the build_linux.sh script uses intermediate stages to extract the artifacts for ./dist, and the default build generates a container image usable by both cuda and rocm cards. This required transitioning the x86 base to the rocm image to avoid layer bloat.\r\n\r\nWe should update our Hub landing page with instructions for ROCm.  The host needs the ROCm driver.\r\n\r\n```\r\ndocker run --privileged --device /dev/kfd ollama/ollama\r\n```\r\n(Both privileged and the device flag are necessary to access the rocm driver)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Make CPU builds parallel and customizable AMD GPUs",
            "url": "https://github.com/ollama/ollama/pull/2130",
            "state": "MERGED",
            "createdAt": "2024-01-21T22:44:27Z",
            "mergedAt": "2024-01-22T00:14:12Z",
            "closedAt": "2024-01-22T00:14:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 68,
            "deletions": 44,
            "body": "The linux build now support parallel CPU builds to speed things up. This also exposes AMD GPU targets as an optional setting for advaced users who want to alter our default set.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Probe GPUs before backend init",
            "url": "https://github.com/ollama/ollama/pull/2131",
            "state": "MERGED",
            "createdAt": "2024-01-21T23:59:49Z",
            "mergedAt": "2024-01-22T00:13:47Z",
            "closedAt": "2024-01-22T00:13:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 34,
            "deletions": 1,
            "body": "Detect potential error scenarios so we can fallback to CPU mode without hitting asserts.\r\n\r\nThis won't fix the underlying errors we're seeing in #1940 and #1877 but it should hopefully allow us to detect the non-working scenario and fallback to CPU.  We still need to understand why `cudaGetDevice` is failing on these systems (maybe incompatible cuda libs or drivers?)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "readline: drop not use min function",
            "url": "https://github.com/ollama/ollama/pull/2134",
            "state": "MERGED",
            "createdAt": "2024-01-22T09:29:41Z",
            "mergedAt": "2024-01-22T16:15:08Z",
            "closedAt": "2024-01-22T16:15:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 7,
            "body": "Since [Go1.21 (go.mod)](https://go.dev/doc/go1.21), Go adds min builtin function.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Community Integrations - Obsidian Local GPT plugin",
            "url": "https://github.com/ollama/ollama/pull/2138",
            "state": "MERGED",
            "createdAt": "2024-01-22T17:11:44Z",
            "mergedAt": "2024-02-22T15:52:36Z",
            "closedAt": "2024-02-22T15:52:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Local GPT plugin for Obsidian mainly relies on Ollama provider\r\n\r\n![image](https://github.com/pfrankov/obsidian-local-gpt/assets/584632/724d4399-cb6c-4531-9f04-a1e5df2e3dad)\r\n\r\nAlso works with images  \r\n<img width=\"400\" src=\"https://github.com/pfrankov/obsidian-local-gpt/assets/584632/a05d68fa-5419-4386-ac43-82b9513999ad\">  \r\n\r\n<img width=\"598\" alt=\"Settings\" src=\"https://github.com/pfrankov/obsidian-local-gpt/assets/584632/6ab2d802-13ed-42be-aab1-6a3f689b18a0\">\r\n\r\nI'd say that Local GPT plugin is enhanced version of [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama) in every way.\r\n\r\nTried to resolve merge conflicts in https://github.com/jmorganca/ollama/pull/1662 but accidentally closed it.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Debug logging on init failure",
            "url": "https://github.com/ollama/ollama/pull/2142",
            "state": "MERGED",
            "createdAt": "2024-01-22T20:09:52Z",
            "mergedAt": "2024-01-22T20:29:23Z",
            "closedAt": "2024-01-22T20:29:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "One class of error we're seeing on ROCm looks like this in the log...\r\n```\r\n2024/01/21 22:00:15 dyn_ext_server.go:90: INFO Loading Dynamic llm server: /tmp/ollama1546965028/rocm_v5/libext_server.so\r\n2024/01/21 22:00:15 dyn_ext_server.go:139: INFO Initializing llama server\r\nfree(): invalid pointer\r\n```\r\n\r\nI'm not sure yet what the root cause is, but hopefully this debug log will yield some more insight.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refine debug logging for llm",
            "url": "https://github.com/ollama/ollama/pull/2143",
            "state": "MERGED",
            "createdAt": "2024-01-22T20:28:38Z",
            "mergedAt": "2024-01-22T21:19:16Z",
            "closedAt": "2024-01-22T21:19:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 13,
            "deletions": 5,
            "body": "This wires up logging in llama.cpp to always go to stderr, and also turns up logging if OLLAMA_DEBUG is set.\r\n\r\nThis solves a couple problems.  We used to emit one line to llama.log in verbose/debug mode before shifting to stdout.  Now all the logging from llama.cpp will go to stderr, and the verbosity can be controlled at runtime.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "faq: update to use launchctl setenv",
            "url": "https://github.com/ollama/ollama/pull/2144",
            "state": "MERGED",
            "createdAt": "2024-01-22T20:31:42Z",
            "mergedAt": "2024-01-22T21:46:57Z",
            "closedAt": "2024-01-22T21:46:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 28,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add keep_alive to generate/chat/embedding api endpoints",
            "url": "https://github.com/ollama/ollama/pull/2146",
            "state": "MERGED",
            "createdAt": "2024-01-22T21:47:04Z",
            "mergedAt": "2024-01-26T22:28:02Z",
            "closedAt": "2024-01-26T22:28:02Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 2
            },
            "additions": 48,
            "deletions": 20,
            "body": "This change adds a new `keep_alive` parameter to `/api/generate` which can control the duration for how long a model is loaded and left in memory. There are three cases:\r\n\r\n1. if `keep_alive` is not set, the model will stay loaded for the default value (5 minutes);\r\n2. if `keep_alive` is set to a positive duration (e.g. \"20m\"), it will stay loaded for the duration;\r\n3. if `keep_alive` is set to a negative duration (e.g. \"-1m\"), it will stay loaded indefinitely\r\n\r\nIf you wish the model to be loaded immediately after generation, you can set it to \"0m\", or even just `0`. Also, maybe *most importantly*, subsequent calls to the `/api/generate` will change the load duration, so even if you called it once with a negative value and the next caller omits it, it will still only stay in memory for 5 minutes after the second call.\r\n\r\nNote that this change only applies to the `/api/generate`. We can either layer on the changes for `/api/chat` on top of this change, or push it as a separate PR.\r\n\r\nresolves #1339 ",
            "participants": {
                "totalCount": 13
            },
            "comments": {
                "totalCount": 14
            }
        }
    },
    {
        "node": {
            "title": "Refine Accelerate usage on mac",
            "url": "https://github.com/ollama/ollama/pull/2148",
            "state": "MERGED",
            "createdAt": "2024-01-23T00:26:46Z",
            "mergedAt": "2024-01-23T00:56:58Z",
            "closedAt": "2024-01-23T00:56:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 5,
            "body": "For old macs, accelerate seems to cause crashes, but for AVX2 capable macs, it does not.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use all layers for metal on macOS if model is small enough",
            "url": "https://github.com/ollama/ollama/pull/2149",
            "state": "MERGED",
            "createdAt": "2024-01-23T01:05:05Z",
            "mergedAt": "2024-01-23T01:40:07Z",
            "closedAt": "2024-01-23T01:40:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Set a default version using git describe",
            "url": "https://github.com/ollama/ollama/pull/2150",
            "state": "MERGED",
            "createdAt": "2024-01-23T01:12:49Z",
            "mergedAt": "2024-01-23T01:38:27Z",
            "closedAt": "2024-01-23T01:38:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 3,
            "deletions": 3,
            "body": "If a VERSION is not specified, this will generate a version string that represents the state of the repo.  For example `0.1.21-12-gffaf52e-dirty` representing 12 commits away from 0.1.21 tag, on commit gffaf52e and the tree is dirty.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Report more information about GPUs in verbose mode",
            "url": "https://github.com/ollama/ollama/pull/2162",
            "state": "MERGED",
            "createdAt": "2024-01-23T19:43:51Z",
            "mergedAt": "2024-01-24T01:45:40Z",
            "closedAt": "2024-01-24T01:45:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 171,
            "deletions": 30,
            "body": "This adds additional calls to both CUDA and ROCm management libraries to discover additional attributes about the GPU(s) detected in the system, and wires up runtime verbosity selection.  When users hit problems with GPUs we can ask them to run with `OLLAMA_DEBUG=1 ollama serve` and share the server log.\r\n\r\nExample output on a CUDA laptop:\r\n```\r\n% OLLAMA_DEBUG=1 ./ollama-linux-amd64 serve\r\n...\r\ntime=2024-01-23T11:31:22.828-08:00 level=INFO source=/go/src/github.com/jmorganca/ollama/gpu/gpu.go:256 msg=\"Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.545.23.08]\"\r\nCUDA driver version: 545.23.08\r\ntime=2024-01-23T11:31:22.859-08:00 level=INFO source=/go/src/github.com/jmorganca/ollama/gpu/gpu.go:96 msg=\"Nvidia GPU detected\"\r\n[0] CUDA device name: NVIDIA GeForce GTX 1650 with Max-Q Design\r\n[0] CUDA part number:\r\nnvmlDeviceGetSerial failed: 3\r\n[0] CUDA vbios version: 90.17.31.00.26\r\n[0] CUDA brand: 5\r\n[0] CUDA totalMem 4294967296\r\n[0] CUDA usedMem 3789357056\r\ntime=2024-01-23T11:31:22.865-08:00 level=INFO source=/go/src/github.com/jmorganca/ollama/gpu/gpu.go:137 msg=\"CUDA Compute Capability detected: 7.5\"\r\n```\r\n\r\n\r\nExample output on a ROCM GPU system\r\n```\r\n% OLLAMA_DEBUG=1 ./ollama-linux-amd64 serve\r\n...\r\ntime=2024-01-23T19:24:55.162Z level=INFO source=/go/src/github.com/jmorganca/ollama/gpu/gpu.go:256 msg=\"Discovered GPU libraries: [/opt/rocm/lib/librocm_smi64.so.6.0.60000 /opt/rocm-6.0.0/lib/librocm_smi64.so.6.0.60000]\"\r\ntime=2024-01-23T19:24:55.163Z level=INFO source=/go/src/github.com/jmorganca/ollama/gpu/gpu.go:106 msg=\"Radeon GPU detected\"\r\ndiscovered 1 ROCm GPU Devices\r\n[0] ROCm device name: Navi 31 [Radeon RX 7900 XT/7900 XTX]\r\n[0] ROCm GPU brand: Navi 31 [Radeon RX 7900 XT/7900 XTX]\r\n[0] ROCm GPU vendor: Advanced Micro Devices, Inc. [AMD/ATI]\r\n[0] ROCm GPU VRAM vendor: samsung\r\n[0] ROCm GPU S/N: 43cfeecf3446fbf7\r\n[0] ROCm GPU subsystem name: NITRO+ RX 7900 XTX Vapor-X\r\n[0] ROCm GPU vbios version: 113-4E4710U-T4Y\r\n[0] ROCm totalMem 25753026560\r\n[0] ROCm usedMem 27852800\r\n```\r\n\r\nThis also implements the TODO on ROCm to handle multiple GPUs reported by the management library.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "More logging for gpu management",
            "url": "https://github.com/ollama/ollama/pull/2174",
            "state": "MERGED",
            "createdAt": "2024-01-24T18:35:14Z",
            "mergedAt": "2024-01-24T19:09:17Z",
            "closedAt": "2024-01-24T19:09:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 61,
            "deletions": 44,
            "body": "Fix an ordering glitch of dlerr/dlclose and add more logging to help root cause some crashes users are hitting. This also refines the function pointer names to use the underlying function names instead of simplified names for readability.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "refactor tensor read",
            "url": "https://github.com/ollama/ollama/pull/2175",
            "state": "MERGED",
            "createdAt": "2024-01-24T19:10:03Z",
            "mergedAt": "2024-01-25T17:22:42Z",
            "closedAt": "2024-01-25T17:22:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 61,
            "deletions": 54,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "stub generate outputs for lint",
            "url": "https://github.com/ollama/ollama/pull/2181",
            "state": "MERGED",
            "createdAt": "2024-01-25T01:30:09Z",
            "mergedAt": "2024-01-25T19:55:16Z",
            "closedAt": "2024-01-25T19:55:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix clearing kv cache between requests with the same prompt",
            "url": "https://github.com/ollama/ollama/pull/2186",
            "state": "MERGED",
            "createdAt": "2024-01-25T10:03:41Z",
            "mergedAt": "2024-01-25T21:46:21Z",
            "closedAt": "2024-01-25T21:46:21Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 4
            },
            "additions": 65,
            "deletions": 0,
            "body": "This is a (draft) fix for #1573, as it seems that the kv cache isn't cleared properly when the exact same prompt is provided repetitively.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Community Integration: PromptingTools.jl",
            "url": "https://github.com/ollama/ollama/pull/2192",
            "state": "MERGED",
            "createdAt": "2024-01-25T19:21:52Z",
            "mergedAt": "2024-05-09T16:39:05Z",
            "closedAt": "2024-05-09T16:39:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "First of all, thank you for the awesome library!\r\n\r\nI was wondering if you would consider adding a link to PromptingTools.jl, a Julia library that has had an integration with Ollama since its early days.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Ignore AMD integrated GPUs",
            "url": "https://github.com/ollama/ollama/pull/2195",
            "state": "MERGED",
            "createdAt": "2024-01-26T00:02:11Z",
            "mergedAt": "2024-01-26T17:30:24Z",
            "closedAt": "2024-01-26T17:30:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 35,
            "deletions": 3,
            "body": "Fixes #2054 \r\n\r\nIntegrated GPUs (APUs) from AMD may be reported by ROCm, but we can't run on them with our current llama.cpp configuration.  These iGPUs report 512M of memory, so I've coded the check to ignore any ROCm reported GPU that has less than 1G of memory.  If we detect only an integrated GPU, this will fallback to CPU mode.  If we detect multiple ROCm GPUs, meaning one or more are discrete, and one is integrated, we'll now set `ROCR_VISIBLE_DEVICES` so we ignore the iGPU.  If the user has explicitly set `ROCR_VISIBLE_DEVICES` we'll respect their setting.",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Switch back to ubuntu base",
            "url": "https://github.com/ollama/ollama/pull/2196",
            "state": "MERGED",
            "createdAt": "2024-01-26T00:46:57Z",
            "mergedAt": "2024-01-26T00:50:32Z",
            "closedAt": "2024-01-26T00:50:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "The size increase for rocm support in the standard image is problematic We'll revisit multiple tags for rocm support in a follow up PR.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add back ROCm container support",
            "url": "https://github.com/ollama/ollama/pull/2197",
            "state": "MERGED",
            "createdAt": "2024-01-26T00:58:57Z",
            "mergedAt": "2024-01-26T17:34:23Z",
            "closedAt": "2024-01-26T17:34:24Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 21,
            "deletions": 1,
            "body": "This adds ROCm support back as a discrete image.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix crash on cuda ml init failure",
            "url": "https://github.com/ollama/ollama/pull/2209",
            "state": "MERGED",
            "createdAt": "2024-01-26T17:19:17Z",
            "mergedAt": "2024-01-26T17:30:13Z",
            "closedAt": "2024-01-26T17:30:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "The new driver lookup code was triggering after init failure due to a missing return\r\n\r\nFixes #2198 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix build",
            "url": "https://github.com/ollama/ollama/pull/2212",
            "state": "MERGED",
            "createdAt": "2024-01-26T19:04:39Z",
            "mergedAt": "2024-01-26T19:19:08Z",
            "closedAt": "2024-01-26T19:19:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect lack of AVX and fallback to CPU mode",
            "url": "https://github.com/ollama/ollama/pull/2214",
            "state": "MERGED",
            "createdAt": "2024-01-26T19:42:11Z",
            "mergedAt": "2024-01-26T20:06:44Z",
            "closedAt": "2024-01-26T20:06:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 3,
            "body": "We build the GPU libraries with AVX enabled to ensure that if not all layers fit on the GPU we get better performance in a mixed mode. If the user is using a virtualization/emulation system that lacks AVX this used to result in an illegal instruction error and crash before this fix.  Now we will report a warning in the server log, and just use CPU mode to ensure we don't crash.\r\n\r\n\r\nThis should mitigate #2187.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "adjust download and upload concurrency based on available bandwidth",
            "url": "https://github.com/ollama/ollama/pull/2221",
            "state": "MERGED",
            "createdAt": "2024-01-26T23:15:44Z",
            "mergedAt": "2024-03-07T17:27:33Z",
            "closedAt": "2024-03-07T17:27:33Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 106,
            "deletions": 17,
            "body": "use basic heuristics to determine concurrency.\r\n\r\n1. start with 2 concurrency\r\n2. watch the rate\r\n3. if the rate is increasing, add more concurrency\r\n4. stop adding concurrency if the rate plateaus\r\n\r\nthis only scales concurrency up, never down",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ROCm: Correct the response string in rocm_get_version function",
            "url": "https://github.com/ollama/ollama/pull/2224",
            "state": "MERGED",
            "createdAt": "2024-01-27T06:10:58Z",
            "mergedAt": "2024-01-27T15:29:33Z",
            "closedAt": "2024-01-27T15:29:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Do not repeat system prompt for chat templating",
            "url": "https://github.com/ollama/ollama/pull/2241",
            "state": "MERGED",
            "createdAt": "2024-01-28T20:53:47Z",
            "mergedAt": "2024-01-28T22:15:57Z",
            "closedAt": "2024-01-28T22:15:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 4,
            "body": "Before:\r\n\r\n```\r\n<|im_start|>system\r\nYou are a happy dog<|im_end|>\r\n<|im_start|>assistant\r\nhi im a friendly assistant<|im_end|>\r\n<|im_start|>system\r\nYou are a happy dog<|im_end|>\r\n<|im_start|>user\r\nwho are you?<|im_end|>\r\n```\r\n\r\nAfter:\r\n\r\n```\r\n<|im_start|>system\r\nYou are a happy dog<|im_end|>\r\n<|im_start|>assistant\r\nhi im a friendly assistant<|im_end|>\r\n<|im_start|>user\r\nwho are you?<|im_end|>\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Harden for zero detected GPUs",
            "url": "https://github.com/ollama/ollama/pull/2243",
            "state": "MERGED",
            "createdAt": "2024-01-28T21:21:16Z",
            "mergedAt": "2024-01-28T21:30:44Z",
            "closedAt": "2024-01-28T21:30:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "At least with the ROCm libraries, its possible to have the library present with zero GPUs.  This fix avoids a divide by zero bug in llm.go when we try to calculate GPU memory with zero GPUs.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Log prompt when running `ollama serve` with `OLLAMA_DEBUG=1`",
            "url": "https://github.com/ollama/ollama/pull/2245",
            "state": "MERGED",
            "createdAt": "2024-01-28T23:12:57Z",
            "mergedAt": "2024-01-28T23:22:35Z",
            "closedAt": "2024-01-28T23:22:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Fixes https://github.com/ollama/ollama/issues/1533\r\nFixes https://github.com/ollama/ollama/issues/1118\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't disable GPUs on arm without AVX",
            "url": "https://github.com/ollama/ollama/pull/2246",
            "state": "MERGED",
            "createdAt": "2024-01-28T23:38:00Z",
            "mergedAt": "2024-01-29T00:26:55Z",
            "closedAt": "2024-01-29T00:26:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "AVX is an x86 feature, so ARM should be excluded from the check.\r\n\r\nRelated to #1979 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update submodule to `1cfb5372cf5707c8ec6dde7c874f4a44a6c4c915`",
            "url": "https://github.com/ollama/ollama/pull/2251",
            "state": "CLOSED",
            "createdAt": "2024-01-29T06:53:55Z",
            "mergedAt": null,
            "closedAt": "2024-02-07T20:08:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add container hints for troubleshooting",
            "url": "https://github.com/ollama/ollama/pull/2256",
            "state": "MERGED",
            "createdAt": "2024-01-29T16:53:54Z",
            "mergedAt": "2024-01-30T16:12:48Z",
            "closedAt": "2024-01-30T16:12:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "Some users are new to containers and unsure where the server logs go",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b1999",
            "url": "https://github.com/ollama/ollama/pull/2263",
            "state": "MERGED",
            "createdAt": "2024-01-30T00:57:33Z",
            "mergedAt": "2024-01-31T16:39:41Z",
            "closedAt": "2024-01-31T16:39:41Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 4
            },
            "additions": 130,
            "deletions": 27,
            "body": "This requires an upstream change to support graceful termination, carried as a patch.\r\n\r\nTracking branches for the 2 patches:\r\n- 01-cache.diff - https://github.com/dhiltgen/llama.cpp/tree/kv_cache\r\n- 02-shutdown.diff - https://github.com/dhiltgen/llama.cpp/tree/server_shutdown\r\n\r\nI'm going to mark it draft until I can run more testing (so far happy path on windows, mac and linux looks good)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add support for MIG mode detection and use",
            "url": "https://github.com/ollama/ollama/pull/2264",
            "state": "CLOSED",
            "createdAt": "2024-01-30T03:47:30Z",
            "mergedAt": null,
            "closedAt": "2024-05-25T15:37:05Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 2
            },
            "additions": 259,
            "deletions": 81,
            "body": "The issue here is that when the startup code checks for the capabilities of the GPU  so it can allocate resources (in particular memory), it mistakenly uses the host GPU for its check rather than the MIG instance. This PR modifies the algorithm of cuda GPU detection.  Essentially for each  host GPU, check it that GPU supports MIG and if MIG is enabled, and if yes then iterate over all MIG instances.  This results in a deviceMAP\r\n\r\n    typedef struct {\r\n      unsigned numDevices;\r\n      nvmlDevice_t **layout;\r\n    } deviceMap_t;\r\n\r\nLater, that map can be iterated over.  `layout[i][0]` is a pointer to the ith host GPU.  layout[i][j + 1] will is the jth MIG instance of host GPU **i**.  A value of `(void*)0` marks the end of the MIG instance list.  There can only be 7 total MIG instances per host GPU, so the size of the pointer array for each host is set to 9.  Both `cuda_check_vram` and `cuda_compute_capability` were updated to use this new data structure.\r\n\r\n MIG-related API calls were added to enable this see [multi GPU management](https://docs.nvidia.com/deploy/archive/R520/nvml-api/group__nvmlMultiInstanceGPU.html) for details\r\n\r\nAddresses #1500 ",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 18
            }
        }
    },
    {
        "node": {
            "title": "Add support for libcudart.so for CUDA devices (Adds Jetson support)",
            "url": "https://github.com/ollama/ollama/pull/2279",
            "state": "MERGED",
            "createdAt": "2024-01-30T16:50:18Z",
            "mergedAt": "2024-03-25T19:46:28Z",
            "closedAt": "2024-03-25T19:46:28Z",
            "reviews": {
                "totalCount": 19
            },
            "files": {
                "totalCount": 8
            },
            "additions": 438,
            "deletions": 83,
            "body": "Added libcudart.so support to gpu.go for CUDA devices that are missing libnvidia-ml.so. CUDA libraries split into nvml (libnvidia-ml.so) and cudart (libcudart.so), can work with either. Tested on Jetson device and on Windows 11 in WSL2.\r\n\r\nDevices used to test:\r\nJetson Orin Nano 8Gb\r\nJetpack 5.1.2, L4T 35.4.1\r\nCUDA 11-8\r\nCUDA Capability Supported 8.7\r\nGo version 1.26.1\r\nCmake 3.28.1\r\nnvcc 11.8.89\r\n\r\nAMD Ryzen 3950x\r\nNVidia RTX 3090ti\r\nWSL2 running Ubuntu 22.04\r\nWSL CUDA Toolkit v12.3 installed\r\n\r\nEdited for updates",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 42
            }
        }
    },
    {
        "node": {
            "title": "remove unnecessary parse raw",
            "url": "https://github.com/ollama/ollama/pull/2284",
            "state": "MERGED",
            "createdAt": "2024-01-31T01:02:05Z",
            "mergedAt": "2024-01-31T17:40:48Z",
            "closedAt": "2024-01-31T17:40:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 6,
            "body": "There's no point parsing the raw private key when all it's doing is creating a ssh key",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: preserve last system message from modelfile",
            "url": "https://github.com/ollama/ollama/pull/2289",
            "state": "MERGED",
            "createdAt": "2024-01-31T17:22:03Z",
            "mergedAt": "2024-02-01T02:45:01Z",
            "closedAt": "2024-02-01T02:45:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 66,
            "deletions": 17,
            "body": "When truncating messages to fit in the context window if the system message from the modelfile was used it was not carried over, this preserves the modelfile system message in the case of truncation. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update slog handler options",
            "url": "https://github.com/ollama/ollama/pull/2294",
            "state": "MERGED",
            "createdAt": "2024-01-31T23:00:55Z",
            "mergedAt": "2024-01-31T23:29:11Z",
            "closedAt": "2024-01-31T23:29:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 5,
            "body": "- consistent format by using text handler for debug and non-debug\r\n- truncate source file to just the file name\r\n\r\nsample outputs:\r\n\r\n```\r\ntime=2024-01-31T15:01:02.632-08:00 level=INFO source=routes.go:983 msg=\"Listening on 127.0.0.1:11434 (version 0.0.0)\"\r\ntime=2024-01-31T15:01:02.632-08:00 level=INFO source=payload_common.go:106 msg=\"Extracting dynamic libraries...\"\r\ntime=2024-01-31T15:01:02.653-08:00 level=INFO source=payload_common.go:145 msg=\"Dynamic LLM libraries [metal]\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "append image tags to user content",
            "url": "https://github.com/ollama/ollama/pull/2296",
            "state": "MERGED",
            "createdAt": "2024-02-01T00:31:48Z",
            "mergedAt": "2024-02-01T21:16:59Z",
            "closedAt": "2024-02-01T21:17:00Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 89,
            "deletions": 36,
            "body": "summary of changes:\r\n\r\n1. add `[img-x]` to prompt content when there are images. `x` corresponds to the image's id. for generate, this is just the image's index in the Images list. for chat, this is the image's index of among all images in the messages list\r\n2. account for image embedding when trimming the context. image projections produce 768 tokens for clip models. check and add this number to the total tokens count\r\n3. if the image tokens exceed the max token count, do not add images to the final images list and strip the image tag from the prompt content",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "structured debug prompt",
            "url": "https://github.com/ollama/ollama/pull/2298",
            "state": "MERGED",
            "createdAt": "2024-02-01T00:47:59Z",
            "mergedAt": "2024-02-01T21:16:49Z",
            "closedAt": "2024-02-01T21:16:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Improvements to `ollama run` for multimodal models",
            "url": "https://github.com/ollama/ollama/pull/2300",
            "state": "MERGED",
            "createdAt": "2024-02-01T03:01:01Z",
            "mergedAt": "2024-02-02T01:09:52Z",
            "closedAt": "2024-02-02T01:09:52Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 21,
            "body": "Depends on #2296 \r\n\r\nThis PR makes two changes to how `ollama run` works:\r\n\r\n1. Images are sent for all messages\r\n2. Messages without images can now be sent to multimodal models (they are language models too!), and instead a better prompt placeholder is provided. This is important for messages that don't include images but are relevant to a multimodal conversation (e.g. `I will provide you a list of messages, provide me the captions for each` or `tell me more about the image I provided earlier`)\r\n3. Only send the system prompt once",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix help string for stop parameter",
            "url": "https://github.com/ollama/ollama/pull/2307",
            "state": "MERGED",
            "createdAt": "2024-02-01T09:47:24Z",
            "mergedAt": "2024-05-07T23:48:35Z",
            "closedAt": "2024-05-07T23:48:35Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Changed the help prompt for setting the stop parameters, and quotes or commas are otherwise included in the stop-token:\r\n\r\n/set parameter stop \"?\", \"!\" # Invalid\r\n/set parameter stop ? ! # Valid",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Clear previous images when submitting a new image to `ollama run`",
            "url": "https://github.com/ollama/ollama/pull/2316",
            "state": "MERGED",
            "createdAt": "2024-02-02T02:11:43Z",
            "mergedAt": "2024-02-02T05:30:26Z",
            "closedAt": "2024-02-02T05:30:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add multimodel support to `ollama run` in noninteractive mode",
            "url": "https://github.com/ollama/ollama/pull/2317",
            "state": "MERGED",
            "createdAt": "2024-02-02T02:39:22Z",
            "mergedAt": "2024-02-02T05:33:06Z",
            "closedAt": "2024-02-02T05:33:06Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 56,
            "deletions": 49,
            "body": "Fixes https://github.com/ollama/ollama/issues/2295\r\n\r\n```\r\n% ollama run llava Describe this image: /Users/jmorgan/Desktop/old-tower.jpg \r\nAdded image '/Users/jmorgan/Desktop/old-tower.jpg'\r\n The image depicts a vibrant cityscape. In the foreground, there's an iconic skyscraper, which is the CN Tower, a landmark of Toronto, Canada. The tower stands prominently against a clear blue sky. In the background, you can see a variety \r\nof buildings, including what appears to be condominiums and commercial structures, all under a bright sunlight. There's a body of water visible in the lower right corner of the image, suggesting that this photo is taken from a vantage \r\npoint overlooking the city. The overall impression is of a bustling urban environment with a mix of architectural styles.\r\n```\r\n\r\n![old-tower](https://github.com/ollama/ollama/assets/251292/46265130-0ee1-43f8-b4f5-a5ed38688ede)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Harden generate patching model",
            "url": "https://github.com/ollama/ollama/pull/2318",
            "state": "MERGED",
            "createdAt": "2024-02-02T03:35:38Z",
            "mergedAt": "2024-02-02T04:41:29Z",
            "closedAt": "2024-02-02T04:41:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 8,
            "body": "Only apply patches if we have any, and make sure to cleanup every file we patched at the end to leave the tree clean",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add fast server stop",
            "url": "https://github.com/ollama/ollama/pull/2330",
            "state": "CLOSED",
            "createdAt": "2024-02-02T15:04:37Z",
            "mergedAt": null,
            "closedAt": "2024-05-06T22:52:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 42,
            "deletions": 13,
            "body": "Resolves #2052\r\n\r\nFirst sigterm for a graceful shutdown, second to kill the server.\r\n\r\n\r\nThere are no automated tests for this. Steps to reproduce:\r\nin 1st terminal:\r\n```sh\r\n# go build . \r\n ./ollama serve\r\n```\r\nin 2nd terminal\r\n```\r\n./ollama run llama2\r\n```\r\nthen start a request that takes some seconds: `long response 100 words min`\r\nWhile running, do  `Control-c` on terminal 1 twice. Server should exit immediately with code 1.\r\n\r\nAlso test graceful shutdown with 1 `Control-c` and wait for end of server response with exit code 0\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add llm-ollama plugin for Datasette's LLM CLI to README",
            "url": "https://github.com/ollama/ollama/pull/2340",
            "state": "MERGED",
            "createdAt": "2024-02-03T23:37:18Z",
            "mergedAt": "2024-02-03T23:40:50Z",
            "closedAt": "2024-02-03T23:40:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "The Datasette project's LLM cli provides a common interface to a variety of LLM APIs and local LLMs. This PR adds a link to an Ollama plugin for that tool.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Revamp the windows tray code",
            "url": "https://github.com/ollama/ollama/pull/2341",
            "state": "MERGED",
            "createdAt": "2024-02-04T01:00:37Z",
            "mergedAt": "2024-02-04T18:45:02Z",
            "closedAt": "2024-02-04T18:45:02Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 24
            },
            "additions": 1200,
            "deletions": 165,
            "body": "To get more control over our windows app this pulls the win32 logic into our Go code instead of using an upstream library.\r\n\r\n\r\nStill gobs of debug logging that I'll clean up soon, but it's now functional.  The upgrade flow doesn't work yet of course.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "reliably determine available VRAM on macOS (resolves #1826, #2370)",
            "url": "https://github.com/ollama/ollama/pull/2354",
            "state": "MERGED",
            "createdAt": "2024-02-04T20:54:11Z",
            "mergedAt": "2024-02-25T23:16:45Z",
            "closedAt": "2024-02-25T23:16:45Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 3
            },
            "additions": 21,
            "deletions": 16,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Fit and finish, clean up cruft on uninstall",
            "url": "https://github.com/ollama/ollama/pull/2355",
            "state": "MERGED",
            "createdAt": "2024-02-04T23:35:39Z",
            "mergedAt": "2024-02-05T00:10:08Z",
            "closedAt": "2024-02-05T00:10:08Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 15
            },
            "additions": 164,
            "deletions": 57,
            "body": "Better job cleaning up, logging improvements, and now spawn a powershell window on first use to help users with their first run",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Get paths right for first run, and deps",
            "url": "https://github.com/ollama/ollama/pull/2357",
            "state": "MERGED",
            "createdAt": "2024-02-05T04:51:02Z",
            "mergedAt": "2024-02-05T16:55:20Z",
            "closedAt": "2024-02-05T16:55:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 32,
            "deletions": 2,
            "body": "Tested inside a Win 10 home hyperV VM with nothing extra added.  This gets all the deps right and loads the CPU runner.\r\n\r\n![Screenshot 2024-02-04 at 8 50 15\u202fPM](https://github.com/ollama/ollama/assets/4033016/5ad64dd7-0203-4510-a672-0b31189439c5)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Replace `reflect` usage in option parsing",
            "url": "https://github.com/ollama/ollama/pull/2368",
            "state": "CLOSED",
            "createdAt": "2024-02-06T05:25:58Z",
            "mergedAt": null,
            "closedAt": "2024-08-11T16:52:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 89,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "disable rocm builds",
            "url": "https://github.com/ollama/ollama/pull/2374",
            "state": "MERGED",
            "createdAt": "2024-02-06T17:29:51Z",
            "mergedAt": "2024-02-06T17:41:03Z",
            "closedAt": "2024-02-06T17:41:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "rocm builds are failing because of disk space issues. disable them temporarily until larger runners\r\n\r\nresolves #2373",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI API compatibility",
            "url": "https://github.com/ollama/ollama/pull/2376",
            "state": "MERGED",
            "createdAt": "2024-02-06T20:08:06Z",
            "mergedAt": "2024-02-07T22:24:29Z",
            "closedAt": "2024-02-07T22:24:30Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 3
            },
            "additions": 466,
            "deletions": 0,
            "body": "This adds experimental compatibility with the OpenAI Chat Completions (i.e. `/v1/chat/completions`) API. Details on compatibility and supported fields are in`docs/openai.md` \r\n\r\nFixes #305 ",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "runners",
            "url": "https://github.com/ollama/ollama/pull/2378",
            "state": "MERGED",
            "createdAt": "2024-02-06T21:26:52Z",
            "mergedAt": "2024-02-06T21:49:58Z",
            "closedAt": "2024-02-06T21:49:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "[fix] /bye and /exit are now treated as prefixes",
            "url": "https://github.com/ollama/ollama/pull/2381",
            "state": "MERGED",
            "createdAt": "2024-02-07T06:03:15Z",
            "mergedAt": "2024-02-20T02:56:49Z",
            "closedAt": "2024-02-20T02:56:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "instead of being treated as entire lines which doesn't align with the way the rest of the commands are treated\r\nit was a little annoying typing \"/bye \" and not having it work as expected",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix response on token error",
            "url": "https://github.com/ollama/ollama/pull/2394",
            "state": "MERGED",
            "createdAt": "2024-02-07T19:06:48Z",
            "mergedAt": "2024-02-07T19:47:31Z",
            "closedAt": "2024-02-07T19:47:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 2,
            "body": "only template response body if it's not empty",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix hanging issue when sending empty content",
            "url": "https://github.com/ollama/ollama/pull/2399",
            "state": "MERGED",
            "createdAt": "2024-02-07T23:17:05Z",
            "mergedAt": "2024-02-08T00:30:33Z",
            "closedAt": "2024-02-08T00:30:33Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 24,
            "deletions": 21,
            "body": "This fixes an issue where the prompt would be templated as an empty string `\"\"`.\r\n\r\nFixes https://github.com/ollama/ollama/issues/2397\r\n\r\n```shell\r\n# loads model\r\n% curl http://localhost:11434/api/chat -d '{\r\n  \"model\": \"llama2\",\r\n  \"messages\": [],\r\n  \"stream\": false           \r\n}'\r\n{\"model\":\"llama2\",\"created_at\":\"2024-02-07T23:21:14.816749Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done\":true}\r\n```\r\n\r\n```shell\r\n# loads model\r\n% curl http://localhost:11434/api/chat -d '{\r\n  \"model\": \"llama2\",\r\n  \"messages\": [{ \"role\": \"user\", \"content\": \"\"}],\r\n  \"stream\": false\r\n}'\r\n{\"model\":\"llama2\",\"created_at\":\"2024-02-07T23:20:28.175454Z\",\"message\": {\"role\":\"assistant\",\"content\":\"\"},\"done\":true}\r\n```\r\n\r\n```shell\r\n# still works\r\n% curl http://localhost:11434/api/chat -d '{\r\n  \"model\": \"llama2\",\r\n  \"messages\": [{ \"role\": \"system\", \"content\": \"sing me a song!\"}],\r\n  \"stream\": false\r\n}'\r\n{\"model\":\"llama2\",\"created_at\":\"2024-02-07T23:19:01.582144Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Of course, I'd be happy to sing you a song! *clears throat* Here we go:\\n\\n\\\"Oh, the stars are shining bright and bold,\\nA celestial show, so fine and cold.\\nThe moon is smiling, its light so pure,\\nA gentle melody, for you I endure.\\n\\nIn the night's embrace, I find my peace,\\nThe world outside, a distant release.\\nI lose myself in the music of the night,\\nAnd let my spirit take flight.\\n\\nSo come and join me, in this song so bright,\\nTogether we'll dance, under the stars' delight.\\nWith every note, our hearts will be as one,\\nIn this magical moment, we'll have fun.\\\"\\n\\nHow was that? Did you enjoy it?\"},\"done\":true,\"total_duration\":5454697000,\"load_duration\":1188958,\"prompt_eval_duration\":177470000,\"eval_count\":182,\"eval_duration\":5275568000}\r\n```\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Ensure the libraries are present",
            "url": "https://github.com/ollama/ollama/pull/2403",
            "state": "MERGED",
            "createdAt": "2024-02-08T01:28:22Z",
            "mergedAt": "2024-02-08T01:55:31Z",
            "closedAt": "2024-02-08T01:55:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 2,
            "body": "When we store our libraries in a temp dir, a reaper might clean them when we are idle, so make sure to check for them before we reload.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "More robust shutdown",
            "url": "https://github.com/ollama/ollama/pull/2422",
            "state": "MERGED",
            "createdAt": "2024-02-09T06:29:27Z",
            "mergedAt": "2024-02-12T22:05:06Z",
            "closedAt": "2024-02-12T22:05:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 44,
            "deletions": 1,
            "body": "Make sure that when a shutdown signal comes, we shutdown quickly instead of waiting for a potentially long exchange to wrap up.\r\n\r\nMy initial strategy was going to be multiple signals to trigger a more aggressive shutdown, but that turned into a much more invasive change to try to recover once shutting down had already started, so I aborted that approach.  This now takes a simpler approach to simply stop new requests from coming in, canceling whatever is in flight at the next completion, and then shutting down once no requests are actively being processed.  If we want to refine this in the future to have the double-signal strategy, we can add that incrementally by just blocking new requests from coming in on the first signal, and on a second signal, cancel tasks that are still iterating in completion.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Install: don't overwrite systemd config if it exists",
            "url": "https://github.com/ollama/ollama/pull/2423",
            "state": "CLOSED",
            "createdAt": "2024-02-09T06:37:12Z",
            "mergedAt": null,
            "closedAt": "2024-02-12T00:50:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 12,
            "body": "A little annoying if you have a custom config of any kind, like an `OLLAMA_HOST` variable; but perhaps also you've set it up custom in other ways, like a custom user account.\r\n\r\n(I've used this locally and it works great)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update domain name references in docs and install script",
            "url": "https://github.com/ollama/ollama/pull/2435",
            "state": "MERGED",
            "createdAt": "2024-02-09T23:16:30Z",
            "mergedAt": "2024-02-09T23:19:30Z",
            "closedAt": "2024-02-09T23:19:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 13
            },
            "additions": 43,
            "deletions": 40,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "replace strings buffer with hasher",
            "url": "https://github.com/ollama/ollama/pull/2437",
            "state": "MERGED",
            "createdAt": "2024-02-10T00:54:52Z",
            "mergedAt": "2024-02-21T00:07:50Z",
            "closedAt": "2024-02-21T00:07:50Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 6,
            "body": "the buffered value is going into the hasher eventually so write directly to the hasher instead",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Add Odin Runes, a Feature-Rich Java UI for Ollama, to README",
            "url": "https://github.com/ollama/ollama/pull/2440",
            "state": "MERGED",
            "createdAt": "2024-02-10T07:41:15Z",
            "mergedAt": "2024-03-06T19:57:49Z",
            "closedAt": "2024-03-06T19:57:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "**Description:**\r\nHello,\r\n\r\nI've added Odin Runes to the README under the \"Community Integrations\" section. Odin Runes is a Java-based GPT client that facilitates seamless interaction with GPT models, enhancing productivity in prompt engineering and text generation tasks. This addition highlights the integration between Odin Runes and Ollama, offering users the flexibility to leverage large language models locally within their development workflow.\r\n\r\n\r\n**Changes:**\r\n- Added Odin Runes to the \"Community Integrations\" section of the README.\r\n\r\n**Demo:**\r\n![OdinRunes-Ollama-integration-demo](https://github.com/ollama/ollama/assets/26918192/ab51d273-f528-4e96-8608-477e36f3b35a)\r\nCaption: This GIF demonstrates the integration between Odin Runes and Ollama in action. \r\n\r\n**Context:**\r\nThis pull request addresses the need to document the integration between Odin Runes and Ollama, providing visibility to users who may benefit from the integration and fostering collaboration between our projects.\r\n\r\n\r\n**Closing Note:**\r\nI believe this addition will be beneficial to users and contributors alike. I'm open to any feedback or suggestions regarding the integration or the proposed README addition.\r\n\r\nThank you for considering my pull request.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Always add token to cache_tokens",
            "url": "https://github.com/ollama/ollama/pull/2459",
            "state": "MERGED",
            "createdAt": "2024-02-12T03:29:38Z",
            "mergedAt": "2024-02-12T16:10:16Z",
            "closedAt": "2024-02-12T16:10:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 25,
            "body": "The diff is a bit hard to read, but this is the actual fix for our `01` patch that fixes due to the kv cache being full\r\n\r\nI believe this fixes #2339 and #1458",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor chat prompt templating",
            "url": "https://github.com/ollama/ollama/pull/2460",
            "state": "MERGED",
            "createdAt": "2024-02-12T07:03:05Z",
            "mergedAt": "2024-02-12T23:06:57Z",
            "closedAt": "2024-02-12T23:06:57Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 7
            },
            "additions": 535,
            "deletions": 1010,
            "body": "This refactors the chat prompt processing to be a little easier to follow. It also fully deprecates `.First` in favor of the chat endpoint\r\n\r\nFixes https://github.com/ollama/ollama/issues/2443\r\nFixes https://github.com/ollama/ollama/issues/2438",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect AMD GPU info via sysfs and block old cards",
            "url": "https://github.com/ollama/ollama/pull/2465",
            "state": "MERGED",
            "createdAt": "2024-02-12T16:10:40Z",
            "mergedAt": "2024-02-12T20:41:43Z",
            "closedAt": "2024-02-12T20:41:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 151,
            "deletions": 34,
            "body": "This wires up some new logic to start using sysfs to discover AMD GPU information and detects old cards we can't yet support so we can fallback to CPU mode.\r\n\r\nThis also serves as an initial foundation where I believe we'll be able to move away from the AMD management library and query the sysfs files to discover the details we need with less complexity.\r\n\r\nThis will mitigate some cases of  #2165 \r\n\r\nTested on a `Radeon RX 580` and it correctly falls back to CPU.\r\n\r\n```\r\ntime=2024-02-12T16:02:58.657Z level=INFO source=gpu.go:157 msg=\"AMD Driver: 6.2.4\"\r\ntime=2024-02-12T16:02:58.657Z level=INFO source=gpu.go:162 msg=\"AMD GPU too old, falling back to CPU gfx803\"\r\ntime=2024-02-12T16:02:58.657Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Check image filetype in api handlers",
            "url": "https://github.com/ollama/ollama/pull/2467",
            "state": "MERGED",
            "createdAt": "2024-02-12T17:17:19Z",
            "mergedAt": "2024-02-12T19:16:20Z",
            "closedAt": "2024-02-12T19:16:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 24,
            "deletions": 1,
            "body": "Fixes: https://github.com/ollama/ollama/issues/2456",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp submodule to `099afc6`",
            "url": "https://github.com/ollama/ollama/pull/2468",
            "state": "MERGED",
            "createdAt": "2024-02-12T20:35:40Z",
            "mergedAt": "2024-02-12T22:01:16Z",
            "closedAt": "2024-02-12T22:01:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add OpenAI /v1/models API support",
            "url": "https://github.com/ollama/ollama/pull/2476",
            "state": "CLOSED",
            "createdAt": "2024-02-13T15:00:57Z",
            "mergedAt": null,
            "closedAt": "2024-07-02T18:50:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 100,
            "deletions": 0,
            "body": "Add openaAI API **v1/models** endpoint compatibility.\r\n\r\nSee spec at: https://platform.openai.com/docs/api-reference/models/list\r\n\r\nPersonally I am not so sure about putting the ListModelsHandlerOpenAI method into the router file, however the original ollama ListModelsHandler function is also there.\r\n\r\nI generally don't write go, so sorry for any weird things. Let me know what you think about this change.\r\n\r\nRequested in #2430\r\n\r\nExample usage:\r\n\r\n```shell\r\n\u276f curl http://localhost:11434/v1/models | jq\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   226  100   226    0     0  33776      0 --:--:-- --:--:-- --:--:--  110k\r\n{\r\n  \"object\": \"list\",\r\n  \"data\": [\r\n    {\r\n      \"id\": \"codegpt/deepseek-coder-1.3b-typescript:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1707753573,\r\n      \"owned_by\": \"ollama\"\r\n    },\r\n    {\r\n      \"id\": \"deepseek-coder:6.7b\",\r\n      \"object\": \"model\",\r\n      \"created\": 1705498161,\r\n      \"owned_by\": \"ollama\"\r\n    }\r\n  ]\r\n}\r\n```",
            "participants": {
                "totalCount": 10
            },
            "comments": {
                "totalCount": 10
            }
        }
    },
    {
        "node": {
            "title": "Update README.md to include link to Ollama-ex Elixir library",
            "url": "https://github.com/ollama/ollama/pull/2477",
            "state": "MERGED",
            "createdAt": "2024-02-13T19:37:20Z",
            "mergedAt": "2024-02-13T19:40:44Z",
            "closedAt": "2024-02-13T19:40:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix negative keep_alive",
            "url": "https://github.com/ollama/ollama/pull/2480",
            "state": "MERGED",
            "createdAt": "2024-02-13T22:00:15Z",
            "mergedAt": "2024-02-13T23:40:32Z",
            "closedAt": "2024-02-13T23:40:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 4,
            "body": "Using a negative `keep_alive` did not work as expected, models would unload instantly. Some logging of the duration which was unmarshalled showed that the `math.MaxFloat64` was resulting in a very large negative duration rather than the desired very large positive duration.\r\n\r\nI just changed these codepaths to use `math.MaxInt64`, which is what I understand `time.Duration` to expect.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Windows App preview",
            "url": "https://github.com/ollama/ollama/pull/2481",
            "state": "CLOSED",
            "createdAt": "2024-02-13T22:22:36Z",
            "mergedAt": null,
            "closedAt": "2024-02-15T17:25:43Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 87
            },
            "additions": 2756,
            "deletions": 219,
            "body": "Fixes #403 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "set `shutting_down` to `false` once shutdown is complete",
            "url": "https://github.com/ollama/ollama/pull/2484",
            "state": "MERGED",
            "createdAt": "2024-02-14T01:44:48Z",
            "mergedAt": "2024-02-14T01:48:42Z",
            "closedAt": "2024-02-14T01:48:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert \"Revert \"bump submodule to `6c00a06` (#2479)\"\"",
            "url": "https://github.com/ollama/ollama/pull/2485",
            "state": "MERGED",
            "createdAt": "2024-02-14T01:52:39Z",
            "mergedAt": "2024-02-14T02:18:41Z",
            "closedAt": "2024-02-14T02:18:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This reverts commit 6920964b87971c8201097130bfdedbf56aaa13a7.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "self extend support",
            "url": "https://github.com/ollama/ollama/pull/2486",
            "state": "CLOSED",
            "createdAt": "2024-02-14T02:43:21Z",
            "mergedAt": null,
            "closedAt": "2024-08-11T01:35:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 0,
            "body": "trying to add support for self-extend as discussed here: https://github.com/ollama/ollama/issues/1964\r\n\r\nI was hoping it would be as simple as adding these parameters as I've seen done in a previous commit, but I was copying moves from an older configuration of the source.\r\n\r\nobviously I'm missing something.. probably rudimentary.\r\n\r\n```\r\nllm/dyn_ext_server.go:102:10: sparams.g_size undefined (type _Ctype_struct_ext_server_params has no field or method g_size)\r\nllm/dyn_ext_server.go:103:10: sparams.w_size undefined (type _Ctype_struct_ext_server_params has no field or method w_size)\r\n```\r\n\r\nHoping some more seasoned golang developer will help from here",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Windows Preview",
            "url": "https://github.com/ollama/ollama/pull/2499",
            "state": "MERGED",
            "createdAt": "2024-02-14T18:40:19Z",
            "mergedAt": "2024-02-16T00:06:32Z",
            "closedAt": "2024-02-16T00:06:33Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 89
            },
            "additions": 2873,
            "deletions": 340,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "handle race condition while setting raw mode in windows",
            "url": "https://github.com/ollama/ollama/pull/2509",
            "state": "MERGED",
            "createdAt": "2024-02-15T05:02:22Z",
            "mergedAt": "2024-02-15T05:28:35Z",
            "closedAt": "2024-02-15T05:28:35Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 38,
            "deletions": 15,
            "body": "This change handles a race condition in the go routine which handles reading in runes. On Windows \"raw mode\" (i.e. turning off echo/line/processed input) gets turned off too late which would cause `ReadRune()` to wait until the buffer was full (when it got a new line). This change goes into raw mode faster, but it still needs to happen before any input since we turn it back off again once we start processing output.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "[nit] Remove unused msg local var.",
            "url": "https://github.com/ollama/ollama/pull/2511",
            "state": "MERGED",
            "createdAt": "2024-02-15T07:46:38Z",
            "mergedAt": "2024-02-20T19:02:35Z",
            "closedAt": "2024-02-20T19:02:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "It's not used but clutters code.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix a couple duplicate instance bugs",
            "url": "https://github.com/ollama/ollama/pull/2516",
            "state": "MERGED",
            "createdAt": "2024-02-15T16:41:42Z",
            "mergedAt": "2024-02-15T23:52:43Z",
            "closedAt": "2024-02-15T23:52:43Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 6
            },
            "additions": 11,
            "deletions": 9,
            "body": "- Prevent the installer running multiple times concurrently\r\n- Detect multiple apps running and exit ~~with a message~~ ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Harden the OLLAMA_HOST lookup for quotes",
            "url": "https://github.com/ollama/ollama/pull/2526",
            "state": "MERGED",
            "createdAt": "2024-02-15T21:47:30Z",
            "mergedAt": "2024-02-15T22:13:40Z",
            "closedAt": "2024-02-15T22:13:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes #2512 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use http.DefaultClient",
            "url": "https://github.com/ollama/ollama/pull/2530",
            "state": "MERGED",
            "createdAt": "2024-02-16T00:18:55Z",
            "mergedAt": "2024-02-20T23:34:47Z",
            "closedAt": "2024-02-20T23:34:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 9,
            "deletions": 52,
            "body": "default client already handles proxy: https://pkg.go.dev/net/http#RoundTripper",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add gguf file types",
            "url": "https://github.com/ollama/ollama/pull/2532",
            "state": "MERGED",
            "createdAt": "2024-02-16T02:12:04Z",
            "mergedAt": "2024-02-21T00:06:29Z",
            "closedAt": "2024-02-21T00:06:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 23,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: use requested model template",
            "url": "https://github.com/ollama/ollama/pull/2541",
            "state": "CLOSED",
            "createdAt": "2024-02-16T15:05:05Z",
            "mergedAt": null,
            "closedAt": "2024-02-16T19:02:13Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "As reported in scenario 1 of #2492 \r\n\r\nWhen a request was made to a model than inherits from the currently loaded model the system and template were not updated in the `/chat` endpoint. The fix is to use the requested model rather than the loaded one.\r\n\r\nSteps to reproduce:\r\n1. Create a model that overrides the system prompt of another model:\r\n```\r\nFROM phi\r\nSYSTEM \"\"\"I want you to speak French only.\"\"\"\r\n```\r\n`ollama create phi-french -f ~/models/phi-french/Modelfile`\r\n2. Run the base model\r\n`ollama run phi`\r\n3. Quit the repl and run the custom model\r\n```\r\nollama run phi-french\r\n```\r\nThe system message from the base model was not changed, as the loaded model did not change.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: chat system prompting overrides",
            "url": "https://github.com/ollama/ollama/pull/2542",
            "state": "MERGED",
            "createdAt": "2024-02-16T16:44:11Z",
            "mergedAt": "2024-02-16T19:42:43Z",
            "closedAt": "2024-02-16T19:42:43Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 4
            },
            "additions": 24,
            "deletions": 41,
            "body": "This change fixes two more system message related issues with the CLI and message templates.\r\n- When `/set system ...` is run multiple times in the CLI, use only the most recent system message rather than adding multiple system messages to the history.\r\n- Do not add the model's default message as a first message when a new system message is specified.\r\n- When a request was made to a model than inherits from the currently loaded model the system and template were not updated in the /chat endpoint. The fix is to use the requested model rather than the loaded one.\r\n\r\nPrevious behavior, when running a model and setting a new system message:\r\n```\r\nollama run phi\r\n>>> /set system you are mario\r\nSet system message.\r\n>>> hi\r\n```\r\n```\r\nlevel=DEBUG source=routes.go:1205 msg=\"chat handler\" prompt=\"System: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful answers to the user's questions.\\nUser: \\nAssistant:System: you are mario\\nUser: hi\\nAssistant:\"\r\n```\r\n\r\nNew behavior:\r\n```\r\nlevel=DEBUG source=routes.go:1205 msg=\"chat handler\" prompt=\"System: you are mario\\nUser: hi\\nAssistant:\"\r\n```\r\n\r\nresolves #2492 \r\n\r\nFollow up: This keep the \"system message history\" further testing on model behavior of this is needed, it could be better to just override the system message, and not keep the old system message in the history.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update faq.md with the location of models on Windows",
            "url": "https://github.com/ollama/ollama/pull/2545",
            "state": "MERGED",
            "createdAt": "2024-02-16T18:07:07Z",
            "mergedAt": "2024-02-16T19:04:19Z",
            "closedAt": "2024-02-16T19:04:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix duplicate menus on update and exit on signals",
            "url": "https://github.com/ollama/ollama/pull/2552",
            "state": "MERGED",
            "createdAt": "2024-02-16T23:35:09Z",
            "mergedAt": "2024-02-17T01:23:37Z",
            "closedAt": "2024-02-17T01:23:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 53,
            "deletions": 37,
            "body": "Also fixes a few fit-and-finish items for better developer experience\r\n\r\nFixes #2521 \r\nFixes #2522 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Harden AMD driver lookup logic",
            "url": "https://github.com/ollama/ollama/pull/2553",
            "state": "MERGED",
            "createdAt": "2024-02-17T00:22:52Z",
            "mergedAt": "2024-02-17T01:23:12Z",
            "closedAt": "2024-02-17T01:23:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 16,
            "deletions": 3,
            "body": "It looks like the version file doesn't exist on older(?) drivers\r\n\r\nFixes #2502 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added FAQ section in readme to reduce issue count",
            "url": "https://github.com/ollama/ollama/pull/2554",
            "state": "CLOSED",
            "createdAt": "2024-02-17T02:05:51Z",
            "mergedAt": null,
            "closedAt": "2024-05-07T23:44:09Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "* There's a lot of issues that are being used as an Ollama stack overflow. Adding a section like this will help a lot of people\r\n\r\n* Fixes #2497\r\n\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Update Web UI link to new project name",
            "url": "https://github.com/ollama/ollama/pull/2563",
            "state": "MERGED",
            "createdAt": "2024-02-17T16:07:29Z",
            "mergedAt": "2024-02-18T04:05:20Z",
            "closedAt": "2024-02-18T04:05:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Ollama WebUI is now known as Open WebUI:\r\n\r\nhttps://openwebui.com\r\n\r\nhttps://github.com/open-webui/open-webui",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added community link for Ollama Copilot",
            "url": "https://github.com/ollama/ollama/pull/2582",
            "state": "MERGED",
            "createdAt": "2024-02-18T20:17:50Z",
            "mergedAt": "2024-03-04T08:40:36Z",
            "closedAt": "2024-03-04T08:40:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix cuda leaks",
            "url": "https://github.com/ollama/ollama/pull/2585",
            "state": "MERGED",
            "createdAt": "2024-02-19T02:37:52Z",
            "mergedAt": "2024-02-19T20:48:00Z",
            "closedAt": "2024-02-19T20:48:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 136,
            "deletions": 9,
            "body": "This should resolve the problem where we don't fully unload from the GPU when we go idle.\r\n\r\nFixes #1848 \r\n\r\nThis carries the upstream PR https://github.com/ggerganov/llama.cpp/pull/5576 as a patch until that's reviewed/merged.\r\n\r\nThis also updates the shutdown patch to match what was [merged upstream](https://github.com/ggerganov/llama.cpp/pull/5244#pullrequestreview-1887279675) since we haven't yet bumped llama.cpp to pick that up.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Explicitly disable AVX2 on GPU builds",
            "url": "https://github.com/ollama/ollama/pull/2599",
            "state": "MERGED",
            "createdAt": "2024-02-19T20:43:31Z",
            "mergedAt": "2024-02-19T21:13:05Z",
            "closedAt": "2024-02-19T21:13:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Even though we weren't setting it to on, somewhere in the cmake config it was getting toggled on.  By explicitly setting it to off, we get `/arch:AVX` as intended.\r\n\r\nfixes #2527 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document setting server vars for windows",
            "url": "https://github.com/ollama/ollama/pull/2600",
            "state": "MERGED",
            "createdAt": "2024-02-19T21:12:26Z",
            "mergedAt": "2024-02-19T21:46:37Z",
            "closedAt": "2024-02-19T21:46:37Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 0,
            "body": "Fixes #2546",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add faqs for memory pre-loading and the keep_alive setting",
            "url": "https://github.com/ollama/ollama/pull/2601",
            "state": "MERGED",
            "createdAt": "2024-02-19T22:31:17Z",
            "mergedAt": "2024-02-19T22:45:25Z",
            "closedAt": "2024-02-19T22:45:25Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 34,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Support for `bert` and `nomic-bert` embedding models",
            "url": "https://github.com/ollama/ollama/pull/2604",
            "state": "MERGED",
            "createdAt": "2024-02-20T04:46:07Z",
            "mergedAt": "2024-02-21T02:37:29Z",
            "closedAt": "2024-02-21T02:37:29Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 14,
            "deletions": 0,
            "body": "Fixes #327 \r\n\r\nThis adds initial support for embedding models using the `/api/embeddings` endpoint.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp submodule to `66c1968f7`",
            "url": "https://github.com/ollama/ollama/pull/2618",
            "state": "MERGED",
            "createdAt": "2024-02-20T19:35:40Z",
            "mergedAt": "2024-02-20T22:42:31Z",
            "closedAt": "2024-02-20T22:42:31Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 6
            },
            "additions": 39,
            "deletions": 130,
            "body": "This update's the llama.cpp commit to one that supports the newer embedding models. A few updates:\r\n\r\n- The previous patch 02 was merged \ud83c\udf89 \r\n- Numa is now an enum\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "note on naming restrictions",
            "url": "https://github.com/ollama/ollama/pull/2625",
            "state": "MERGED",
            "createdAt": "2024-02-21T05:52:41Z",
            "mergedAt": "2024-05-06T23:03:21Z",
            "closedAt": "2024-05-06T23:03:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "else push would fail with cryptic\r\nretrieving manifest \r\nError: file does not exist\r\n==> maybe change that in code too",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update big-AGI config file link",
            "url": "https://github.com/ollama/ollama/pull/2626",
            "state": "MERGED",
            "createdAt": "2024-02-21T06:21:02Z",
            "mergedAt": "2024-02-21T06:24:49Z",
            "closedAt": "2024-02-21T06:24:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The old URL of big-AGI config file is not available, replace it to the latest",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update install.sh success message",
            "url": "https://github.com/ollama/ollama/pull/2657",
            "state": "MERGED",
            "createdAt": "2024-02-21T23:30:32Z",
            "mergedAt": "2024-02-21T23:55:20Z",
            "closedAt": "2024-02-21T23:55:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixed #2640 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove format private key",
            "url": "https://github.com/ollama/ollama/pull/2719",
            "state": "MERGED",
            "createdAt": "2024-02-24T00:54:51Z",
            "mergedAt": "2024-02-24T01:15:14Z",
            "closedAt": "2024-02-24T01:15:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 11,
            "deletions": 155,
            "body": "the utility format/openssh.go is no longer necessary since x/crypto/ssh v0.14.0 introduced MarshalPrivateKey",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update types.go",
            "url": "https://github.com/ollama/ollama/pull/2744",
            "state": "MERGED",
            "createdAt": "2024-02-25T15:21:39Z",
            "mergedAt": "2024-02-25T18:41:25Z",
            "closedAt": "2024-02-25T18:41:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "specfied -> specified",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: Add LLM-X to Web Integration section",
            "url": "https://github.com/ollama/ollama/pull/2759",
            "state": "MERGED",
            "createdAt": "2024-02-26T07:03:20Z",
            "mergedAt": "2024-03-07T15:11:53Z",
            "closedAt": "2024-03-07T15:11:53Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Adding yet another web project to the list in the readme!",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b2276",
            "url": "https://github.com/ollama/ollama/pull/2771",
            "state": "MERGED",
            "createdAt": "2024-02-27T00:51:50Z",
            "mergedAt": "2024-02-27T19:29:53Z",
            "closedAt": "2024-02-27T19:29:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes #2758 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refine container image build script",
            "url": "https://github.com/ollama/ollama/pull/2772",
            "state": "MERGED",
            "createdAt": "2024-02-27T01:26:58Z",
            "mergedAt": "2024-02-27T19:29:08Z",
            "closedAt": "2024-02-27T19:29:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 3,
            "body": "Allow overriding the platform, image name, and tag latest for standard and rocm images.\r\n\r\nFixes #2721 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: NextChat",
            "url": "https://github.com/ollama/ollama/pull/2780",
            "state": "MERGED",
            "createdAt": "2024-02-27T13:24:14Z",
            "mergedAt": "2024-02-29T20:12:13Z",
            "closedAt": "2024-02-29T20:12:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "NextChat just drop support for Ollama, Adding NextChat reference in README.md\r\n\r\nDoc: https://docs.nextchat.dev/models/ollama\r\nRelease: https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/tag/v2.11.2",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Log unexpected server errors checking for update",
            "url": "https://github.com/ollama/ollama/pull/2785",
            "state": "MERGED",
            "createdAt": "2024-02-27T17:17:53Z",
            "mergedAt": "2024-02-27T18:43:14Z",
            "closedAt": "2024-02-27T18:43:15Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "This should unmask some failure modes that likely\r\nshow up in app logs as unmarshal errors",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "prepend image tags",
            "url": "https://github.com/ollama/ollama/pull/2789",
            "state": "MERGED",
            "createdAt": "2024-02-27T22:25:53Z",
            "mergedAt": "2024-02-29T19:30:14Z",
            "closedAt": "2024-02-29T19:30:14Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 21,
            "deletions": 18,
            "body": "instead of appending image tags, prepend them which produces better results in general\r\n\r\nresolves #2769\r\nresolves #2788 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Convert Safetensors to an Ollama model",
            "url": "https://github.com/ollama/ollama/pull/2824",
            "state": "MERGED",
            "createdAt": "2024-02-29T03:06:54Z",
            "mergedAt": "2024-03-07T05:01:52Z",
            "closedAt": "2024-03-07T05:01:52Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 9
            },
            "additions": 3083,
            "deletions": 153,
            "body": "This (admittedly very large) change converts a safetensors file in the `FROM` line of a Modelfile into a 16 bit *non-quantized* Ollama model without having to use llamacpp's `convert.py` script. This initial version works with Mistral v0.2 (and presumably v0.1 although I haven't yet tested this), and with some tweaks should probably also work with Gemma as well.\r\n\r\nSome things to note:\r\n* Currently it only works with SentencePiece tokenization. We can add BPE and other Tokenizers in the future to support more model types\r\n* Quantization is not yet supported, but this will be supported in a future change to easily allow different quantization levels\r\n* LORA adapters don't currently work, but it would be nice to support his in the future\r\n* llamacpp requires repacking the `q` and `k` attention layers to swap some of the axes. This required pulling in the `gorgonia.org/tensors` package which is somewhat abandoned. I forked that library and it's hosted at `github.com/pdevine/tensors` but ideally we wouldn't have to fork it.\r\n* A lot of processing is around converting brainfloat16 numbers into float16. Neither of those formats are supported by golang which kind of sucks.\r\n* I mapped all of the params that Mistral required to build the GGUF file, but there are probably some that are missing. Those should be added as we support more models.\r\n* I haven't *yet* added unit tests here, which would be really nice, but there's so much to test!",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "fix: print usedMemory size right",
            "url": "https://github.com/ollama/ollama/pull/2827",
            "state": "MERGED",
            "createdAt": "2024-02-29T06:02:07Z",
            "mergedAt": "2024-02-29T19:11:04Z",
            "closedAt": "2024-02-29T19:11:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Omit build date from gzip headers",
            "url": "https://github.com/ollama/ollama/pull/2836",
            "state": "MERGED",
            "createdAt": "2024-02-29T16:32:43Z",
            "mergedAt": "2024-02-29T23:46:46Z",
            "closedAt": "2024-02-29T23:46:46Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "See https://reproducible-builds.org/ for why this is good.\n\nThis patch was done while working on reproducible builds for openSUSE.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add env var so podman will map cuda GPUs",
            "url": "https://github.com/ollama/ollama/pull/2837",
            "state": "MERGED",
            "createdAt": "2024-02-29T16:43:41Z",
            "mergedAt": "2024-02-29T23:47:37Z",
            "closedAt": "2024-02-29T23:47:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Without this env var, podman's GPU logic doesn't map the GPU through\r\n\r\nFixes #2716 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add ollama user to video group",
            "url": "https://github.com/ollama/ollama/pull/2838",
            "state": "MERGED",
            "createdAt": "2024-02-29T16:50:39Z",
            "mergedAt": "2024-02-29T23:47:57Z",
            "closedAt": "2024-02-29T23:47:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "On OpenSUSE, ollama needs to be a member of the video group to access the GPU\r\n\r\nFixes #2587 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix embeddings load model behavior",
            "url": "https://github.com/ollama/ollama/pull/2848",
            "state": "MERGED",
            "createdAt": "2024-03-01T01:26:51Z",
            "mergedAt": "2024-03-01T01:40:56Z",
            "closedAt": "2024-03-01T01:40:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 5,
            "deletions": 7,
            "body": "Fixes https://github.com/ollama/ollama/issues/2810. Also cleans up the `embedding_only` API option which had no effect.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp submodule to `c29af7e`",
            "url": "https://github.com/ollama/ollama/pull/2868",
            "state": "MERGED",
            "createdAt": "2024-03-01T22:14:27Z",
            "mergedAt": "2024-03-01T23:26:04Z",
            "closedAt": "2024-03-01T23:26:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 24,
            "deletions": 25,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "api: start adding documentation to package api",
            "url": "https://github.com/ollama/ollama/pull/2878",
            "state": "MERGED",
            "createdAt": "2024-03-02T14:08:51Z",
            "mergedAt": "2024-04-10T17:31:55Z",
            "closedAt": "2024-04-10T17:31:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 74,
            "deletions": 10,
            "body": "Updates #2840\r\n\r\nThis is an initial PR just to double check that I'm heading in the right direction. If it looks good, I can update it (or send separate ones) to fill up the whole documentation for the `api` package.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "examples: start adding Go examples using api/",
            "url": "https://github.com/ollama/ollama/pull/2879",
            "state": "MERGED",
            "createdAt": "2024-03-02T14:38:39Z",
            "mergedAt": "2024-04-10T17:26:45Z",
            "closedAt": "2024-04-10T17:26:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 77,
            "deletions": 0,
            "body": "We can have the same examples as e.g. https://github.com/ollama/ollama-python/tree/main/examples here. Using consistent naming and renaming the existing example to have -http- since it uses direct HTTP requests rather than api/\r\n\r\nUpdates #2840",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: Alpaca webUI",
            "url": "https://github.com/ollama/ollama/pull/2881",
            "state": "MERGED",
            "createdAt": "2024-03-02T15:39:54Z",
            "mergedAt": "2024-03-25T19:00:19Z",
            "closedAt": "2024-03-25T19:00:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Created a simple web UI for Ollama, so it would be nice to add a link to the repo to access it from your README's link list.  :blush:",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Revamp ROCm support",
            "url": "https://github.com/ollama/ollama/pull/2885",
            "state": "MERGED",
            "createdAt": "2024-03-03T00:16:23Z",
            "mergedAt": "2024-03-07T18:51:00Z",
            "closedAt": "2024-03-07T18:51:00Z",
            "reviews": {
                "totalCount": 29
            },
            "files": {
                "totalCount": 27
            },
            "additions": 1091,
            "deletions": 588,
            "body": "This refines where we extract the LLM libraries to by adding a new OLLAMA_HOME env var, that defaults to `~/.ollama` The logic was already idempotenent, so this should speed up startups after the first time a new release is deployed.  It also cleans up after itself.\r\n\r\nWe now build only a single ROCm version (latest major) on both windows and linux.  Given the large size of ROCms tensor files, we split the dependency out.  It's bundled into the installer on windows, and a separate download on windows.  The linux install script is now smart and detects the presence of AMD GPUs and looks to see if rocm v6 is already present, and if not, then downloads our dependency tar file.\r\n\r\nFor Linux discovery, we now use sysfs and check each GPU against what ROCm supports so we can degrade to CPU gracefully instead of having llama.cpp+rocm assert/crash on us.  For Windows, we now use go's windows dynamic library loading logic to access the amdhip64.dll APIs to query the GPU information.\r\n\r\nFixes #2598 \r\nFixes #738 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add NotesOllama to Community Integrations",
            "url": "https://github.com/ollama/ollama/pull/2909",
            "state": "MERGED",
            "createdAt": "2024-03-04T09:14:47Z",
            "mergedAt": "2024-03-04T09:18:10Z",
            "closedAt": "2024-03-04T09:18:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Run inference in a subprocess",
            "url": "https://github.com/ollama/ollama/pull/2910",
            "state": "CLOSED",
            "createdAt": "2024-03-04T10:01:47Z",
            "mergedAt": null,
            "closedAt": "2024-04-07T06:09:01Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 1,
            "body": "This changes the underlying llama server to run in a subprocess, bringing back code from https://github.com/ollama/ollama/blob/v0.1.17/llm/llama.go while keeping the multi-variant support. This is helpful to make sure resources are freed when a model is unloaded and will help allow concurrent models to be loaded.\r\n\r\nNote this should probably go in after https://github.com/ollama/ollama/pull/2885 \r\n\r\nRemaining\r\n- [ ] Handle crash/exit scenario (api will hang)\r\n- [ ] Surface stderr message as an api error\r\n- [ ] CI",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: LibreChat",
            "url": "https://github.com/ollama/ollama/pull/2918",
            "state": "MERGED",
            "createdAt": "2024-03-04T16:51:11Z",
            "mergedAt": "2024-03-25T18:59:18Z",
            "closedAt": "2024-03-25T18:59:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "**LibreChat** is the leading ChatGPT-style Web UI that supports virtually every OpenAI-API-compatible endpoint, which now includes Ollama.\r\n\r\n**Relevant docs:** https://docs.librechat.ai/install/configuration/ollama.html\r\n\r\n**Screenshot**\r\n\r\n![309832679-fcfe48cf-bbd6-4010-915f-66b4b238728d](https://github.com/ollama/ollama/assets/110412045/3756434e-1201-444e-b4b7-d4592b5c26cb)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "refactor model parsing",
            "url": "https://github.com/ollama/ollama/pull/2926",
            "state": "MERGED",
            "createdAt": "2024-03-04T22:12:49Z",
            "mergedAt": "2024-04-01T20:58:13Z",
            "closedAt": "2024-04-01T20:58:13Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 5
            },
            "additions": 131,
            "deletions": 197,
            "body": "- refactor metadata",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: OllamaGUI",
            "url": "https://github.com/ollama/ollama/pull/2927",
            "state": "MERGED",
            "createdAt": "2024-03-05T04:21:38Z",
            "mergedAt": "2024-03-25T18:58:29Z",
            "closedAt": "2024-03-25T18:58:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "\ud83d\udc4b I have added new integrations for macos!\r\n\r\n\r\nhttps://github.com/ollama/ollama/assets/54224095/eff7fd87-d8f6-4b97-bf06-f52e40e56dcc\r\n\r\n\r\nlink: https://github.com/enoch1118/ollamaGUI",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Community Integration: OpenAOE",
            "url": "https://github.com/ollama/ollama/pull/2946",
            "state": "MERGED",
            "createdAt": "2024-03-06T02:52:01Z",
            "mergedAt": "2024-03-25T18:57:40Z",
            "closedAt": "2024-03-25T18:57:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "[OpenAOE](https://github.com/InternLM/OpenAOE) is a LLM Group Chat Framework: chat with multiple LLMs at the same time. \r\nNow the mistral-7b and gemma-7b models are added into OpenAOE under Ollama inference.\r\n\r\n![image](https://github.com/ollama/ollama/assets/11885550/bc964015-4a8e-44c9-92d9-60a2ea1b8b30)\r\n\r\n![image](https://github.com/ollama/ollama/assets/11885550/cf3f4003-c0eb-44ed-b2ad-646369bbbe2a)\r\n\r\n![image](https://github.com/ollama/ollama/assets/11885550/4adc6ad5-2f89-45c1-b1be-2ce96f89863d)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix json encoder",
            "url": "https://github.com/ollama/ollama/pull/2959",
            "state": "CLOSED",
            "createdAt": "2024-03-06T19:48:35Z",
            "mergedAt": null,
            "closedAt": "2024-03-06T21:04:14Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: document environment variables for serve command",
            "url": "https://github.com/ollama/ollama/pull/2961",
            "state": "MERGED",
            "createdAt": "2024-03-06T21:17:52Z",
            "mergedAt": "2024-03-06T21:48:46Z",
            "closedAt": "2024-03-06T21:48:46Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 1,
            "body": "Updates #2944",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: add usage for `ollama run` environment variables",
            "url": "https://github.com/ollama/ollama/pull/2962",
            "state": "MERGED",
            "createdAt": "2024-03-06T21:56:32Z",
            "mergedAt": "2024-03-07T21:57:08Z",
            "closedAt": "2024-03-07T21:57:08Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 15,
            "body": "Also, fix the indentation of usage for `ollama serve` environment variables.\r\n\r\nFixes #2944 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Allow setting max vram for workarounds",
            "url": "https://github.com/ollama/ollama/pull/2964",
            "state": "MERGED",
            "createdAt": "2024-03-07T00:55:57Z",
            "mergedAt": "2024-03-07T17:25:44Z",
            "closedAt": "2024-03-07T17:25:44Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 24,
            "deletions": 0,
            "body": "Until we get all the memory calculations correct, this can provide and escape valve for users to workaround out of memory crashes.\r\n\r\nExample usage (windows)\r\n```powershell\r\n$env:OLLAMA_MAX_VRAM=\"3221225472\"\r\nollama.exe serve\r\n...\r\ntime=2024-03-06T16:52:04.246-08:00 level=INFO source=gpu.go:251 msg=\"user override OLLAMA_MAX_VRAM=3221225472\"\r\n...\r\nllm_load_tensors: offloading 20 repeating layers to GPU\r\nllm_load_tensors: offloaded 20/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  3647.87 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2171.88 MiB\r\n```\r\n\r\nThis was llama2 which would have normally fit entirely on my GPU.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add ROCm support to linux install script",
            "url": "https://github.com/ollama/ollama/pull/2966",
            "state": "MERGED",
            "createdAt": "2024-03-07T01:10:38Z",
            "mergedAt": "2024-03-15T01:00:16Z",
            "closedAt": "2024-03-15T01:00:16Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 2
            },
            "additions": 43,
            "deletions": 11,
            "body": "Merge after #2885 and the release is out to avoid users with rocm failing to install due to the dependency file not being available yet.\r\n\r\nThis depends on corresponding path changes in PR #3008 \r\n\r\n\r\nPrior to merging this, folks who want to install the pre-release on Radeon systems can use the following:\r\n\r\n```\r\ncurl -fsSL https://raw.githubusercontent.com/dhiltgen/ollama/rocm_install/scripts/install.sh  | OLLAMA_VERSION=\"0.1.29\" sh\r\n```",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix some typos",
            "url": "https://github.com/ollama/ollama/pull/2973",
            "state": "MERGED",
            "createdAt": "2024-03-07T06:42:44Z",
            "mergedAt": "2024-03-07T06:50:12Z",
            "closedAt": "2024-03-07T06:50:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update Go to 1.22 in other places",
            "url": "https://github.com/ollama/ollama/pull/2975",
            "state": "MERGED",
            "createdAt": "2024-03-07T07:18:50Z",
            "mergedAt": "2024-03-07T15:39:49Z",
            "closedAt": "2024-03-07T15:39:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 20,
            "deletions": 21,
            "body": "https://github.com/ollama/ollama/pull/2824 updated Ollama to require Go 1.22, but a few places still use 1.21",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Get term size from stderr and /dev/tty to avoid redirection caused getting size error.",
            "url": "https://github.com/ollama/ollama/pull/2983",
            "state": "CLOSED",
            "createdAt": "2024-03-07T15:57:52Z",
            "mergedAt": null,
            "closedAt": "2024-03-10T15:54:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 38,
            "deletions": 6,
            "body": "As issue #2970 said, `ollama run phi | tee llms/out.txt` leads to error. When using a pipe operator following the `ollama` command the`os.Stdout` points to a pipe instead of the terminal, so `term.Getsize` failed. \r\n\r\nNow the code gets terminal size from stderr first, which enables `ollama run phi | tee llms/out.txt` to execute normally. \r\nIf the user enter `ollama run phi 2>&1 | tee llms/out.txt`, the code then tries to get `/dev/tty` instead. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "remove empty examples",
            "url": "https://github.com/ollama/ollama/pull/2985",
            "state": "MERGED",
            "createdAt": "2024-03-07T18:40:51Z",
            "mergedAt": "2024-03-07T18:49:40Z",
            "closedAt": "2024-03-07T18:49:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 44,
            "body": "resolves #2984 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refined ROCm troubleshooting docs",
            "url": "https://github.com/ollama/ollama/pull/2988",
            "state": "MERGED",
            "createdAt": "2024-03-07T19:24:11Z",
            "mergedAt": "2024-03-08T21:33:30Z",
            "closedAt": "2024-03-08T21:33:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 30,
            "deletions": 24,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: default terminal width, height",
            "url": "https://github.com/ollama/ollama/pull/2990",
            "state": "MERGED",
            "createdAt": "2024-03-07T19:29:12Z",
            "mergedAt": "2024-03-08T23:20:54Z",
            "closedAt": "2024-03-08T23:20:54Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 4,
            "body": "resolves #2970 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix ci",
            "url": "https://github.com/ollama/ollama/pull/2991",
            "state": "MERGED",
            "createdAt": "2024-03-07T19:34:00Z",
            "mergedAt": "2024-03-07T19:35:06Z",
            "closedAt": "2024-03-07T19:35:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "tune concurrency manager",
            "url": "https://github.com/ollama/ollama/pull/2994",
            "state": "CLOSED",
            "createdAt": "2024-03-07T23:27:51Z",
            "mergedAt": null,
            "closedAt": "2024-08-20T20:15:20Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 6,
            "body": "- higher initial concurrency\r\n- lower cooldown after ramping up\r\n- lower threshold for ramp up",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: allow importing a model from name reference",
            "url": "https://github.com/ollama/ollama/pull/3005",
            "state": "MERGED",
            "createdAt": "2024-03-08T17:06:04Z",
            "mergedAt": "2024-03-08T17:27:47Z",
            "closedAt": "2024-03-08T17:27:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "fixes #3003 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Finish unwinding idempotent payload logic",
            "url": "https://github.com/ollama/ollama/pull/3008",
            "state": "MERGED",
            "createdAt": "2024-03-08T17:47:17Z",
            "mergedAt": "2024-03-09T17:13:24Z",
            "closedAt": "2024-03-09T17:13:24Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 6
            },
            "additions": 71,
            "deletions": 87,
            "body": "The recent ROCm change partially removed idempotent payloads, but the ggml-metal.metal file for mac was still idempotent.  This finishes switching to always extract the payloads, and now that idempotentcy is gone, the version directory is no longer useful.\r\n\r\n\r\nTested on mac, linux, and windows.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "decode ggla",
            "url": "https://github.com/ollama/ollama/pull/3014",
            "state": "MERGED",
            "createdAt": "2024-03-08T23:47:08Z",
            "mergedAt": "2024-03-09T00:14:53Z",
            "closedAt": "2024-03-09T00:14:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 172,
            "deletions": 34,
            "body": "split from #2926 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Rename ROCm deps file to avoid confusion",
            "url": "https://github.com/ollama/ollama/pull/3025",
            "state": "MERGED",
            "createdAt": "2024-03-09T19:22:33Z",
            "mergedAt": "2024-03-10T01:48:39Z",
            "closedAt": "2024-03-10T01:48:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Doc how to set up ROCm builds on windows",
            "url": "https://github.com/ollama/ollama/pull/3026",
            "state": "MERGED",
            "createdAt": "2024-03-09T19:30:02Z",
            "mergedAt": "2024-03-09T22:17:20Z",
            "closedAt": "2024-03-09T22:17:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI release process",
            "url": "https://github.com/ollama/ollama/pull/3028",
            "state": "MERGED",
            "createdAt": "2024-03-09T21:21:42Z",
            "mergedAt": "2024-03-15T23:40:54Z",
            "closedAt": "2024-03-15T23:40:54Z",
            "reviews": {
                "totalCount": 19
            },
            "files": {
                "totalCount": 5
            },
            "additions": 601,
            "deletions": 64,
            "body": "This is now fully wired up and tested with tag pushes and completes in ~34 minutes.\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: Add AI telegram to Community Integrations.",
            "url": "https://github.com/ollama/ollama/pull/3033",
            "state": "MERGED",
            "createdAt": "2024-03-10T04:08:51Z",
            "mergedAt": "2024-03-25T18:56:42Z",
            "closedAt": "2024-03-25T18:56:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix paste of text with line feed characters",
            "url": "https://github.com/ollama/ollama/pull/3043",
            "state": "MERGED",
            "createdAt": "2024-03-10T15:48:33Z",
            "mergedAt": "2024-05-07T22:26:07Z",
            "closedAt": "2024-05-07T22:26:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Hey there, thanks for this awesome tool \ud83d\ude4c \r\n\r\nThis is a small patch to fix copy-paste with some terminals that send line feed characters when pasting text with newlines.\r\n\r\nTo test these changes, run ollama in interactive mode on [Kitty](https://github.com/kovidgoyal/kitty) and paste:\r\n```txt\r\ntext\r\non multiple\r\nlines\r\n```\r\n\r\nCuriously this issue doesn't occur on MacOS's default terminal, because it sends CR characters for the newlines. I'm not sure which terminal behavior is the correct one, anyway I figured it would make sense for ollama to support CR and LF characters in the same way.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "convert: fix shape",
            "url": "https://github.com/ollama/ollama/pull/3044",
            "state": "MERGED",
            "createdAt": "2024-03-10T17:44:26Z",
            "mergedAt": "2024-03-11T16:56:57Z",
            "closedAt": "2024-03-11T16:56:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "This commit reverts 18979ad4a1d40d04e3b981a477fa6323a40304b6 which was merged in #3014\r\n\r\n#3014 broke convert by setting dimensions to an array filled with 1s which is incorrect. while this is how the reader works, the writer only writes the array items if it's greater than zero[^1]. filling with 1s incorrectly potentially adds extra dimensions\r\n\r\nthe root cause is the way dimensions are encoded. using a slice is less error prone\r\n\r\n[^1]: https://github.com/ollama/ollama/blob/main/llm/gguf.go#L427",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add ollama executable peer dir for rocm",
            "url": "https://github.com/ollama/ollama/pull/3046",
            "state": "MERGED",
            "createdAt": "2024-03-10T19:16:42Z",
            "mergedAt": "2024-03-10T19:30:56Z",
            "closedAt": "2024-03-10T19:30:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 0,
            "body": "This allows people who package up ollama on their own to place the rocm dependencies in a peer directory to the ollama executable much like our windows install flow.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Harden for deps file being empty (or short)",
            "url": "https://github.com/ollama/ollama/pull/3048",
            "state": "MERGED",
            "createdAt": "2024-03-10T21:46:06Z",
            "mergedAt": "2024-03-10T22:17:23Z",
            "closedAt": "2024-03-10T22:17:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "Breadcrumb....\r\n\r\nRosetta had a bug in 14.3 that causes our build to fail to generate the deps file properly since ldd fails with an error when trying to process the hip library dependency chain `cannot enable executable stack as shared object requires: Invalid argument` ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Avoid rocm runner and dependency clash",
            "url": "https://github.com/ollama/ollama/pull/3056",
            "state": "MERGED",
            "createdAt": "2024-03-11T15:47:46Z",
            "mergedAt": "2024-03-11T16:48:48Z",
            "closedAt": "2024-03-11T16:48:48Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 10,
            "deletions": 6,
            "body": "Putting the rocm symlink next to the runners is risky.  This moves the payloads into a subdir to avoid potential clashes.\r\n\r\nFixes #3035 \r\n\r\nUntested at the moment...",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp submodule to `ceca1ae`",
            "url": "https://github.com/ollama/ollama/pull/3064",
            "state": "MERGED",
            "createdAt": "2024-03-11T19:24:57Z",
            "mergedAt": "2024-03-11T19:57:48Z",
            "closedAt": "2024-03-11T19:57:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 36,
            "deletions": 61,
            "body": "Fixes https://github.com/ollama/ollama/issues/3058",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "relay load model errors to the client",
            "url": "https://github.com/ollama/ollama/pull/3065",
            "state": "MERGED",
            "createdAt": "2024-03-11T20:25:07Z",
            "mergedAt": "2024-03-11T20:48:27Z",
            "closedAt": "2024-03-11T20:48:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 51,
            "deletions": 11,
            "body": "Relay errors on model load, this is needed to help people troubleshoot the specific problem they are experiencing when running a model. This function is a bottle-neck where many different errors can occur. As seen in #2753, there are many issues when the generic \"failed to load model\" error being reported. \r\n\r\nIn order to effectively identify and fix issues we need to relay more information to the client.\r\n\r\nThis change:\r\n- Re-throws load errors, so that the client can output the problem\r\n\r\nThis change does not:\r\n- Provide enhanced information to user's about how to resolve their problem. We will have to add these as we see the issues people face, if they can't be fixed on the Ollama side.\r\n\r\nErrors will be relayed like this:\r\n```\r\nollama run bad-model\r\nError: exception invalid model dimensions\r\n```\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use stdin for term discovery on windows",
            "url": "https://github.com/ollama/ollama/pull/3068",
            "state": "MERGED",
            "createdAt": "2024-03-11T22:28:55Z",
            "mergedAt": "2024-03-14T18:55:19Z",
            "closedAt": "2024-03-14T18:55:19Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 2,
            "body": "When you feed input to the cmd via a pipe it no longer reports a warning\r\n\r\n\r\nBefore:\r\n```\r\n> echo \"what is the captial of australia\" | .\\ollama.exe run phi\r\nfailed to get console mode for stdin: The handle is invalid.\r\n The capital of Australia is Canberra. It's located in the Australian Capital Territory, about 120 kilometers\r\nnorthwest of Sydney and 130 kilometers southwest of Melbourne.\r\n```\r\n\r\nAfter fix:\r\n```\r\n> echo \"what is the captial of australia\" | .\\ollama.exe run phi\r\n The capital city of Australia is Canberra. It is located in the Australian Capital Territory (ACT) and is home to important government buildings, such as Parliament House and the National Museum of Australia\r\n```\r\n\r\n\r\nFixes #2698 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use `-trimpath` when building releases",
            "url": "https://github.com/ollama/ollama/pull/3069",
            "state": "MERGED",
            "createdAt": "2024-03-11T22:56:50Z",
            "mergedAt": "2024-03-11T22:58:47Z",
            "closedAt": "2024-03-11T22:58:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 6,
            "deletions": 6,
            "body": "Fixes https://github.com/ollama/ollama/issues/2958",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add docs explaining GPU selection env vars",
            "url": "https://github.com/ollama/ollama/pull/3070",
            "state": "MERGED",
            "createdAt": "2024-03-11T23:55:08Z",
            "mergedAt": "2024-03-12T18:36:46Z",
            "closedAt": "2024-03-12T18:36:46Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "Fixes #2781 \r\nFixes #2156 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chore: fix typo",
            "url": "https://github.com/ollama/ollama/pull/3073",
            "state": "MERGED",
            "createdAt": "2024-03-12T08:22:16Z",
            "mergedAt": "2024-03-12T18:09:23Z",
            "closedAt": "2024-03-12T18:09:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix gpu_info_cuda.c compile warning",
            "url": "https://github.com/ollama/ollama/pull/3077",
            "state": "MERGED",
            "createdAt": "2024-03-12T12:50:59Z",
            "mergedAt": "2024-03-12T18:08:40Z",
            "closedAt": "2024-03-12T18:08:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "fix compile warning \r\n`gpu_info_cuda.c: In function \u2018cuda_check_vram\u2019:\r\ngpu_info_cuda.c:158:20: warning: format \u2018%ld\u2019 expects argument of type \u2018long int\u2019, but argument 4 has type \u2018long long unsigned int\u2019`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "token repeat limit for prediction requests",
            "url": "https://github.com/ollama/ollama/pull/3080",
            "state": "MERGED",
            "createdAt": "2024-03-12T18:03:51Z",
            "mergedAt": "2024-03-13T02:08:25Z",
            "closedAt": "2024-03-13T02:08:25Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 27,
            "deletions": 7,
            "body": "- abort prediction (generate/chat) requests when a token repeat limit is hit\r\n- this prevent json format infinite loops\r\n- this prevents a stuck request from starving the other queued requests\r\n- move completion cancellation to its own function\r\n\r\nTested with this code from #1910 \r\n```python\r\nimport requests \r\nimport json\r\ncountry = \"france\"\r\nschema = {\r\n\t\"city\": {\r\n\t\t\"type\": \"string\",\r\n\t\t\"description\": \"Name of the city\"\r\n\t},\r\n\t\"lat\":{\r\n\t\t\"type\": \"float\",\r\n\t\t\"description\": \"Decimal Latitude of the city\"\r\n\t},\r\n\t\"lon\":{\r\n\t\t\"type\": \"float\",\r\n\t\t\"description\": \"Decimal Longitude of the city\"\r\n\t}\r\n}\r\npayload = {\r\n\t\"model\": \"mistral-no-repeat\",\r\n\t\"messages\": [\r\n\t\t{\"role\": \"system\", \"content\": f\"You are a helpful AI assistant. The user will enter a country name and the assistant will return the decimal latitude and decimal longitude of the capital of the country. Output in JSON using the schema defined here: {schema}.\"},\r\n\t\t{\"role\": \"user\", \"content\": \"japan\"},\r\n\t\t{\"role\": \"assistant\", \"content\": \"{\\\"city\\\": \\\"Tokyo\\\", \\\"lat\\\": 35.6748, \\\"lon\\\": 139.7624}\"},\r\n\t\t{\"role\": \"user\", \"content\": country},\r\n\t\t],\r\n\t\t\"format\": \"json\",\r\n\t\t\"stream\": False\r\n\t\t\r\n}\r\nresponse = requests.post (\"http://localhost:11434/api/chat\", json=payload)\r\nresponse.raise_for_status()\r\nchat = response.json()\r\ntry:\r\n    message_content_json = json.loads(chat['message']['content'])\r\n    print(message_content_json)\r\nexcept json.JSONDecodeError:\r\n    print(\"JSONDecodeError: The content is not in proper JSON format.\")\r\n```\r\n\r\noutput is more reliable, with occasional JSON format failures which can be handled with a retry:\r\n```bash\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', ' lat ': 48.8566, ' lon': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', 'lat': 48.8566, ' lon ': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', 'lat': 48.8566, ' lon ': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', ' lat': 48.8566, ' lon': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', 'lat': 48.8566, ' lon ': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', ' lat ': 48.8566, ' lon': 2.3522}\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\nJSONDecodeError: The content is not in proper JSON format.\r\nbruce@Bruces-MBP triage % poetry run python3 make_json_request.py\r\n{'city': 'Paris', ' lat ': 48.8534, ' lon': 2.3522}\r\n```\r\n\r\nresolves #1910 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "warn when json format is expected but not mentioned in prompt",
            "url": "https://github.com/ollama/ollama/pull/3081",
            "state": "MERGED",
            "createdAt": "2024-03-12T18:25:59Z",
            "mergedAt": "2024-03-12T23:07:11Z",
            "closedAt": "2024-03-12T23:07:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "This is a complimentary PR to #3080 \r\n\r\nIf `format: json` is specified but the prompt makes no mention of JSON log a warning. This is a lenient, rather than rejecting the request.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor readseeker",
            "url": "https://github.com/ollama/ollama/pull/3083",
            "state": "MERGED",
            "createdAt": "2024-03-12T19:44:28Z",
            "mergedAt": "2024-03-16T19:08:56Z",
            "closedAt": "2024-03-16T19:08:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 72,
            "deletions": 70,
            "body": "no functional change",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update convert",
            "url": "https://github.com/ollama/ollama/pull/3084",
            "state": "CLOSED",
            "createdAt": "2024-03-12T19:48:35Z",
            "mergedAt": null,
            "closedAt": "2024-03-27T21:02:34Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 10
            },
            "additions": 791,
            "deletions": 817,
            "body": "the output of convert remains exactly the same",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Import server.cpp to retain llava support",
            "url": "https://github.com/ollama/ollama/pull/3086",
            "state": "MERGED",
            "createdAt": "2024-03-12T21:21:05Z",
            "mergedAt": "2024-03-15T23:10:35Z",
            "closedAt": "2024-03-15T23:10:35Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 12
            },
            "additions": 43307,
            "deletions": 23,
            "body": "Recent refactoring upstream has temporarily(?) removed llava support from the server.cpp code, which we rely on.  This pulls the server just before that change into our repo so we can keep current with the base llama.cpp code updates until llava support is added back.\r\n\r\nVerified on Mac, Linux and Windows.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add more docs on for the modelfile message command",
            "url": "https://github.com/ollama/ollama/pull/3087",
            "state": "MERGED",
            "createdAt": "2024-03-12T23:36:41Z",
            "mergedAt": "2024-03-12T23:41:41Z",
            "closedAt": "2024-03-12T23:41:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 2,
            "body": "This change adds more documentation for the `MESSAGE` command in Modelfiles.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix iGPU detection for linux",
            "url": "https://github.com/ollama/ollama/pull/3088",
            "state": "MERGED",
            "createdAt": "2024-03-13T00:01:25Z",
            "mergedAt": "2024-03-13T00:20:28Z",
            "closedAt": "2024-03-13T00:20:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 28,
            "deletions": 14,
            "body": "This fixes a few bugs in the new sysfs discovery logic.  iGPUs are now correctly identified by their <1G VRAM reported.  the sysfs IDs are off by one compared to what HIP wants due to the CPU being reported in amdgpu, but HIP only cares about GPUs.\r\n\r\n\r\nTested on a Ryzen 9 7900X system with an RX 7900 XTX.  The amdgpu driver exposes 3 nodes, 0 is CPU, 1 is the discrete GPU, and 2 is the iGPU.  This logic now correctly detects this system and sets the visible devices properly.\r\n\r\nExample scenario 1:\r\n```\r\n% OLLAMA_DEBUG=1 ./ollama-linux-amd64 serve\r\n..\r\ntime=2024-03-13T00:03:55.440Z level=DEBUG source=amd_linux.go:110 msg=\"rocm supported GPU types [gfx1030 gfx1100 gfx1101 gfx1102 gfx900 gfx906 gfx908 gfx90a gfx940 gfx941 gfx942]\"\r\ntime=2024-03-13T00:03:55.440Z level=INFO source=amd_linux.go:119 msg=\"amdgpu [0] gfx1100 is supported\"\r\ntime=2024-03-13T00:03:55.440Z level=WARN source=amd_linux.go:114 msg=\"amdgpu [1] gfx1036 is not supported by /tmp/ollama2436258655/rocm [gfx1030 gfx1100 gfx1101 gfx1102 gfx900 gfx906 gfx908 gfx90a gfx940 gfx941 gfx942]\"\r\ntime=2024-03-13T00:03:55.440Z level=WARN source=amd_linux.go:116 msg=\"See https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md for HSA_OVERRIDE_GFX_VERSION usage\"\r\ntime=2024-03-13T00:03:55.440Z level=DEBUG source=amd_linux.go:152 msg=\"discovering VRAM for amdgpu devices\"\r\ntime=2024-03-13T00:03:55.440Z level=DEBUG source=amd_linux.go:171 msg=\"amdgpu devices [0 1]\"\r\ntime=2024-03-13T00:03:55.441Z level=INFO source=amd_linux.go:246 msg=\"[0] amdgpu totalMemory 24560M\"\r\ntime=2024-03-13T00:03:55.441Z level=INFO source=amd_linux.go:247 msg=\"[0] amdgpu freeMemory  24560M\"\r\ntime=2024-03-13T00:03:55.441Z level=INFO source=amd_common.go:54 msg=\"Setting HIP_VISIBLE_DEVICES=0\"\r\ntime=2024-03-13T00:03:55.441Z level=DEBUG source=gpu.go:180 msg=\"rocm detected 1 devices with 22104M available memory\"\r\n```\r\n\r\nIf I force the override to bypass the compatibility check, the new iGPU detection logic kicks in  (both of these scenarios work)\r\n```\r\n% OLLAMA_DEBUG=1 HSA_OVERRIDE_GFX_VERSION=11.0.0 ./ollama-linux-amd64 serve\r\n...\r\ntime=2024-03-13T00:04:59.799Z level=DEBUG source=amd_linux.go:123 msg=\"skipping rocm gfx compatibility check with HSA_OVERRIDE_GFX_VERSION=11.0.0\"\r\ntime=2024-03-13T00:04:59.799Z level=DEBUG source=amd_linux.go:152 msg=\"discovering VRAM for amdgpu devices\"\r\ntime=2024-03-13T00:04:59.799Z level=DEBUG source=amd_linux.go:171 msg=\"amdgpu devices [0 1]\"\r\ntime=2024-03-13T00:04:59.799Z level=INFO source=amd_linux.go:246 msg=\"[0] amdgpu totalMemory 24560M\"\r\ntime=2024-03-13T00:04:59.799Z level=INFO source=amd_linux.go:247 msg=\"[0] amdgpu freeMemory  24560M\"\r\ntime=2024-03-13T00:04:59.799Z level=INFO source=amd_linux.go:217 msg=\"amdgpu [1] appears to be an iGPU with 512M reported total memory, skipping\"\r\ntime=2024-03-13T00:04:59.799Z level=INFO source=amd_common.go:54 msg=\"Setting HIP_VISIBLE_DEVICES=0\"\r\ntime=2024-03-13T00:04:59.799Z level=DEBUG source=gpu.go:180 msg=\"rocm detected 1 devices with 22104M available memory\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Limit `num_predict` to `num_ctx`",
            "url": "https://github.com/ollama/ollama/pull/3092",
            "state": "CLOSED",
            "createdAt": "2024-03-13T04:59:30Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T06:09:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 0,
            "body": "This limits the number of tokens generated to the context window size, allowing a maximum of two \"context shifts\" should the context window limit be passed. It will also provide a maximum token limit to stop any \"runaway\" prompts that occur from smaller models that continue to generate indefinitely (e.g. in JSON mode)\r\n\r\nIdeally we have no context shifts: we only generate the number of tokens left in the context window after the prompt, but this felt like a simple change that would ease our way into this since chat prompts can get quite large and we'd have to change our prompt trimming strategy to leave enough space for longer responses.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Default Keep Alive environment variable",
            "url": "https://github.com/ollama/ollama/pull/3094",
            "state": "MERGED",
            "createdAt": "2024-03-13T05:11:04Z",
            "mergedAt": "2024-03-13T20:29:40Z",
            "closedAt": "2024-03-13T20:29:40Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 81,
            "deletions": 3,
            "body": "This change adds a new environment variable called `OLLAMA_KEEP_ALIVE` which sets how long a model will be loaded into memory. It uses the same semantics as the `keep_alive` parameter in the `generate`, `chat`, and `embeddings` API calls, namely:\r\n\r\n* if set to a positive value, it will default to whatever time was set\r\n* if set to zero it will unload immediately after generation\r\n* if set to a negative value it will remain in memory\r\n\r\nYou can either use a value in seconds (e.g. `OLLAMA_KEEP_ALIVE=60` for 60 seconds), or as a duration string (e.g. `OLLAMA_KEEP_ALIVE=10m`).\r\n\r\nThis change works with both the API, and with the REPL.\r\n\r\nFixes #2508 \r\n",
            "participants": {
                "totalCount": 8
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Improve usability with Bash completion for Ollama on Linux",
            "url": "https://github.com/ollama/ollama/pull/3105",
            "state": "CLOSED",
            "createdAt": "2024-03-13T14:40:44Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T18:58:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 89,
            "deletions": 0,
            "body": "Please review this PR for adding Bash completion to install.sh for Linux.\r\n\r\nCurrently works for all arguments and options, including autocomplete for long model names (yay!)\r\n\r\n`ollama ls` works, but it's missing from the -h/--help and ollama listing. I'll open a separate issue for it.\r\n\r\nIt does not include  autocomplete for `ollama rm` as I consider easy destructive actions would need user confirmation before execution. I'll also create an issue for it.\r\n\r\nThis version of install.sh has been tested successfully on Fedora 39, Ubuntu 23.10 and Debian 12.5. \r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Update ollama.iss",
            "url": "https://github.com/ollama/ollama/pull/3111",
            "state": "MERGED",
            "createdAt": "2024-03-13T15:23:08Z",
            "mergedAt": "2024-03-15T23:47:00Z",
            "closedAt": "2024-03-15T23:47:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "add arm64 support",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "simplify parsing safetensor file",
            "url": "https://github.com/ollama/ollama/pull/3121",
            "state": "CLOSED",
            "createdAt": "2024-03-13T18:44:49Z",
            "mergedAt": null,
            "closedAt": "2024-04-10T19:45:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 13
            },
            "additions": 910,
            "deletions": 932,
            "body": "remove the indirection in parsing the safetensor head json",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Better tmpdir cleanup",
            "url": "https://github.com/ollama/ollama/pull/3122",
            "state": "MERGED",
            "createdAt": "2024-03-13T18:53:51Z",
            "mergedAt": "2024-03-20T15:28:03Z",
            "closedAt": "2024-03-20T15:28:03Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 52,
            "deletions": 1,
            "body": "If expanding the runners fails, don't leave a corrupt/incomplete payloads dir. We now write a pid file out to the tmpdir, which allows us to scan for stale tmpdirs and remove this as long as there isn't still a process running.\r\n\r\nFixes #3051 \r\nFixes #2472 \r\nFixes #2658  (indirectly)\r\n\r\nVerified on Mac, Linux and Windows.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add `OLLAMA_KEEP_ALIVE` to environment variable docs for `ollama serve`",
            "url": "https://github.com/ollama/ollama/pull/3127",
            "state": "MERGED",
            "createdAt": "2024-03-13T21:33:57Z",
            "mergedAt": "2024-03-13T21:35:33Z",
            "closedAt": "2024-03-13T21:35:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: pbcopy on mac",
            "url": "https://github.com/ollama/ollama/pull/3129",
            "state": "MERGED",
            "createdAt": "2024-03-14T00:29:06Z",
            "mergedAt": "2024-05-06T20:47:00Z",
            "closedAt": "2024-05-06T20:47:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Hey!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "doc: faq gpu compatibility",
            "url": "https://github.com/ollama/ollama/pull/3142",
            "state": "MERGED",
            "createdAt": "2024-03-14T14:47:11Z",
            "mergedAt": "2024-03-21T09:21:34Z",
            "closedAt": "2024-03-21T09:21:34Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 79,
            "deletions": 34,
            "body": "Add some information about GPU compatibility to the FAQs.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": ".github: add issue templates",
            "url": "https://github.com/ollama/ollama/pull/3143",
            "state": "MERGED",
            "createdAt": "2024-03-14T16:31:28Z",
            "mergedAt": "2024-03-14T22:19:11Z",
            "closedAt": "2024-03-14T22:19:11Z",
            "reviews": {
                "totalCount": 20
            },
            "files": {
                "totalCount": 4
            },
            "additions": 194,
            "deletions": 0,
            "body": "You can view a functional test version of this PR here: https://github.com/bmizerany/ollama-test-issues-tempates/issues/new/choose",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "server: replace blob prefix separator from ':' to '-'",
            "url": "https://github.com/ollama/ollama/pull/3146",
            "state": "MERGED",
            "createdAt": "2024-03-14T18:32:54Z",
            "mergedAt": "2024-03-15T03:18:06Z",
            "closedAt": "2024-03-15T03:18:06Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 120,
            "deletions": 13,
            "body": "This fixes issues with blob file names that contain ':' characters to be\r\n9 rejected by file systems that do not support them.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: support wide characters in lib path",
            "url": "https://github.com/ollama/ollama/pull/3148",
            "state": "CLOSED",
            "createdAt": "2024-03-14T19:46:53Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T16:50:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 9,
            "body": "Previous behavior:\r\n```\r\nError: Unable to load dynamic library: Unable to load dynamic server library: \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \u00e3\ufffd\ufffd \ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\u03f4\ufffd.\r\n```\r\n\r\nWhen the user's home path contains unicode characters on Windows the packaged runtime libraries failed to open, add support for wide characters to fix this.\r\n\r\nresolves #2615 \r\nresolces #3367 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: clip memory leak",
            "url": "https://github.com/ollama/ollama/pull/3149",
            "state": "MERGED",
            "createdAt": "2024-03-14T19:47:41Z",
            "mergedAt": "2024-03-14T20:34:15Z",
            "closedAt": "2024-03-14T20:34:15Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 45,
            "deletions": 0,
            "body": "this change patches llama.cpp and fixes two bugs\r\n\r\n1. llama_server_context never calls clip_free\r\n2. clip_free does not fully free its context",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm,readline: use errors.Is instead of simple == check",
            "url": "https://github.com/ollama/ollama/pull/3161",
            "state": "MERGED",
            "createdAt": "2024-03-15T05:54:24Z",
            "mergedAt": "2024-03-15T14:14:12Z",
            "closedAt": "2024-03-15T14:14:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 3,
            "body": "This fixes some brittle, simple equality checks to use errors.Is. Since go1.13, errors.Is is the idiomatic way to check for errors.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: timeout between token generation",
            "url": "https://github.com/ollama/ollama/pull/3169",
            "state": "CLOSED",
            "createdAt": "2024-03-15T16:22:28Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T18:19:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 72,
            "deletions": 44,
            "body": "- if 30 seconds pass since the last token generation abort the request\r\n- stop the llama thread to flush any accumulated context\r\n\r\nThis is an attempt to mitigate server hangs as seen in #2805. It is not a complete solution since we still need to address the root cause of the hangs, but it will make them recoverable.\r\n\r\nTODO:\r\n- [ ] reproduce hang and validate this allows the server to recover",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Radeon gfx940-942 GPU support",
            "url": "https://github.com/ollama/ollama/pull/3171",
            "state": "MERGED",
            "createdAt": "2024-03-15T22:40:56Z",
            "mergedAt": "2024-03-15T22:58:34Z",
            "closedAt": "2024-03-15T22:58:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 0,
            "body": "Looking at the Tensor files in ROCm v6, I noticed these newer gfx types were present, but we're not compiling them in.  (e.g. this would cover the new MI300X GPU)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: remove server static assets",
            "url": "https://github.com/ollama/ollama/pull/3174",
            "state": "MERGED",
            "createdAt": "2024-03-16T00:42:03Z",
            "mergedAt": "2024-03-16T02:24:12Z",
            "closedAt": "2024-03-16T02:24:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 3,
            "deletions": 6014,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Saddle",
            "url": "https://github.com/ollama/ollama/pull/3178",
            "state": "MERGED",
            "createdAt": "2024-03-16T06:55:03Z",
            "mergedAt": "2024-03-25T18:54:09Z",
            "closedAt": "2024-03-25T18:54:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Another simple, no build, no setup web interface.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove global",
            "url": "https://github.com/ollama/ollama/pull/3217",
            "state": "MERGED",
            "createdAt": "2024-03-18T08:49:22Z",
            "mergedAt": "2024-03-18T09:13:30Z",
            "closedAt": "2024-03-18T09:13:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 5,
            "body": "the global isn't being used",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Switch back to subprocessing for llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/3218",
            "state": "MERGED",
            "createdAt": "2024-03-18T09:28:02Z",
            "mergedAt": "2024-04-02T17:49:44Z",
            "closedAt": "2024-04-02T17:49:44Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 43
            },
            "additions": 1511,
            "deletions": 1946,
            "body": "This should resolve a number of memory leak and stability defects by allowing us to isolate llama.cpp in a separate process and shutdown when idle, and gracefully restart if it has problems.  This also serves as a first step to be able to run multiple copies to support multiple models concurrently.\r\n\r\nTested on Windows, Linux, Mac.  Nvidia/AMD, and simulated a number of different failure modes to ensure it detected the runner not responding and restarted it on next request.\r\n\r\nFixes #1691 \r\nFixes #1848 \r\nFixes #1871\r\nFixes #2767 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "update memory estimations for gpu offloading",
            "url": "https://github.com/ollama/ollama/pull/3241",
            "state": "MERGED",
            "createdAt": "2024-03-19T09:30:00Z",
            "mergedAt": "2024-04-01T20:59:14Z",
            "closedAt": "2024-04-01T20:59:14Z",
            "reviews": {
                "totalCount": 22
            },
            "files": {
                "totalCount": 7
            },
            "additions": 121,
            "deletions": 85,
            "body": "take into account memory footprint of each layer\r\n\r\n1. replace percentage overhead with static overhead of 377 MiB for cuda and rocm\r\n2. add projector memory footprint to estimation\r\n3. add layer footprint to estimation on a per-layer basis, including output layers\r\n4. replace static kv memory footprint with pro-rated footprint based on how many layers to offload\r\n5. set minimum context length for multimodal models to 2048\r\n6. report memory requirements as structured logs",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add gemma safetensors conversion",
            "url": "https://github.com/ollama/ollama/pull/3250",
            "state": "MERGED",
            "createdAt": "2024-03-19T14:54:17Z",
            "mergedAt": "2024-03-29T01:54:01Z",
            "closedAt": "2024-03-29T01:54:01Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 11
            },
            "additions": 941,
            "deletions": 825,
            "body": "This change adds the ability to convert from a Gemma safetensors model.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "Community Integration: tlm - cli copilot with ollama",
            "url": "https://github.com/ollama/ollama/pull/3274",
            "state": "MERGED",
            "createdAt": "2024-03-20T17:58:25Z",
            "mergedAt": "2024-03-25T18:53:26Z",
            "closedAt": "2024-03-25T18:53:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I have been advised to create a PR to include [tlm](https://github.com/yusufcanb/tlm) inside [README.md](https://github.com/ollama/ollama/blob/main/README.md) during the KubeCon 2024 Paris by the Ollama staff. Thanks for all of them who expressed their excitement for what I've created. \u2764\r\n\r\nSo, here is the PR to include [tlm](https://github.com/yusufcanb/tlm)  in Terminal section. Thanks again for the advise.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Enabling ollama to run on Intel GPUs with SYCL backend",
            "url": "https://github.com/ollama/ollama/pull/3278",
            "state": "MERGED",
            "createdAt": "2024-03-21T05:44:14Z",
            "mergedAt": "2024-05-28T23:30:50Z",
            "closedAt": "2024-05-28T23:30:50Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 7
            },
            "additions": 614,
            "deletions": 31,
            "body": "Hi, I am submitting this pr to enable ollama to run on Intel GPUs with SYCL as the backend. This pr was [originally](https://github.com/ollama/ollama/pull/2458) started by @felipeagc who is currently unable to actively participate due to relocation.\r\nThe original pr had fallen behind the main branch, making it inconvenient for maintainers @mxyng @jmorganca @dhiltgen to review. Therefore, I rebased the latest main branch and opened this new pull request. I have verified that it works correctly on Ubuntu 22.04 with ARC 770 GPU.\r\nWhile I am not very familiar with this project and I welcome any guidance and assistance from the community. Let\u2019s work together to make ollama support Intel GPU platforms. cc:@hshen14 @kevinintel @airmeng\r\n\r\nUPDATE: works well on windows10 + ARC 770\r\nUPDATE: works well on oneapi-docker-image(oneapi-basekit-Ubuntu22.04) + ARC770  ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 10
            }
        }
    },
    {
        "node": {
            "title": "Add docs for GPU selection and nvidia uvm workaround",
            "url": "https://github.com/ollama/ollama/pull/3282",
            "state": "MERGED",
            "createdAt": "2024-03-21T10:20:29Z",
            "mergedAt": "2024-03-24T18:15:04Z",
            "closedAt": "2024-03-24T18:15:04Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 2
            },
            "additions": 28,
            "deletions": 10,
            "body": "Fixes #1813 \r\nFixes #2934 \r\nFixes #2718 ",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add MarshalJSON to Duration",
            "url": "https://github.com/ollama/ollama/pull/3284",
            "state": "MERGED",
            "createdAt": "2024-03-21T11:57:51Z",
            "mergedAt": "2024-05-06T22:59:18Z",
            "closedAt": "2024-05-06T22:59:18Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 67,
            "deletions": 1,
            "body": "fix #3283 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/3288",
            "state": "MERGED",
            "createdAt": "2024-03-21T17:15:44Z",
            "mergedAt": "2024-03-25T18:52:25Z",
            "closedAt": "2024-03-25T18:52:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Added Ollama Basic chat based on hyperdiv",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Testcontainers into Libraries section",
            "url": "https://github.com/ollama/ollama/pull/3291",
            "state": "MERGED",
            "createdAt": "2024-03-22T04:53:32Z",
            "mergedAt": "2024-03-23T18:55:25Z",
            "closedAt": "2024-03-23T18:55:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Testcontainers provides a module for Ollama.\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b2510",
            "url": "https://github.com/ollama/ollama/pull/3308",
            "state": "MERGED",
            "createdAt": "2024-03-23T11:24:46Z",
            "mergedAt": "2024-03-25T19:56:12Z",
            "closedAt": "2024-03-25T19:56:12Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 7,
            "deletions": 184,
            "body": "~latest release, after the cuda refactoring",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revamp go based integration tests",
            "url": "https://github.com/ollama/ollama/pull/3309",
            "state": "MERGED",
            "createdAt": "2024-03-23T13:36:20Z",
            "mergedAt": "2024-03-23T18:08:49Z",
            "closedAt": "2024-03-23T18:08:49Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 8
            },
            "additions": 313,
            "deletions": 261,
            "body": "This uplevels the integration tests to run the server which can allow testing an existing server, or a remote server.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: prevent race appending to slice",
            "url": "https://github.com/ollama/ollama/pull/3320",
            "state": "MERGED",
            "createdAt": "2024-03-24T03:48:22Z",
            "mergedAt": "2024-03-24T18:35:55Z",
            "closedAt": "2024-03-24T18:35:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 2,
            "body": "llm: prevent race appending to slice        \r\n\r\nPreviously, multiple goroutines were appending to the same unguarded\r\nslice.\r\n\r\nAlso, convert slice declaration to idiomatic zero value form.\r\n\r\nAlso, convert errgroup.Group declaration to idiomatic zero value form.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Integration tests conditionally pull",
            "url": "https://github.com/ollama/ollama/pull/3331",
            "state": "MERGED",
            "createdAt": "2024-03-24T23:44:05Z",
            "mergedAt": "2024-03-25T19:48:52Z",
            "closedAt": "2024-03-25T19:48:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 70,
            "deletions": 10,
            "body": "If images aren't present, pull them.\r\nAlso fixes the expected responses",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "doc: specify ADAPTER is optional",
            "url": "https://github.com/ollama/ollama/pull/3333",
            "state": "MERGED",
            "createdAt": "2024-03-25T03:53:16Z",
            "mergedAt": "2024-03-25T16:43:19Z",
            "closedAt": "2024-03-25T16:43:19Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/3338",
            "state": "MERGED",
            "createdAt": "2024-03-25T08:19:05Z",
            "mergedAt": "2024-03-25T18:50:51Z",
            "closedAt": "2024-03-25T18:50:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "adding drazdra/ollama-chats to the list of UI :)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b2581",
            "url": "https://github.com/ollama/ollama/pull/3343",
            "state": "MERGED",
            "createdAt": "2024-03-25T13:37:08Z",
            "mergedAt": "2024-04-02T22:08:26Z",
            "closedAt": "2024-04-02T22:08:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 15,
            "body": "Verified on all platforms",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "change `github.com/jmorganca/ollama` to `github.com/ollama/ollama`",
            "url": "https://github.com/ollama/ollama/pull/3347",
            "state": "MERGED",
            "createdAt": "2024-03-25T20:35:34Z",
            "mergedAt": "2024-03-26T20:04:17Z",
            "closedAt": "2024-03-26T20:04:17Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 54
            },
            "additions": 115,
            "deletions": 115,
            "body": "This huge patch bomb just migrates from `github.com/jmorganca/ollama` to `github.com/ollama/ollama`.\r\n\r\nFixes #3327 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b2527",
            "url": "https://github.com/ollama/ollama/pull/3348",
            "state": "MERGED",
            "createdAt": "2024-03-25T20:48:19Z",
            "mergedAt": "2024-03-25T21:15:53Z",
            "closedAt": "2024-03-25T21:15:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove need for `$VSINSTALLDIR` since build will fail if `ninja.exe` cannot be found",
            "url": "https://github.com/ollama/ollama/pull/3350",
            "state": "MERGED",
            "createdAt": "2024-03-25T21:59:50Z",
            "mergedAt": "2024-03-26T20:23:16Z",
            "closedAt": "2024-03-26T20:23:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 15,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add license in file header for vendored llama.cpp code",
            "url": "https://github.com/ollama/ollama/pull/3351",
            "state": "MERGED",
            "createdAt": "2024-03-25T22:02:20Z",
            "mergedAt": "2024-03-26T20:23:23Z",
            "closedAt": "2024-03-26T20:23:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 44,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Switch arm cuda base image to centos 7",
            "url": "https://github.com/ollama/ollama/pull/3352",
            "state": "MERGED",
            "createdAt": "2024-03-25T22:56:29Z",
            "mergedAt": "2024-03-25T23:13:10Z",
            "closedAt": "2024-03-25T23:13:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "We had started using rocky linux 8, but they've updated to GCC 10.3, which breaks NVCC.  10.2 is compatible (or 10.4, but that's not available from rocky linux 8 repos yet)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use Rocky Linux Vault to get GCC 10.2 installed",
            "url": "https://github.com/ollama/ollama/pull/3353",
            "state": "MERGED",
            "createdAt": "2024-03-26T02:20:02Z",
            "mergedAt": "2024-03-26T02:38:56Z",
            "closedAt": "2024-03-26T02:38:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "malformed markdown link",
            "url": "https://github.com/ollama/ollama/pull/3358",
            "state": "MERGED",
            "createdAt": "2024-03-26T09:04:44Z",
            "mergedAt": "2024-03-26T14:46:36Z",
            "closedAt": "2024-03-26T14:46:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Just a quick PR for malformed link on the README I noticed.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect arrow keys on windows",
            "url": "https://github.com/ollama/ollama/pull/3363",
            "state": "MERGED",
            "createdAt": "2024-03-26T21:45:04Z",
            "mergedAt": "2024-03-26T22:21:56Z",
            "closedAt": "2024-03-26T22:21:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 39,
            "body": "Also simplifies to use the `golang.org/x/sys/windows` package. Note: this could be simplified more using the `x/term` package as well on the unix side of things, but I kept this small to fix windows first\r\n\r\nFixes https://github.com/ollama/ollama/issues/2639",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update modelfile readme",
            "url": "https://github.com/ollama/ollama/pull/3371",
            "state": "CLOSED",
            "createdAt": "2024-03-27T14:37:07Z",
            "mergedAt": null,
            "closedAt": "2024-05-03T20:09:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "only generate cuda/rocm when changes to llm detected",
            "url": "https://github.com/ollama/ollama/pull/3375",
            "state": "MERGED",
            "createdAt": "2024-03-27T19:09:41Z",
            "mergedAt": "2024-03-27T19:40:55Z",
            "closedAt": "2024-03-27T19:40:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 22,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "only generate on changes to llm subdirectory",
            "url": "https://github.com/ollama/ollama/pull/3376",
            "state": "MERGED",
            "createdAt": "2024-03-27T19:45:40Z",
            "mergedAt": "2024-03-27T21:12:53Z",
            "closedAt": "2024-03-27T21:12:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 36,
            "deletions": 13,
            "body": "follow up to #3375 to also skip generate (linux, macos, windows) if there's no changes to the llm subdirectory",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump ROCm to 6.0.2 patch release",
            "url": "https://github.com/ollama/ollama/pull/3377",
            "state": "MERGED",
            "createdAt": "2024-03-27T21:32:27Z",
            "mergedAt": "2024-03-28T23:07:54Z",
            "closedAt": "2024-03-28T23:07:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "Fixes #2455 \r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/3378",
            "state": "MERGED",
            "createdAt": "2024-03-27T22:16:56Z",
            "mergedAt": "2024-03-31T17:10:05Z",
            "closedAt": "2024-03-31T17:10:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Plugins list updated",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: trim quotes on OLLAMA_ORIGINS",
            "url": "https://github.com/ollama/ollama/pull/3379",
            "state": "MERGED",
            "createdAt": "2024-03-27T22:27:04Z",
            "mergedAt": "2024-03-28T21:14:18Z",
            "closedAt": "2024-03-28T21:14:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 6,
            "body": "resolves #3365 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: workflows",
            "url": "https://github.com/ollama/ollama/pull/3380",
            "state": "MERGED",
            "createdAt": "2024-03-27T23:14:35Z",
            "mergedAt": "2024-03-27T23:35:27Z",
            "closedAt": "2024-03-27T23:35:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Security fix: examples/langchain-python-rag-privategpt/requirements.txt to reduce vulnerabilities",
            "url": "https://github.com/ollama/ollama/pull/3382",
            "state": "MERGED",
            "createdAt": "2024-03-28T05:04:47Z",
            "mergedAt": "2024-06-09T17:58:09Z",
            "closedAt": "2024-06-09T17:58:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "The following vulnerabilities are fixed by pinning transitive dependencies:\r\n- https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321964\r\n- https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321966\r\n- https://snyk.io/vuln/SNYK-PYTHON-NUMPY-2321970\r\n\r\n@[snyk-bot](https://github.com/ollama/ollama/commits?author=snyk-bot)\r\nsnyk-bot committed 3 days ago",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update troubleshooting link",
            "url": "https://github.com/ollama/ollama/pull/3391",
            "state": "MERGED",
            "createdAt": "2024-03-28T19:05:29Z",
            "mergedAt": "2024-03-28T20:15:57Z",
            "closedAt": "2024-03-28T20:15:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI windows gpu builds",
            "url": "https://github.com/ollama/ollama/pull/3392",
            "state": "MERGED",
            "createdAt": "2024-03-28T19:08:23Z",
            "mergedAt": "2024-03-28T23:03:52Z",
            "closedAt": "2024-03-28T23:03:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 96,
            "deletions": 8,
            "body": "Changes in the llm dir (e.g. bumping llama.cpp) can result in regressions on windows.  This will help us catch build failures in CI.\r\n\r\n\r\nThis also moves to a hand-crafted cuda install for windows instead of the action we were using that turned out to be buggy.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "CI automation for tagging latest images",
            "url": "https://github.com/ollama/ollama/pull/3398",
            "state": "MERGED",
            "createdAt": "2024-03-28T22:49:53Z",
            "mergedAt": "2024-03-28T23:25:54Z",
            "closedAt": "2024-03-28T23:25:54Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 69,
            "deletions": 12,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Community Integration: ChatOllama",
            "url": "https://github.com/ollama/ollama/pull/3400",
            "state": "MERGED",
            "createdAt": "2024-03-29T00:02:53Z",
            "mergedAt": "2024-03-31T02:46:50Z",
            "closedAt": "2024-03-31T02:46:50Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "# Community Integration - ChatOllama\r\n\r\n[ChatOllama](https://github.com/sugarforever/chat-ollama) is an open source chatbot based on LLMs. It supports a wide range of language models including:\r\n\r\n- Ollama served models\r\n- OpenAI\r\n- Azure OpenAI\r\n- Anthropic\r\n\r\nChatOllama supports multiple types of chat:\r\n\r\n- Free chat with LLMs\r\n- Chat with LLMs based on knowledge base\r\n\r\nChatOllama feature list:\r\n- Ollama models management\r\n- Knowledge bases management\r\n- Chat\r\n- Commercial LLMs API keys management",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add 'Knowledge Cutoff' column to model library table",
            "url": "https://github.com/ollama/ollama/pull/3414",
            "state": "CLOSED",
            "createdAt": "2024-03-30T10:27:21Z",
            "mergedAt": null,
            "closedAt": "2024-03-31T17:11:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 17,
            "body": "resolves #3412 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Request and model concurrency",
            "url": "https://github.com/ollama/ollama/pull/3418",
            "state": "MERGED",
            "createdAt": "2024-03-30T16:56:41Z",
            "mergedAt": "2024-04-23T15:31:38Z",
            "closedAt": "2024-04-23T15:31:38Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 30
            },
            "additions": 2591,
            "deletions": 1363,
            "body": "This change adds support for multiple concurrent requests, as well as loading multiple models by spawning multiple runners.  This change is designed to be \"opt in\" initially, so the default behavior mimics the current sequential implementation (1 request at a time, and only a single model), but can be changed by setting environment variables for the server.  In the future we will adjust the default settings to enable concurrency by default.\r\n\r\nBy default, this change supports 1 concurrent requests to a loaded model, which can be adjusted via `OLLAMA_NUM_PARALLEL`.\r\n\r\nBy default, this change supports 1 loaded model at a time, which can be adjusted via `OLLAMA_MAX_LOADED_MODELS `.  Set to zero for fully dynamic based on VRAM capacity, or a fixed number greater than 1 to limit the total number of loaded models regardless of VRAM capacity.  In the >1 scenario, we'll still perform VRAM prediction calculations to see if the new model will completely fit into available VRAM, but will prevent loading more than the specified number of runners even if they would have fit.  \r\n\r\nThis change also adjusts our GPU selection algorithm in multi-GPU scenarios.  If we can fit the model into a single GPU, we'll favor that over spreading it to multiple GPUs.\r\n\r\nNote: system memory capacity is not taken into consideration in this change, so for CPU mode, setting max runners to zero could result in memory exhaustion and paging/swapping.\r\n\r\nFixes #2109\r\nFixes #1656\r\nFixes #1514\r\nFixes #3304 \r\nFixes #961 \r\nFixes #3507 ",
            "participants": {
                "totalCount": 17
            },
            "comments": {
                "totalCount": 44
            }
        }
    },
    {
        "node": {
            "title": "Simplify model conversion",
            "url": "https://github.com/ollama/ollama/pull/3422",
            "state": "MERGED",
            "createdAt": "2024-03-31T01:23:36Z",
            "mergedAt": "2024-04-01T23:14:53Z",
            "closedAt": "2024-04-01T23:14:54Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 4
            },
            "additions": 364,
            "deletions": 249,
            "body": "This change splits up the gemma/mistral conversion logic into their own files and creates a new ModelArch interface which any new converter can implement to support different model types.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Community Integration: CRAG Ollama Chat",
            "url": "https://github.com/ollama/ollama/pull/3423",
            "state": "MERGED",
            "createdAt": "2024-03-31T09:48:09Z",
            "mergedAt": "2024-04-01T15:16:14Z",
            "closedAt": "2024-04-01T15:16:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Corrective Retrieval Augmented Generation Demo, powered by Langgraph and Streamlit \ud83e\udd17\r\n\r\nSupports: \r\n\r\n- Ollama\r\n- OpenAI APIs",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/3436",
            "state": "MERGED",
            "createdAt": "2024-04-01T10:49:28Z",
            "mergedAt": "2024-04-01T15:16:31Z",
            "closedAt": "2024-04-01T15:16:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Just added lollms-webui to the list of supported webuis.\r\nLollms is a webui that can perform a large range of tasks, from generating text and chatting with more than 500 agents to generating images, music and videos. Lollms supports multimodality and can use it along with ollama. it can also offer RAG and summary services. All running locally and for free.\r\nFor more about lollms, you can check out its website :\r\n[https://lollms.com/](https://lollms.com/)\r\nor the github\r\n[https://github.com/ParisNeo/lollms-webui](https://github.com/ParisNeo/lollms-webui)\r\nor my youtube channel:\r\nhttps://www.youtube.com/@Parisneo\r\n\r\nThank you very much for this wonderful backend.\r\nI'll make a video about how to install lollms along with ollama.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add chromem-go to community integrations",
            "url": "https://github.com/ollama/ollama/pull/3437",
            "state": "MERGED",
            "createdAt": "2024-04-01T12:54:08Z",
            "mergedAt": "2024-04-01T15:17:37Z",
            "closedAt": "2024-04-01T15:17:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hello :wave: , I've been a happy Ollama user for a while (both on macOS and Linux), thank you so much for creating and maintaining this project! I recommend it when talking to other people about running LLMs locally.\r\n\r\nI recently worked on a Go library for a simple embedded vector DB, to be able to write RAG applications without having to host a separate vector DB server (like you have to with most of them, like Qdrant and Milvus). Chroma is embeddable in Python, Weaviate in Python and JS/TS, but not in Go.\r\n\r\nI had locally running embedding models and LLMs in mind from the beginning, and the repo includes a detailed example of using Ollama with the `nomic-embed-text` and `gemma:2b` models for a simple RAG app.\r\n\r\n- Library: https://github.com/philippgille/chromem-go\r\n- Ollama integration: https://github.com/philippgille/chromem-go/blob/v0.5.0/embed_ollama.go\r\n- RAG example using Ollama: https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama\r\n\r\nI would be very happy if the project could be included in the list of community integrations!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix generate output",
            "url": "https://github.com/ollama/ollama/pull/3442",
            "state": "MERGED",
            "createdAt": "2024-04-01T20:47:47Z",
            "mergedAt": "2024-04-01T20:56:09Z",
            "closedAt": "2024-04-01T20:56:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: add OLLAMA_DEBUG in ollama serve help message",
            "url": "https://github.com/ollama/ollama/pull/3461",
            "state": "MERGED",
            "createdAt": "2024-04-02T17:22:52Z",
            "mergedAt": "2024-04-03T01:20:03Z",
            "closedAt": "2024-04-03T01:20:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Hello,\r\n added the OLLAMA_DEBUG flag in the ollama serve help message.\r\n This solves this issue: https://github.com/ollama/ollama/issues/3401",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update graph size estimate",
            "url": "https://github.com/ollama/ollama/pull/3463",
            "state": "MERGED",
            "createdAt": "2024-04-02T22:12:34Z",
            "mergedAt": "2024-04-03T21:27:30Z",
            "closedAt": "2024-04-03T21:27:30Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 52,
            "deletions": 4,
            "body": "precise scratch space estimates for gemma and llama models with other models falling back to the previous algorithm",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix numgpu opt miscomparison",
            "url": "https://github.com/ollama/ollama/pull/3464",
            "state": "MERGED",
            "createdAt": "2024-04-02T22:58:56Z",
            "mergedAt": "2024-04-03T03:10:17Z",
            "closedAt": "2024-04-03T03:10:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 7,
            "body": "opts are now a pointer which means we incorrectly reloaded the model when the actual layers loaded didn't match the input request",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix metal gpu",
            "url": "https://github.com/ollama/ollama/pull/3465",
            "state": "MERGED",
            "createdAt": "2024-04-02T23:09:20Z",
            "mergedAt": "2024-04-02T23:29:59Z",
            "closedAt": "2024-04-02T23:29:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "this includes two fixes for macos\r\n\r\n1. in the rare case a model's minimal memory does not fit into gpu, it will fallback to `cpu` which doesn't exist for metal devices\r\n2. num_gpu 0 should be passed to the subprocess since it's a completely valid configuration",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "default head_kv to 1",
            "url": "https://github.com/ollama/ollama/pull/3466",
            "state": "MERGED",
            "createdAt": "2024-04-02T23:38:44Z",
            "mergedAt": "2024-04-03T17:41:00Z",
            "closedAt": "2024-04-03T17:41:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 6,
            "body": "some older models do not set this kv resulting in the kv size estimate to be 0",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix macOS builds on older SDKs",
            "url": "https://github.com/ollama/ollama/pull/3467",
            "state": "MERGED",
            "createdAt": "2024-04-03T00:21:37Z",
            "mergedAt": "2024-04-03T17:45:54Z",
            "closedAt": "2024-04-03T17:45:54Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 28,
            "deletions": 33,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: provide feedback if OLLAMA_MODELS is set on `run` command",
            "url": "https://github.com/ollama/ollama/pull/3470",
            "state": "MERGED",
            "createdAt": "2024-04-03T03:50:34Z",
            "mergedAt": "2024-04-03T05:11:13Z",
            "closedAt": "2024-04-03T05:11:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 5,
            "body": "This also moves the checkServerHeartbeat call out of the \"RunE\" Cobra stuff (that's the only word I have for that) to on-site where it's after the check for OLLAMA_MODELS, which allows the helpful error message to be printed before the server heartbeat check. This also arguably makes the code more readable without the magic/superfluous \"pre\" function caller.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add BrainSoup to compatible clients list",
            "url": "https://github.com/ollama/ollama/pull/3473",
            "state": "MERGED",
            "createdAt": "2024-04-03T09:41:58Z",
            "mergedAt": "2024-05-06T20:42:16Z",
            "closedAt": "2024-05-06T20:42:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 8,
            "body": "Hi there,\r\n\r\nBrainSoup is a native multi-LLM client for Windows with advanced features such as local document indexing, RAG, multi-modality, multi-agent automation, code interpreter, sandboxed file system and the ability for agents to interact with the local system via customizable events and tools. More information can be found [here](https://www.nurgo-software.com/products/brainsoup).\r\n\r\nThe latest version is know compatible with Ollama, making BrainSoup an excellent choice for users looking to leverage Ollama's LLMs to their full potential.\r\n\r\nI kindly ask for your review of this PR. Thanks for creating such an awesome platform!",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "refactor tensor query",
            "url": "https://github.com/ollama/ollama/pull/3478",
            "state": "MERGED",
            "createdAt": "2024-04-03T23:20:07Z",
            "mergedAt": "2024-04-10T19:45:03Z",
            "closedAt": "2024-04-10T19:45:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 134,
            "deletions": 110,
            "body": "create a layers container to hold tensors of the same layer as described by their tensor name. this makes it easy get layers as a whole as well as tensors within a certain layer",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix CI release glitches",
            "url": "https://github.com/ollama/ollama/pull/3479",
            "state": "MERGED",
            "createdAt": "2024-04-03T23:42:27Z",
            "mergedAt": "2024-04-04T01:42:28Z",
            "closedAt": "2024-04-04T01:42:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 8,
            "deletions": 8,
            "body": "The subprocess change moved the build directory\r\narm64 builds weren't setting cross-compilation flags when building on x86",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use an older version of the macOS sdk in release",
            "url": "https://github.com/ollama/ollama/pull/3484",
            "state": "MERGED",
            "createdAt": "2024-04-04T07:37:01Z",
            "mergedAt": "2024-04-04T16:48:54Z",
            "closedAt": "2024-04-04T16:48:54Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 20,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix dll compress in windows building",
            "url": "https://github.com/ollama/ollama/pull/3488",
            "state": "MERGED",
            "createdAt": "2024-04-04T13:29:13Z",
            "mergedAt": "2024-04-04T23:12:13Z",
            "closedAt": "2024-04-04T23:12:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The compiled DLL on the Windows platform has not been properly compressed.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add test case for context exhaustion",
            "url": "https://github.com/ollama/ollama/pull/3491",
            "state": "MERGED",
            "createdAt": "2024-04-04T14:42:44Z",
            "mergedAt": "2024-04-04T23:20:20Z",
            "closedAt": "2024-04-04T23:20:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 29,
            "deletions": 0,
            "body": "Confirmed this fails on 0.1.30 with known regression but passes on main",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fail fast if mingw missing on windows",
            "url": "https://github.com/ollama/ollama/pull/3494",
            "state": "MERGED",
            "createdAt": "2024-04-04T16:52:06Z",
            "mergedAt": "2024-04-04T17:15:40Z",
            "closedAt": "2024-04-04T17:15:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add command-r graph estimate",
            "url": "https://github.com/ollama/ollama/pull/3496",
            "state": "MERGED",
            "createdAt": "2024-04-04T21:07:41Z",
            "mergedAt": "2024-04-05T19:26:22Z",
            "closedAt": "2024-04-05T19:26:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Chatbot UI v2 to Community Integrations",
            "url": "https://github.com/ollama/ollama/pull/3503",
            "state": "MERGED",
            "createdAt": "2024-04-05T13:38:00Z",
            "mergedAt": "2024-04-23T00:09:55Z",
            "closedAt": "2024-04-23T00:09:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "This adds Chatbot UI v2 by @mckaywrigley to the list of Community Integrations. This version has native Ollama compatibility now.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cgo quantize",
            "url": "https://github.com/ollama/ollama/pull/3506",
            "state": "MERGED",
            "createdAt": "2024-04-05T16:00:19Z",
            "mergedAt": "2024-04-09T19:32:53Z",
            "closedAt": "2024-04-09T19:32:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 145,
            "deletions": 45,
            "body": "revive #307 \r\n\r\nthis will _only_ quantize a converted model. quantizing an arbitrary fp16/fp32 will be a follow up",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "no rope parameters",
            "url": "https://github.com/ollama/ollama/pull/3508",
            "state": "MERGED",
            "createdAt": "2024-04-06T01:00:32Z",
            "mergedAt": "2024-04-06T01:46:06Z",
            "closedAt": "2024-04-06T01:46:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 0,
            "deletions": 14,
            "body": "rope parameters should be omitted because the model should set it",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Docs: Remove wrong parameter for Chat Completion",
            "url": "https://github.com/ollama/ollama/pull/3515",
            "state": "MERGED",
            "createdAt": "2024-04-06T11:56:35Z",
            "mergedAt": "2024-04-06T16:08:35Z",
            "closedAt": "2024-04-06T16:08:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "Fixes gh-3514",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ignore vscode debug build",
            "url": "https://github.com/ollama/ollama/pull/3518",
            "state": "CLOSED",
            "createdAt": "2024-04-07T01:01:04Z",
            "mergedAt": null,
            "closedAt": "2024-04-23T00:47:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "Prevent this from accidentally getting added to the repo history.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: close files in the CreateHandler func",
            "url": "https://github.com/ollama/ollama/pull/3519",
            "state": "CLOSED",
            "createdAt": "2024-04-07T03:27:39Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T07:25:09Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor(cmd): distribute commands into root.go, create.go, run.go, etc.",
            "url": "https://github.com/ollama/ollama/pull/3522",
            "state": "CLOSED",
            "createdAt": "2024-04-07T07:09:05Z",
            "mergedAt": null,
            "closedAt": "2024-04-16T11:16:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 12
            },
            "additions": 782,
            "deletions": 675,
            "body": "Previously, all commands were located in a single cmd.go file. This refactor improves the organization and readability of the code by distributing commands into their respective files such as root.go, create.go, run.go, etc.\r\n\r\nAdditionally, the modification this time was just copying and pasting the code into another file without making any changes to the code logic.",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update generate scripts with new `LLAMA_CUDA` variable, set `HIP_PLATFORM` on Windows to avoid compiler errors",
            "url": "https://github.com/ollama/ollama/pull/3528",
            "state": "MERGED",
            "createdAt": "2024-04-07T21:05:46Z",
            "mergedAt": "2024-04-07T23:29:51Z",
            "closedAt": "2024-04-07T23:29:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add metrics endpoint and request metrics",
            "url": "https://github.com/ollama/ollama/pull/3529",
            "state": "CLOSED",
            "createdAt": "2024-04-07T23:39:42Z",
            "mergedAt": null,
            "closedAt": "2024-09-30T19:52:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 194,
            "deletions": 5,
            "body": "Resolves https://github.com/ollama/ollama/issues/3144\r\n\r\nThis pull request is to add /metrics endpoint and generally used metrics. \r\nIt exposes default prometheus metrics and custom metrics for request endpoints.\r\n\r\nThis PR does not try to cover all metrics to keep it simple. If this looks good. I could add few more that will be useful.\r\n\r\nHow to test: \r\nonce Ollama server is running pull a model and list(or any other Ollama actions)\r\n\r\n```\r\ncurl http://127.0.0.1:11434/metrics\r\n```\r\n\r\nexample custom metrics(not all are shown since i tried only few commands):\r\n\r\n```\r\ncurl http://127.0.0.1:11434/metrics | grep -i ollama\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  6664    0  6664    0     0   519k      0 --:--:-- --:--:-- --:--:--  542k\r\n# HELP ollama_model_list_requests_total The total number of model list requets that have been attempted.\r\n# TYPE ollama_model_list_requests_total counter\r\nollama_model_list_requests_total{action=\"list\",status=\"OK\",status_code=\"200\"} 1\r\n# HELP ollama_model_pull_requests_total The total number of model pulls that have been attempted.\r\n# TYPE ollama_model_pull_requests_total counter\r\nollama_model_pull_requests_total{action=\"pull\",status=\"OK\",status_code=\"200\"} 1\r\n# HELP ollama_model_requests_total The total number of requests on all endpoints.\r\n# TYPE ollama_model_requests_total counter\r\nollama_model_requests_total{action=\"all\",status=\"OK\",status_code=\"200\"} 6\r\n```\r\n\r\nFull output:\r\n\r\n```\r\n# curl http://127.0.0.1:11434/metrics                 \r\n# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.\r\n# TYPE go_gc_duration_seconds summary\r\ngo_gc_duration_seconds{quantile=\"0\"} 1.0458e-05\r\ngo_gc_duration_seconds{quantile=\"0.25\"} 5.4042e-05\r\ngo_gc_duration_seconds{quantile=\"0.5\"} 6.9792e-05\r\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000121417\r\ngo_gc_duration_seconds{quantile=\"1\"} 0.00423525\r\ngo_gc_duration_seconds_sum 0.019693795\r\ngo_gc_duration_seconds_count 42\r\n# HELP go_goroutines Number of goroutines that currently exist.\r\n# TYPE go_goroutines gauge\r\ngo_goroutines 9\r\n# HELP go_info Information about the Go environment.\r\n# TYPE go_info gauge\r\ngo_info{version=\"go1.22.1\"} 1\r\n# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.\r\n# TYPE go_memstats_alloc_bytes gauge\r\ngo_memstats_alloc_bytes 2.429592e+06\r\n# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.\r\n# TYPE go_memstats_alloc_bytes_total counter\r\ngo_memstats_alloc_bytes_total 9.359476e+07\r\n# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.\r\n# TYPE go_memstats_buck_hash_sys_bytes gauge\r\ngo_memstats_buck_hash_sys_bytes 11319\r\n# HELP go_memstats_frees_total Total number of frees.\r\n# TYPE go_memstats_frees_total counter\r\ngo_memstats_frees_total 545225\r\n# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.\r\n# TYPE go_memstats_gc_sys_bytes gauge\r\ngo_memstats_gc_sys_bytes 3.465616e+06\r\n# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.\r\n# TYPE go_memstats_heap_alloc_bytes gauge\r\ngo_memstats_heap_alloc_bytes 2.429592e+06\r\n# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.\r\n# TYPE go_memstats_heap_idle_bytes gauge\r\ngo_memstats_heap_idle_bytes 6.660096e+06\r\n# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.\r\n# TYPE go_memstats_heap_inuse_bytes gauge\r\ngo_memstats_heap_inuse_bytes 5.136384e+06\r\n# HELP go_memstats_heap_objects Number of allocated objects.\r\n# TYPE go_memstats_heap_objects gauge\r\ngo_memstats_heap_objects 12324\r\n# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.\r\n# TYPE go_memstats_heap_released_bytes gauge\r\ngo_memstats_heap_released_bytes 6.201344e+06\r\n# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.\r\n# TYPE go_memstats_heap_sys_bytes gauge\r\ngo_memstats_heap_sys_bytes 1.179648e+07\r\n# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.\r\n# TYPE go_memstats_last_gc_time_seconds gauge\r\ngo_memstats_last_gc_time_seconds 1.7125328358982313e+09\r\n# HELP go_memstats_lookups_total Total number of pointer lookups.\r\n# TYPE go_memstats_lookups_total counter\r\ngo_memstats_lookups_total 0\r\n# HELP go_memstats_mallocs_total Total number of mallocs.\r\n# TYPE go_memstats_mallocs_total counter\r\ngo_memstats_mallocs_total 557549\r\n# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.\r\n# TYPE go_memstats_mcache_inuse_bytes gauge\r\ngo_memstats_mcache_inuse_bytes 4800\r\n# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.\r\n# TYPE go_memstats_mcache_sys_bytes gauge\r\ngo_memstats_mcache_sys_bytes 15600\r\n# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.\r\n# TYPE go_memstats_mspan_inuse_bytes gauge\r\ngo_memstats_mspan_inuse_bytes 123840\r\n# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.\r\n# TYPE go_memstats_mspan_sys_bytes gauge\r\ngo_memstats_mspan_sys_bytes 179520\r\n# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.\r\n# TYPE go_memstats_next_gc_bytes gauge\r\ngo_memstats_next_gc_bytes 5.554272e+06\r\n# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.\r\n# TYPE go_memstats_other_sys_bytes gauge\r\ngo_memstats_other_sys_bytes 1.004569e+06\r\n# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.\r\n# TYPE go_memstats_stack_inuse_bytes gauge\r\ngo_memstats_stack_inuse_bytes 786432\r\n# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.\r\n# TYPE go_memstats_stack_sys_bytes gauge\r\ngo_memstats_stack_sys_bytes 786432\r\n# HELP go_memstats_sys_bytes Number of bytes obtained from system.\r\n# TYPE go_memstats_sys_bytes gauge\r\ngo_memstats_sys_bytes 1.7259536e+07\r\n# HELP go_threads Number of OS threads created.\r\n# TYPE go_threads gauge\r\ngo_threads 15\r\n# HELP ollama_model_list_requests_total The total number of model list requets that have been attempted.\r\n# TYPE ollama_model_list_requests_total counter\r\nollama_model_list_requests_total{action=\"list\",status=\"OK\",status_code=\"200\"} 1\r\n# HELP ollama_model_pull_requests_total The total number of model pulls that have been attempted.\r\n# TYPE ollama_model_pull_requests_total counter\r\nollama_model_pull_requests_total{action=\"pull\",status=\"OK\",status_code=\"200\"} 1\r\n# HELP ollama_model_requests_total The total number of requests on all endpoints.\r\n# TYPE ollama_model_requests_total counter\r\nollama_model_requests_total{action=\"all\",status=\"OK\",status_code=\"200\"} 7\r\n# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\r\n# TYPE process_cpu_seconds_total counter\r\nprocess_cpu_seconds_total 73.31\r\n# HELP process_max_fds Maximum number of open file descriptors.\r\n# TYPE process_max_fds gauge\r\nprocess_max_fds 1.048576e+06\r\n# HELP process_open_fds Number of open file descriptors.\r\n# TYPE process_open_fds gauge\r\nprocess_open_fds 9\r\n# HELP process_resident_memory_bytes Resident memory size in bytes.\r\n# TYPE process_resident_memory_bytes gauge\r\nprocess_resident_memory_bytes 2.942976e+08\r\n# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.\r\n# TYPE process_start_time_seconds gauge\r\nprocess_start_time_seconds 1.71253222787e+09\r\n# HELP process_virtual_memory_bytes Virtual memory size in bytes.\r\n# TYPE process_virtual_memory_bytes gauge\r\nprocess_virtual_memory_bytes 2.536882176e+09\r\n# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.\r\n# TYPE process_virtual_memory_max_bytes gauge\r\nprocess_virtual_memory_max_bytes 1.8446744073709552e+19\r\n# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.\r\n# TYPE promhttp_metric_handler_requests_in_flight gauge\r\npromhttp_metric_handler_requests_in_flight 1\r\n# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.\r\n# TYPE promhttp_metric_handler_requests_total counter\r\npromhttp_metric_handler_requests_total{code=\"200\"} 3\r\npromhttp_metric_handler_requests_total{code=\"500\"} 0\r\npromhttp_metric_handler_requests_total{code=\"503\"} 0\r\n```\r\n\r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 7
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/3539",
            "state": "MERGED",
            "createdAt": "2024-04-08T14:56:36Z",
            "mergedAt": "2024-04-08T14:58:14Z",
            "closedAt": "2024-04-08T14:58:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "RAGFlow now supports integration with Ollama.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: init with Name and Digest types",
            "url": "https://github.com/ollama/ollama/pull/3541",
            "state": "MERGED",
            "createdAt": "2024-04-08T18:52:24Z",
            "mergedAt": "2024-04-10T23:30:05Z",
            "closedAt": "2024-04-10T23:30:05Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 11
            },
            "additions": 1264,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "build.go: introduce a friendlier way to build Ollama",
            "url": "https://github.com/ollama/ollama/pull/3548",
            "state": "MERGED",
            "createdAt": "2024-04-09T04:58:39Z",
            "mergedAt": "2024-04-09T21:18:47Z",
            "closedAt": "2024-04-09T21:18:47Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 11
            },
            "additions": 249,
            "deletions": 59,
            "body": "This commit introduces a more friendly way to build Ollama dependencies and the binary without abusing `go generate` and removing the unnecessary extra steps it brings with it.\r\n\r\nThis script also provides nicer feedback to the user about what is happening during the build process.\r\n\r\nAt the end, it prints a helpful message to the user about what to do next (e.g. run the new local Ollama).\r\n\r\nThis addresses a few other issues:\r\n\r\n1. We can now introduce new code generation tools without forcing rebuilds of llama.cpp \r\n1. We can now introduce new build flags/features (e.g. `rangefunc`), et al without hurting developer on-boarding experience\r\n2. We also have more power and flexibility with how/where we layout our directory structure in the future.\r\n\r\nHow to test:\r\n\r\n    ; git clone https://github.com/ollama/ollama\r\n    ; go run build.go\r\n\r\nTest that `./ollama serve` runs, and then try:\r\n\r\n    ; go run build.go\r\n\r\nThat should skip the generate step, then try:\r\n\r\n    ; go run build.go -f\r\n\r\nThat should rebuild llama.cpp again.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct directory reference in macapp/README",
            "url": "https://github.com/ollama/ollama/pull/3555",
            "state": "MERGED",
            "createdAt": "2024-04-09T10:23:12Z",
            "mergedAt": "2024-04-09T13:48:46Z",
            "closedAt": "2024-04-09T13:48:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Minor README change that was likely omitted from https://github.com/ollama/ollama/commit/9da9e8fb7254df1148f9619bec781e52dc954678",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "ci: use go-version-file",
            "url": "https://github.com/ollama/ollama/pull/3559",
            "state": "MERGED",
            "createdAt": "2024-04-09T16:50:41Z",
            "mergedAt": "2024-04-09T18:03:18Z",
            "closedAt": "2024-04-09T18:03:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 12,
            "body": "use go-version-file to synchronize go versions between go.mod and ci",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: rope",
            "url": "https://github.com/ollama/ollama/pull/3565",
            "state": "MERGED",
            "createdAt": "2024-04-09T23:18:35Z",
            "mergedAt": "2024-04-09T23:36:55Z",
            "closedAt": "2024-04-09T23:36:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "Some models set RopeFrequencyBase and RopeFrequencyScale. Removing these fields makes those models unusable",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Handle very slow model loads",
            "url": "https://github.com/ollama/ollama/pull/3566",
            "state": "MERGED",
            "createdAt": "2024-04-09T23:36:02Z",
            "mergedAt": "2024-04-09T23:53:49Z",
            "closedAt": "2024-04-09T23:53:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "During testing, we're seeing some models take over 3 minutes.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "partial offloading",
            "url": "https://github.com/ollama/ollama/pull/3567",
            "state": "MERGED",
            "createdAt": "2024-04-10T00:30:22Z",
            "mergedAt": "2024-04-10T17:58:04Z",
            "closedAt": "2024-04-10T17:58:04Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 96,
            "deletions": 84,
            "body": "partial offloading of a model requires more memory in most cases (except for metal)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix ci",
            "url": "https://github.com/ollama/ollama/pull/3579",
            "state": "MERGED",
            "createdAt": "2024-04-10T18:27:10Z",
            "mergedAt": "2024-04-10T18:37:01Z",
            "closedAt": "2024-04-10T18:37:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 3,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: provide helpful workaround hint when stalling on pull",
            "url": "https://github.com/ollama/ollama/pull/3584",
            "state": "MERGED",
            "createdAt": "2024-04-10T20:28:04Z",
            "mergedAt": "2024-04-10T23:24:37Z",
            "closedAt": "2024-04-10T23:24:37Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "This is a quick fix to help users who are stuck on the \"pull\" step at 99%.\r\n\r\nIn the near future we're introducing a new registry client that should/will hopefully be smarter. In the meantime, this should unblock the users hitting issue #1736.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove MarshalText/UnmarshalText from Digest",
            "url": "https://github.com/ollama/ollama/pull/3586",
            "state": "MERGED",
            "createdAt": "2024-04-10T23:52:03Z",
            "mergedAt": "2024-04-10T23:52:49Z",
            "closedAt": "2024-04-10T23:52:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 14,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove header while getting model list",
            "url": "https://github.com/ollama/ollama/pull/3588",
            "state": "CLOSED",
            "createdAt": "2024-04-11T06:31:14Z",
            "mergedAt": null,
            "closedAt": "2024-06-10T01:57:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Added MindsDB information",
            "url": "https://github.com/ollama/ollama/pull/3595",
            "state": "MERGED",
            "createdAt": "2024-04-11T13:10:33Z",
            "mergedAt": "2024-04-15T22:35:30Z",
            "closedAt": "2024-04-15T22:35:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Added more details to MindsDB so that Ollama users can know that they can connect their Ollama model with nearly 200 data platforms, including databases, vector stores, and applications.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "api: fill up API documentation",
            "url": "https://github.com/ollama/ollama/pull/3596",
            "state": "MERGED",
            "createdAt": "2024-04-11T13:47:32Z",
            "mergedAt": "2024-05-07T23:27:47Z",
            "closedAt": "2024-05-07T23:27:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 99,
            "deletions": 13,
            "body": "Followup for #2878\r\n\r\nNow that the documentation is more complete, mention it in the README. Once a new version of ollama is tagged, pkg.go.dev will pick up the documentation comments and display everything on the linked page\r\n\r\nUpdates #2840",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "examples: add more Go examples using the API",
            "url": "https://github.com/ollama/ollama/pull/3599",
            "state": "MERGED",
            "createdAt": "2024-04-11T15:44:09Z",
            "mergedAt": "2024-04-15T22:34:54Z",
            "closedAt": "2024-04-15T22:34:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 129,
            "deletions": 0,
            "body": "Updates #2840 \r\n\r\nFollowup on #2879 \r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "mixtral mem",
            "url": "https://github.com/ollama/ollama/pull/3600",
            "state": "MERGED",
            "createdAt": "2024-04-11T18:10:55Z",
            "mergedAt": "2024-04-11T19:23:37Z",
            "closedAt": "2024-04-11T19:23:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix rocm deps with new subprocess paths",
            "url": "https://github.com/ollama/ollama/pull/3604",
            "state": "MERGED",
            "createdAt": "2024-04-11T19:52:29Z",
            "mergedAt": "2024-04-11T20:08:29Z",
            "closedAt": "2024-04-11T20:08:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This fixes a regression on main and in 0.1.32-rc1 where the rocm dependency file was missing the libraries.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove (*Digest).Scan and Digest.Value",
            "url": "https://github.com/ollama/ollama/pull/3605",
            "state": "MERGED",
            "createdAt": "2024-04-11T20:13:07Z",
            "mergedAt": "2024-04-11T20:32:31Z",
            "closedAt": "2024-04-11T20:32:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 27,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add llama2 / torch models for `ollama create`",
            "url": "https://github.com/ollama/ollama/pull/3607",
            "state": "MERGED",
            "createdAt": "2024-04-11T23:40:21Z",
            "mergedAt": "2024-04-15T18:26:42Z",
            "closedAt": "2024-04-15T18:26:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 11
            },
            "additions": 860,
            "deletions": 291,
            "body": "This change adds support for llama2 converting from pytorch files. It splits adds the concept of a `ModelFormat` (to split up torch vs safetensors) and a `ModelArch` (for models like llama2, mistral, and gemma).\r\n\r\nFor the torch files this change only looks for `.bin` files, but should include `.pth` and other torch filename extensions.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added Solar example at README.md",
            "url": "https://github.com/ollama/ollama/pull/3610",
            "state": "MERGED",
            "createdAt": "2024-04-12T03:37:07Z",
            "mergedAt": "2024-04-15T23:54:23Z",
            "closedAt": "2024-04-15T23:54:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Added just one line\r\n\r\n| Solar              | 10.7B      | 6.1GB | `ollama run solar`             |",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add qa-pilot link",
            "url": "https://github.com/ollama/ollama/pull/3612",
            "state": "MERGED",
            "createdAt": "2024-04-12T06:03:25Z",
            "mergedAt": "2024-04-23T00:10:34Z",
            "closedAt": "2024-04-23T00:10:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "add a [qa-pilot](https://github.com/reid41/QA-Pilot.git) tool which is also based on `ollama`.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "types/model: make ParseName variants less confusing",
            "url": "https://github.com/ollama/ollama/pull/3617",
            "state": "MERGED",
            "createdAt": "2024-04-12T20:29:11Z",
            "mergedAt": "2024-04-12T20:57:57Z",
            "closedAt": "2024-04-12T20:57:58Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 2
            },
            "additions": 176,
            "deletions": 135,
            "body": "Also, fix http stripping bug.\r\n\r\nAlso, improve upon docs about fills and masks.\r\n\r\nTo \"test\": Please load and read in `pkgsite`\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: add path helpers",
            "url": "https://github.com/ollama/ollama/pull/3619",
            "state": "MERGED",
            "createdAt": "2024-04-12T20:59:55Z",
            "mergedAt": "2024-04-13T19:59:19Z",
            "closedAt": "2024-04-13T19:59:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 308,
            "deletions": 42,
            "body": "This commit adds path helpers for working with Names in URL and file\r\npaths. The new helpers are ParseNameFromPath, ParseNameFromFilePath,\r\nName.Path, and Name.FilePath.\r\n    \r\nThis commit also adds Name.DisplayLongest, and Name.DisplayLong.\r\n    \r\nAlso, update a place where strings.StripPrefix is more consistent\r\nwith the surrounding code.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md with StreamDeploy",
            "url": "https://github.com/ollama/ollama/pull/3621",
            "state": "MERGED",
            "createdAt": "2024-04-13T00:16:41Z",
            "mergedAt": "2024-05-06T18:14:41Z",
            "closedAt": "2024-05-06T18:14:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add podman-ollama to terminal apps",
            "url": "https://github.com/ollama/ollama/pull/3626",
            "state": "MERGED",
            "createdAt": "2024-04-13T15:31:06Z",
            "mergedAt": "2024-04-23T00:13:23Z",
            "closedAt": "2024-04-23T00:13:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "The goal of podman-ollama is to make AI even more boring.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md with Discord-Ollama project",
            "url": "https://github.com/ollama/ollama/pull/3633",
            "state": "MERGED",
            "createdAt": "2024-04-13T20:16:45Z",
            "mergedAt": "2024-04-23T00:14:20Z",
            "closedAt": "2024-04-23T00:14:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Generalized TypeScript Discord Bot that integrates the Ollama-js library so any model from Ollama can be built or pulled into use.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "app: gracefully shut down `ollama serve` on windows",
            "url": "https://github.com/ollama/ollama/pull/3641",
            "state": "MERGED",
            "createdAt": "2024-04-14T21:08:23Z",
            "mergedAt": "2024-04-14T22:33:25Z",
            "closedAt": "2024-04-14T22:33:25Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 118,
            "deletions": 7,
            "body": "Fixes https://github.com/ollama/ollama/issues/3623",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Batch embeddings",
            "url": "https://github.com/ollama/ollama/pull/3642",
            "state": "CLOSED",
            "createdAt": "2024-04-15T01:11:56Z",
            "mergedAt": null,
            "closedAt": "2024-07-09T21:52:59Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 8
            },
            "additions": 241,
            "deletions": 70,
            "body": "This change makes it possible to generate embeddings for more than one string at once, making it much faster to process many chunks simultaneously with the `/api/embeddings` endpoint:\r\n\r\n```\r\n curl http://localhost:11434/api/embeddings -d '{\r\n  \"model\": \"all-minilm\",\r\n  \"prompt_batch\": [\r\n    \"Here is an article about llamas...\",\r\n    \"Here is another article about llamas...\",\r\n    \"Here is a third article about llamas...\",\r\n    ...\r\n    \"Here is yet another article...\"\r\n  ]\r\n}'\r\n```\r\n\r\nThe API will then respond with an `embeddings` field that contains a list of vector embeddings:\r\n\r\n```\r\n{\r\n  \"embedding_batch\": [\r\n    [0.010080209001898766 ... 0.308726966381073],\r\n    ...\r\n    [-0.15468695759773254 ... 0.09876912087202072]\r\n  ]\r\n}\r\n```\r\n\r\nTODO:\r\n- [ ] Finalize API\r\n- [ ] Finalize behavior when empty prompt is provided\r\n- [ ] Add more tests",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Terminate subprocess if receiving `SIGINT` or `SIGTERM` signals while model is loading",
            "url": "https://github.com/ollama/ollama/pull/3653",
            "state": "MERGED",
            "createdAt": "2024-04-15T14:43:20Z",
            "mergedAt": "2024-04-15T16:09:32Z",
            "closedAt": "2024-04-15T16:09:32Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 23,
            "deletions": 32,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add simple rag-chatbot to community integrations",
            "url": "https://github.com/ollama/ollama/pull/3655",
            "state": "MERGED",
            "createdAt": "2024-04-15T16:38:47Z",
            "mergedAt": "2024-04-23T00:16:55Z",
            "closedAt": "2024-04-23T00:16:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "- Hi there! I've been using Ollama for some time now, and I've been really pleased with it. Thanks so much for developing and keeping up with this project. It's been a great help for students like me to effortlessly run llm models locally. \r\n- I have created a simple chatbot app with Ollama, with a simple interface so you can use and chat with multiple PDFs. I hope my contribute useful for our community! I will be very happy if this project is in the community integrations.\r\n- My Repo: https://github.com/datvodinh/rag-chatbot.git\r\n- Demo: https://github.com/datvodinh/rag-chatbot/blob/master/README.md\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add support for IQ1_S, IQ3_S, IQ2_S, IQ4_XS. IQ4_NL is not functional",
            "url": "https://github.com/ollama/ollama/pull/3657",
            "state": "CLOSED",
            "createdAt": "2024-04-15T17:33:46Z",
            "mergedAt": null,
            "closedAt": "2024-05-10T20:26:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 27,
            "deletions": 2,
            "body": "This patch adds support for IQ1_S, IQ3_S, IQ2_S, IQ4_XS.\r\n\r\nIQ4_NL is using a different format, have to investigate further what are the differences.",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 14
            }
        }
    },
    {
        "node": {
            "title": "better checking for OLLAMA_HOST variable",
            "url": "https://github.com/ollama/ollama/pull/3661",
            "state": "MERGED",
            "createdAt": "2024-04-15T23:39:21Z",
            "mergedAt": "2024-04-29T23:14:07Z",
            "closedAt": "2024-04-29T23:14:07Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 4
            },
            "additions": 83,
            "deletions": 15,
            "body": "This change adds better validation to the `OLLAMA_HOST` variable when used with `ollama serve`. It should work with both IPv4 and IPv6, and includes unit tests.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "This proved to be more painful than useful.",
            "url": "https://github.com/ollama/ollama/pull/3662",
            "state": "MERGED",
            "createdAt": "2024-04-15T23:42:12Z",
            "mergedAt": "2024-04-15T23:58:00Z",
            "closedAt": "2024-04-15T23:58:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 12,
            "body": "This reverts commit 7d05a6ee8f44b314fa697a427439e5fa4d78c3d7.  \r\n\r\nThis proved to be more painful than useful.                                 \r\n\r\nSee: https://github.com/ollama/ollama/issues/3624                  \r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix padding in decode",
            "url": "https://github.com/ollama/ollama/pull/3663",
            "state": "MERGED",
            "createdAt": "2024-04-16T00:27:51Z",
            "mergedAt": "2024-04-16T00:44:54Z",
            "closedAt": "2024-04-16T00:44:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "TODO: update padding() to _only_ returning the padding",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix padding to only return padding",
            "url": "https://github.com/ollama/ollama/pull/3664",
            "state": "MERGED",
            "createdAt": "2024-04-16T00:32:11Z",
            "mergedAt": "2024-04-17T22:57:40Z",
            "closedAt": "2024-04-17T22:57:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 8,
            "body": "follow up to #3663 to simplify padding()",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "darwin: no partial offloading if required memory greater than system",
            "url": "https://github.com/ollama/ollama/pull/3678",
            "state": "MERGED",
            "createdAt": "2024-04-16T18:23:02Z",
            "mergedAt": "2024-04-16T19:05:56Z",
            "closedAt": "2024-04-16T19:05:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 17,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update install.sh added /etc/default/ollama",
            "url": "https://github.com/ollama/ollama/pull/3679",
            "state": "CLOSED",
            "createdAt": "2024-04-16T19:05:15Z",
            "mergedAt": null,
            "closedAt": "2024-05-16T01:06:44Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "Added persistent env file for the server, so one update to /etc/default/ollama will stay between updates.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: add FilepathNoBuild",
            "url": "https://github.com/ollama/ollama/pull/3680",
            "state": "MERGED",
            "createdAt": "2024-04-16T19:38:08Z",
            "mergedAt": "2024-04-17T01:35:43Z",
            "closedAt": "2024-04-17T01:35:43Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 42,
            "deletions": 15,
            "body": "Also, add test for DisplayLongest.\r\n\r\nAlso, plumb fill param to ParseName in MustParseName",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Support unicode characters in model path",
            "url": "https://github.com/ollama/ollama/pull/3681",
            "state": "MERGED",
            "createdAt": "2024-04-16T20:11:25Z",
            "mergedAt": "2024-04-16T21:00:12Z",
            "closedAt": "2024-04-16T21:00:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 2,
            "body": "When running the c++ subprocess, unicode characters in file names were not being parsed correctly, resulting in an error. This changes `server.cpp` to use `wmain` to receive the wide characters and converts them first.\r\n\r\nCloses #3273\r\nFixes #2888\r\nFixes #3120 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "quantize any fp16/fp32 model",
            "url": "https://github.com/ollama/ollama/pull/3682",
            "state": "MERGED",
            "createdAt": "2024-04-16T20:37:34Z",
            "mergedAt": "2024-05-07T22:20:49Z",
            "closedAt": "2024-05-07T22:20:49Z",
            "reviews": {
                "totalCount": 23
            },
            "files": {
                "totalCount": 14
            },
            "additions": 624,
            "deletions": 589,
            "body": "- FROM /path/to/{safetensors,pytorch}\r\n- FROM /path/to/fp{16,32}.bin\r\n- FROM model:fp{16,32}",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "scale graph based on gpu count",
            "url": "https://github.com/ollama/ollama/pull/3684",
            "state": "MERGED",
            "createdAt": "2024-04-16T21:45:28Z",
            "mergedAt": "2024-04-16T21:57:10Z",
            "closedAt": "2024-04-16T21:57:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update api.md",
            "url": "https://github.com/ollama/ollama/pull/3705",
            "state": "MERGED",
            "createdAt": "2024-04-17T17:04:11Z",
            "mergedAt": "2024-04-20T19:17:03Z",
            "closedAt": "2024-04-20T19:17:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "account for all non-repeating layers",
            "url": "https://github.com/ollama/ollama/pull/3706",
            "state": "MERGED",
            "createdAt": "2024-04-17T18:21:39Z",
            "mergedAt": "2024-04-17T18:58:21Z",
            "closedAt": "2024-04-17T18:58:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 50,
            "deletions": 11,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "move Ollama static build to its own flag",
            "url": "https://github.com/ollama/ollama/pull/3708",
            "state": "MERGED",
            "createdAt": "2024-04-17T18:45:17Z",
            "mergedAt": "2024-04-18T23:04:12Z",
            "closedAt": "2024-04-18T23:04:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 19,
            "body": "`static` builds by default, allows skipping, forces build if OLLAMA_CPU_TARGET=\"static\"",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Adds support for customizing GPU build flags in llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/3709",
            "state": "MERGED",
            "createdAt": "2024-04-17T20:03:02Z",
            "mergedAt": "2024-04-23T16:28:34Z",
            "closedAt": "2024-04-23T16:28:34Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 25,
            "deletions": 2,
            "body": "Appends OLLAMA_CUSTOM_GPU_DEFS to CMAKE_DEFS. Will override any previously set build flags, allows for customizing GPU options when building from source.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "update jetson tutorial",
            "url": "https://github.com/ollama/ollama/pull/3710",
            "state": "MERGED",
            "createdAt": "2024-04-17T20:18:07Z",
            "mergedAt": "2024-04-18T23:02:09Z",
            "closedAt": "2024-04-18T23:02:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 29,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add stablelm graph calculation",
            "url": "https://github.com/ollama/ollama/pull/3712",
            "state": "MERGED",
            "createdAt": "2024-04-17T20:57:31Z",
            "mergedAt": "2024-04-17T22:57:51Z",
            "closedAt": "2024-04-17T22:57:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update copy handler to use model.Name",
            "url": "https://github.com/ollama/ollama/pull/3713",
            "state": "MERGED",
            "createdAt": "2024-04-17T21:12:53Z",
            "mergedAt": "2024-04-24T23:00:32Z",
            "closedAt": "2024-04-24T23:00:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 30,
            "deletions": 32,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: support : in PartHost for host:port",
            "url": "https://github.com/ollama/ollama/pull/3714",
            "state": "MERGED",
            "createdAt": "2024-04-17T22:14:59Z",
            "mergedAt": "2024-04-17T22:34:03Z",
            "closedAt": "2024-04-17T22:34:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 0,
            "body": "PartHost needs to allow `:` to support host:port",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update list handler to use model.Name",
            "url": "https://github.com/ollama/ollama/pull/3715",
            "state": "MERGED",
            "createdAt": "2024-04-17T23:23:09Z",
            "mergedAt": "2024-05-07T22:21:39Z",
            "closedAt": "2024-05-07T22:21:39Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 6
            },
            "additions": 215,
            "deletions": 87,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "prompt to display and add local ollama keys to account",
            "url": "https://github.com/ollama/ollama/pull/3717",
            "state": "MERGED",
            "createdAt": "2024-04-18T00:22:41Z",
            "mergedAt": "2024-04-30T18:02:08Z",
            "closedAt": "2024-04-30T18:02:08Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 4
            },
            "additions": 155,
            "deletions": 7,
            "body": "- return descriptive error messages when unauthorized to create blob or push a model\r\n- display the local public key associated with the request that was denied",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update delete handler to use model.Name",
            "url": "https://github.com/ollama/ollama/pull/3718",
            "state": "MERGED",
            "createdAt": "2024-04-18T00:24:42Z",
            "mergedAt": "2024-05-29T19:02:07Z",
            "closedAt": "2024-05-29T19:02:07Z",
            "reviews": {
                "totalCount": 12
            },
            "files": {
                "totalCount": 8
            },
            "additions": 587,
            "deletions": 122,
            "body": "also update list handler to use the `Manifests()` iter\r\n\r\nfollow up to #3715 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: accept former `:` as a separator in digest",
            "url": "https://github.com/ollama/ollama/pull/3724",
            "state": "MERGED",
            "createdAt": "2024-04-18T05:07:15Z",
            "mergedAt": "2024-04-18T21:17:46Z",
            "closedAt": "2024-04-18T21:17:46Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "This also converges the old sep `:` to the new sep `-`.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update create handler to use model.Name",
            "url": "https://github.com/ollama/ollama/pull/3737",
            "state": "MERGED",
            "createdAt": "2024-04-18T22:48:01Z",
            "mergedAt": "2024-06-05T19:05:05Z",
            "closedAt": "2024-06-05T19:05:05Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 6
            },
            "additions": 436,
            "deletions": 65,
            "body": "follow up to #3718 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Critical fix from llama.cpp JSON grammar to forbid un-escaped escape characters in JSON strings",
            "url": "https://github.com/ollama/ollama/pull/3782",
            "state": "MERGED",
            "createdAt": "2024-04-20T19:18:56Z",
            "mergedAt": "2024-06-09T17:57:09Z",
            "closedAt": "2024-06-09T17:57:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "JSON generation is broken, as models can insert control characters inside strings, which violates JSON.  For example, with the current JSON grammar, models could generate:\r\n\r\n```\r\n{ \"key\": \"value\r\nbroken\" }\r\n```\r\n\r\nThis is incorrect, and if a linebreak is wanted in the middle of the string there, it should be:\r\n\r\n```\r\n{ \"key\": \"value\\nbroken\" }\r\n```\r\n\r\nThe former breaks at least the nodejs JSON parser, and likely many many others, since it's not compliant JSON.\r\n\r\nThis PR injects the grammar directly from llama.cpp upstream, which prohibits `\\x00` through `\\x1f` inside JSON strings, fixing the problem.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "types/model: export IsValidNamePart",
            "url": "https://github.com/ollama/ollama/pull/3788",
            "state": "MERGED",
            "createdAt": "2024-04-21T01:11:55Z",
            "mergedAt": "2024-04-21T01:26:34Z",
            "closedAt": "2024-04-21T01:26:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chore: use errors.New to replace fmt.Errorf with no parameters will much better",
            "url": "https://github.com/ollama/ollama/pull/3789",
            "state": "MERGED",
            "createdAt": "2024-04-21T01:52:14Z",
            "mergedAt": "2024-04-21T02:11:06Z",
            "closedAt": "2024-04-21T02:11:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 5,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: enable OLLAMA Arc GPU support with SYCL backend",
            "url": "https://github.com/ollama/ollama/pull/3796",
            "state": "CLOSED",
            "createdAt": "2024-04-21T12:55:04Z",
            "mergedAt": null,
            "closedAt": "2024-06-09T17:59:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 12
            },
            "additions": 627,
            "deletions": 18,
            "body": "This is based on the original PR created by @felipeagc:main https://github.com/ollama/ollama/pull/2458.\r\n\r\nIt seems that the work on that pull request has come to a halt. I would like to work on this project in the next few days and accelerate the progress. I have tested the build with Ubuntu LTS and GPU Arc770.\r\n\r\nI'm happy to progress the PR with the community feedback.\r\n\r\n```log\r\ntime=2024-04-21T17:39:51.870+05:30 level=INFO source=images.go:817 msg=\"total blobs: 0\"\r\ntime=2024-04-21T17:39:51.870+05:30 level=INFO source=images.go:824 msg=\"total unused blobs removed: 0\"\r\ntime=2024-04-21T17:39:51.874+05:30 level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.1.32-17-g91f1201-dirty)\"\r\ntime=2024-04-21T17:39:51.874+05:30 level=INFO source=payload.go:28 msg=\"extracting embedded files\" dir=/tmp/ollama2497595442/runners\r\ntime=2024-04-21T17:39:55.586+05:30 level=INFO source=payload.go:41 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60002 cpu]\"\r\ntime=2024-04-21T17:39:55.586+05:30 level=INFO source=gpu.go:140 msg=\"Detecting GPU type\"\r\ntime=2024-04-21T17:39:55.586+05:30 level=INFO source=gpu.go:320 msg=\"Searching for GPU management library libcudart.so*\"\r\ntime=2024-04-21T17:39:55.588+05:30 level=INFO source=gpu.go:366 msg=\"Discovered GPU libraries: [/tmp/ollama2497595442/runners/cuda_v11/libcudart.so.11.0]\"\r\ntime=2024-04-21T17:39:55.601+05:30 level=INFO source=gpu.go:395 msg=\"Unable to load cudart CUDA management library /tmp/ollama2497595442/runners/cuda_v11/libcudart.so.11.0: cudart init failure: 100\"\r\ntime=2024-04-21T17:39:55.601+05:30 level=INFO source=gpu.go:320 msg=\"Searching for GPU management library libnvidia-ml.so\"\r\ntime=2024-04-21T17:39:55.603+05:30 level=INFO source=gpu.go:366 msg=\"Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.535.171.04]\"\r\ntime=2024-04-21T17:39:55.608+05:30 level=INFO source=gpu.go:378 msg=\"Unable to load NVML management library /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.535.171.04: nvml vram init failure: 9\"\r\ntime=2024-04-21T17:39:55.608+05:30 level=INFO source=gpu.go:320 msg=\"Searching for GPU management library libze_intel_gpu.so\"\r\ntime=2024-04-21T17:39:55.610+05:30 level=INFO source=gpu.go:366 msg=\"Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libze_intel_gpu.so.1.3.28202.51]\"\r\ntime=2024-04-21T17:39:55.662+05:30 level=INFO source=gpu.go:166 msg=\"Intel GPU detected\"\r\ntime=2024-04-21T17:39:55.662+05:30 level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\r\n```",
            "participants": {
                "totalCount": 9
            },
            "comments": {
                "totalCount": 21
            }
        }
    },
    {
        "node": {
            "title": "docs: update README to add chat (web UI) for LLM",
            "url": "https://github.com/ollama/ollama/pull/3810",
            "state": "MERGED",
            "createdAt": "2024-04-22T03:47:16Z",
            "mergedAt": "2024-04-23T00:19:39Z",
            "closedAt": "2024-04-23T00:19:39Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I have used chat with llama3 in local successfully and the code is MIT licensed.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "docs: Update README for Lobe-chat integration.",
            "url": "https://github.com/ollama/ollama/pull/3817",
            "state": "MERGED",
            "createdAt": "2024-04-22T08:49:42Z",
            "mergedAt": "2024-04-23T00:18:16Z",
            "closedAt": "2024-04-23T00:18:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Added Lobe Chat for Ollama",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: from blob",
            "url": "https://github.com/ollama/ollama/pull/3833",
            "state": "MERGED",
            "createdAt": "2024-04-22T22:57:28Z",
            "mergedAt": "2024-04-24T22:13:47Z",
            "closedAt": "2024-04-24T22:13:47Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 1
            },
            "additions": 96,
            "deletions": 87,
            "body": "from calls replace all which may affect other parts of the model file. instead, replace just the line.\r\n\r\nresolves #3685 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Report errors on server lookup instead of path lookup failure",
            "url": "https://github.com/ollama/ollama/pull/3834",
            "state": "MERGED",
            "createdAt": "2024-04-22T23:23:35Z",
            "mergedAt": "2024-04-24T17:50:48Z",
            "closedAt": "2024-04-24T17:50:48Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "This should help identify more details on the failure mode for #3738 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Trim spaces and quotes from llm lib override",
            "url": "https://github.com/ollama/ollama/pull/3835",
            "state": "MERGED",
            "createdAt": "2024-04-23T00:11:52Z",
            "mergedAt": "2024-04-23T02:06:54Z",
            "closedAt": "2024-04-23T02:06:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes #3512 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: mixtral graph",
            "url": "https://github.com/ollama/ollama/pull/3836",
            "state": "MERGED",
            "createdAt": "2024-04-23T00:19:55Z",
            "mergedAt": "2024-04-23T16:15:10Z",
            "closedAt": "2024-04-23T16:15:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct the kubernetes terminology",
            "url": "https://github.com/ollama/ollama/pull/3843",
            "state": "MERGED",
            "createdAt": "2024-04-23T13:01:41Z",
            "mergedAt": "2024-05-07T16:53:09Z",
            "closedAt": "2024-05-07T16:53:09Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 12,
            "body": "Correct the kubernetes terminology and explain the steps for testing. ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect and recover if runner removed",
            "url": "https://github.com/ollama/ollama/pull/3846",
            "state": "MERGED",
            "createdAt": "2024-04-23T17:06:59Z",
            "mergedAt": "2024-04-23T20:14:12Z",
            "closedAt": "2024-04-23T20:14:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 1,
            "body": "Tmp cleaners can nuke the file out from underneath us.  This detects the missing runner, and re-initializes the payloads.\r\n\r\nManually tested by hand removing the server after loading it once, trigger an unload with `keep_alive: 0` sent another request, saw the log message, and it loaded correctly.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move nested payloads to installer and zip file on windows",
            "url": "https://github.com/ollama/ollama/pull/3850",
            "state": "MERGED",
            "createdAt": "2024-04-23T19:21:17Z",
            "mergedAt": "2024-04-23T23:35:20Z",
            "closedAt": "2024-04-23T23:35:20Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 9
            },
            "additions": 105,
            "deletions": 48,
            "body": "Now that the llm runner is an executable and not just a dll, more users are facing problems with security policy configurations on windows that prevent users writing to directories and then executing binaries from the same location. This change removes payloads from the main executable on windows and shifts them over to be packaged in the installer and discovered based on the executables location. This also adds a new zip file for people who want to \"roll their own\" installation model.\r\n\r\nFixes #3763 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update windows.md",
            "url": "https://github.com/ollama/ollama/pull/3855",
            "state": "MERGED",
            "createdAt": "2024-04-23T22:02:07Z",
            "mergedAt": "2024-04-26T20:04:15Z",
            "closedAt": "2024-04-26T20:04:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixed a typo",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add back memory escape valve",
            "url": "https://github.com/ollama/ollama/pull/3857",
            "state": "MERGED",
            "createdAt": "2024-04-24T00:09:27Z",
            "mergedAt": "2024-04-24T00:32:24Z",
            "closedAt": "2024-04-24T00:32:24Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 0,
            "body": "If we get our predictions wrong, this can be used to set a lower memory limit as a workaround.  Recent multi-gpu refactoring accidentally removed it, so this adds it back.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: restrict digest hash part to a minimum of 2 characters",
            "url": "https://github.com/ollama/ollama/pull/3858",
            "state": "MERGED",
            "createdAt": "2024-04-24T00:36:11Z",
            "mergedAt": "2024-04-24T01:24:17Z",
            "closedAt": "2024-04-24T01:24:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 34,
            "deletions": 19,
            "body": "This allows users of a valid Digest to know it has a minimum of 2 characters in the hash part for use when sharding.\r\n\r\nThis is a reasonable restriction as the hash part is a SHA256 hash which is 64 characters long, which is the common hash used. There is no anticipation of using a hash with less than 2 characters.\r\n\r\nAlso, add MustParseDigest.\r\n\r\nAlso, replace Digest.Type with Digest.Split for getting both the type and hash parts together, which is most the common case when asking for either.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add mixtral 8x7b model conversion",
            "url": "https://github.com/ollama/ollama/pull/3859",
            "state": "MERGED",
            "createdAt": "2024-04-24T01:24:35Z",
            "mergedAt": "2024-04-24T03:17:04Z",
            "closedAt": "2024-04-24T03:17:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 138,
            "deletions": 25,
            "body": "This change converts Mixtral 8x7b directly into an Ollama model. The 8x22b model will added in a separate PR as it has a different structure for the way the experts get laid out. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fixes for gguf",
            "url": "https://github.com/ollama/ollama/pull/3863",
            "state": "MERGED",
            "createdAt": "2024-04-24T03:31:32Z",
            "mergedAt": "2024-04-24T03:57:20Z",
            "closedAt": "2024-04-24T03:57:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 6,
            "body": "This change:\r\n  * adds the KVs for moes\r\n  * catches when an improper type was used in the KVs (we can add more if/when this changes)\r\n  * accounts for n-tensors in 8x22b moe models (where n > 2)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add OLLAMA_KEEP_ALIVE env variable to FAQ",
            "url": "https://github.com/ollama/ollama/pull/3865",
            "state": "MERGED",
            "createdAt": "2024-04-24T03:58:24Z",
            "mergedAt": "2024-04-24T04:06:51Z",
            "closedAt": "2024-04-24T04:06:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add macai to list of Web & Desktop integrations",
            "url": "https://github.com/ollama/ollama/pull/3881",
            "state": "MERGED",
            "createdAt": "2024-04-24T15:52:09Z",
            "mergedAt": "2024-05-07T20:31:34Z",
            "closedAt": "2024-05-07T20:31:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I'm not sure if there is some documented process of making changes in README.md - I was not able to find any instructions, so I'm sorry if I missed something. Please let me know if I should perform some extra steps to propose these changes. Thanks!",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "AMD gfx patch rev is hex",
            "url": "https://github.com/ollama/ollama/pull/3882",
            "state": "MERGED",
            "createdAt": "2024-04-24T16:44:29Z",
            "mergedAt": "2024-04-24T18:07:49Z",
            "closedAt": "2024-04-24T18:07:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 4,
            "body": "Correctly handle gfx90a discovery\r\n\r\nFixes #3809 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: add Hollama to Web & Desktop integrations",
            "url": "https://github.com/ollama/ollama/pull/3884",
            "state": "MERGED",
            "createdAt": "2024-04-24T16:46:34Z",
            "mergedAt": "2024-05-07T20:17:36Z",
            "closedAt": "2024-05-07T20:17:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "**Hollama** is a minimal web-UI for talking to Ollama servers.\r\nhttps://hollama.fernando.is\r\n\r\n**Repository:**\r\nhttps://github.com/fmaclen/hollama\r\n\r\n**Current features:**\r\n- Large prompt fields\r\n- Streams completions\r\n- Copy completions as raw text\r\n- Markdown parsing w/syntax highlighting\r\n- Saves sessions/context in your browser's localStorage\r\n- With [more to come](https://github.com/fmaclen/hollama/issues)...\r\n\r\n**Screenshots:**\r\n> ![image](https://github.com/ollama/ollama/assets/1434675/ec9d95b6-19c1-44fa-b4e2-9d47e32edd18)\r\n> ![image](https://github.com/ollama/ollama/assets/1434675/315d6b06-01a7-45c8-b7ef-b1a074e893bb)\r\n> ![image](https://github.com/ollama/ollama/assets/1434675/333e9ab3-3474-42cf-a0c1-cbf26025ad45)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: make ParseName use default without question",
            "url": "https://github.com/ollama/ollama/pull/3886",
            "state": "MERGED",
            "createdAt": "2024-04-24T18:35:27Z",
            "mergedAt": "2024-04-24T18:52:55Z",
            "closedAt": "2024-04-24T18:52:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 45,
            "deletions": 34,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: require all names parts start with an alnum char",
            "url": "https://github.com/ollama/ollama/pull/3887",
            "state": "CLOSED",
            "createdAt": "2024-04-24T18:55:11Z",
            "mergedAt": null,
            "closedAt": "2024-04-26T03:13:22Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 2
            },
            "additions": 26,
            "deletions": 17,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Remove trailing spaces",
            "url": "https://github.com/ollama/ollama/pull/3889",
            "state": "MERGED",
            "createdAt": "2024-04-24T20:00:05Z",
            "mergedAt": "2024-04-25T18:32:26Z",
            "closedAt": "2024-04-25T18:32:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 4,
            "body": "Remove trailing spaces in the bash scripts",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "refactor modelfile parser",
            "url": "https://github.com/ollama/ollama/pull/3892",
            "state": "MERGED",
            "createdAt": "2024-04-24T21:52:23Z",
            "mergedAt": "2024-05-03T00:04:47Z",
            "closedAt": "2024-05-03T00:04:47Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 748,
            "deletions": 206,
            "body": "split from #3833 \r\n\r\nresolves #3977 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move ggml loading to when attempting to fit",
            "url": "https://github.com/ollama/ollama/pull/3895",
            "state": "MERGED",
            "createdAt": "2024-04-24T23:55:30Z",
            "mergedAt": "2024-04-25T16:24:08Z",
            "closedAt": "2024-04-25T16:24:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 45,
            "deletions": 36,
            "body": "Fixes #3860\r\n\r\nThis moves the loading of the model until we attempt to see if we need we can fit the model into memory. I decided to not pull it out into a separate function as I don't think the 4 lines warrants it especially after moving some of the logic around. I kept that in a separate commit to easily rollback if the preference is to keep them separated in the main conditional flow.\r\n\r\n- Tested on a device without a GPU, loading a single model and concurrent requests to multiple models.\r\n- Test on a device with a dedicated GPU, limiting it to a single model as well as loading multiple models and making concurrent requests.\r\n\r\n\r\nAdditionally, fixed a panic in the tests when running with `-race`.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "show ggml modelinfo through the show api",
            "url": "https://github.com/ollama/ollama/pull/3899",
            "state": "CLOSED",
            "createdAt": "2024-04-25T01:54:59Z",
            "mergedAt": null,
            "closedAt": "2024-07-12T03:36:34Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 3
            },
            "additions": 52,
            "deletions": 2,
            "body": "This change exposes the GGML KVs and tensor data to make it easier to introspect a model.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add /api/infill for fill-in-the-middle",
            "url": "https://github.com/ollama/ollama/pull/3907",
            "state": "CLOSED",
            "createdAt": "2024-04-25T11:13:52Z",
            "mergedAt": null,
            "closedAt": "2024-09-12T01:39:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 380,
            "deletions": 118,
            "body": "This PR closes #3869\r\n\r\nAdds `/api/infill` to leverage llama.cpp's [POST /infill](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#api-endpoints) API for infilling / fill-in-the-middle / code-completion.\r\n\r\nAn example request looks like this:\r\n```http\r\nPOST /api/infill HTTP/1.1\r\nHost: localhost:11434\r\nContent-Type: application/json\r\nContent-Length: 199\r\n{\r\n    \"stream\": false,\r\n    \"model\": \"codellama:7b-instruct-q3_K_M\",\r\n    \"input_prefix\": \"public int gcd(int x, int y) {\",\r\n    \"input_suffix\": \"\\n}\",\r\n    \"options\": {\r\n        \"num_predict\": 10\r\n    }\r\n}\r\n```\r\nResponse:\r\n```json\r\n{\r\n    \"model\": \"codellama:7b-instruct-q3_K_M\",\r\n    \"created_at\": \"2024-04-25T11:09:06.691622744Z\",\r\n    \"response\": \"\\n    return y == 0 ? x :\",\r\n    \"done\": true,\r\n    \"total_duration\": 3385921466,\r\n    \"load_duration\": 948913941,\r\n    \"prompt_eval_count\": 18,\r\n    \"prompt_eval_duration\": 1175793000,\r\n    \"eval_count\": 10,\r\n    \"eval_duration\": 1219642000\r\n}\r\n``` \r\n(Streaming is also available)\r\n\r\nNote: could probably need more refactoring to cleanup duplicate code, I am very new to Golang programming, so feedback would be appreciated",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "Improve mac parallel performance",
            "url": "https://github.com/ollama/ollama/pull/3914",
            "state": "MERGED",
            "createdAt": "2024-04-25T16:40:14Z",
            "mergedAt": "2024-04-25T23:28:31Z",
            "closedAt": "2024-04-25T23:28:31Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 2
            },
            "additions": 56,
            "deletions": 0,
            "body": "Carries #3900 \r\n\r\nTesting underway...",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: limit generation to 10x context size to avoid run on generations",
            "url": "https://github.com/ollama/ollama/pull/3918",
            "state": "MERGED",
            "createdAt": "2024-04-25T18:20:21Z",
            "mergedAt": "2024-04-25T23:02:30Z",
            "closedAt": "2024-04-25T23:02:30Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 11,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Reload model if `num_gpu` changes",
            "url": "https://github.com/ollama/ollama/pull/3920",
            "state": "MERGED",
            "createdAt": "2024-04-25T19:20:28Z",
            "mergedAt": "2024-04-25T23:02:40Z",
            "closedAt": "2024-04-25T23:02:40Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "precalculate output tensor memory for metal and mmap",
            "url": "https://github.com/ollama/ollama/pull/3923",
            "state": "MERGED",
            "createdAt": "2024-04-25T22:00:38Z",
            "mergedAt": "2024-04-25T23:34:17Z",
            "closedAt": "2024-04-25T23:34:17Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 9,
            "body": "on metal with mmap, the output tensors are always allocated even if the offloaded layers < total layers + 1. other backends are unaffected",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: overhaul Name and Digest types",
            "url": "https://github.com/ollama/ollama/pull/3924",
            "state": "MERGED",
            "createdAt": "2024-04-25T23:20:10Z",
            "mergedAt": "2024-04-26T20:08:32Z",
            "closedAt": "2024-04-26T20:08:32Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 11
            },
            "additions": 528,
            "deletions": 1412,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Bump llama.cpp to b2737",
            "url": "https://github.com/ollama/ollama/pull/3925",
            "state": "MERGED",
            "createdAt": "2024-04-26T00:16:20Z",
            "mergedAt": "2024-04-26T17:09:38Z",
            "closedAt": "2024-04-26T17:09:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 13,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix release CI",
            "url": "https://github.com/ollama/ollama/pull/3926",
            "state": "MERGED",
            "createdAt": "2024-04-26T00:27:37Z",
            "mergedAt": "2024-04-26T00:42:31Z",
            "closedAt": "2024-04-26T00:42:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 11,
            "body": "download-artifact path was being used incorrectly.  It is where to extract the zip not the files in the zip to extract.  Default is workspace dir which is what we want, so omit it",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move cuda/rocm dependency gathering into generate script",
            "url": "https://github.com/ollama/ollama/pull/3933",
            "state": "MERGED",
            "createdAt": "2024-04-26T05:38:55Z",
            "mergedAt": "2024-04-26T14:01:24Z",
            "closedAt": "2024-04-26T14:01:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 15,
            "deletions": 20,
            "body": "This will make it simpler for CI to accumulate artifacts from prior steps",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update api.md",
            "url": "https://github.com/ollama/ollama/pull/3945",
            "state": "MERGED",
            "createdAt": "2024-04-26T13:55:48Z",
            "mergedAt": "2024-05-06T21:39:59Z",
            "closedAt": "2024-05-06T21:39:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Changed the calculation of tps (token/s) in the documentation",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add a /clear command",
            "url": "https://github.com/ollama/ollama/pull/3947",
            "state": "MERGED",
            "createdAt": "2024-04-26T15:29:39Z",
            "mergedAt": "2024-05-01T21:44:36Z",
            "closedAt": "2024-05-01T21:44:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "This has came up a couple of times in the issue history where you would like to clear your chat context. It can be annoying to reload a large model.\r\n\r\nThis adds a new command `/clear` in interactive sessions that just wipes the full chat history.\r\n\r\nLet me know if there other considerations I should take into account or other names we may want to have for the command.\r\n\r\nFixes #3937",
            "participants": {
                "totalCount": 6
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Refactor windows generate for more modular usage",
            "url": "https://github.com/ollama/ollama/pull/3948",
            "state": "MERGED",
            "createdAt": "2024-04-26T15:48:09Z",
            "mergedAt": "2024-04-26T16:17:20Z",
            "closedAt": "2024-04-26T16:17:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 169,
            "deletions": 129,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix exe name for zip packaging on windows",
            "url": "https://github.com/ollama/ollama/pull/3950",
            "state": "MERGED",
            "createdAt": "2024-04-26T16:18:18Z",
            "mergedAt": "2024-04-26T16:27:37Z",
            "closedAt": "2024-04-26T16:27:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The zip file encodes the OS and architecture, so keep the short exe name",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "check file type before zip",
            "url": "https://github.com/ollama/ollama/pull/3951",
            "state": "MERGED",
            "createdAt": "2024-04-26T18:40:12Z",
            "mergedAt": "2024-04-26T21:51:23Z",
            "closedAt": "2024-04-26T21:51:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 65,
            "deletions": 39,
            "body": "also include all json",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Put back non-avx CPU build for windows",
            "url": "https://github.com/ollama/ollama/pull/3954",
            "state": "MERGED",
            "createdAt": "2024-04-26T19:44:47Z",
            "mergedAt": "2024-04-26T20:09:04Z",
            "closedAt": "2024-04-26T20:09:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "return code `499` when user cancels request while a model is loading",
            "url": "https://github.com/ollama/ollama/pull/3955",
            "state": "MERGED",
            "createdAt": "2024-04-26T20:03:32Z",
            "mergedAt": "2024-04-26T21:38:29Z",
            "closedAt": "2024-04-26T21:38:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 16,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": ".github/workflows: add in-flight cancellations on new push",
            "url": "https://github.com/ollama/ollama/pull/3956",
            "state": "MERGED",
            "createdAt": "2024-04-26T20:13:25Z",
            "mergedAt": "2024-04-26T20:54:25Z",
            "closedAt": "2024-04-26T20:54:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: export ParseNameBare and Merge",
            "url": "https://github.com/ollama/ollama/pull/3957",
            "state": "MERGED",
            "createdAt": "2024-04-26T20:54:58Z",
            "mergedAt": "2024-04-26T21:58:07Z",
            "closedAt": "2024-04-26T21:58:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 9,
            "body": "These are useful outside this package.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use merge base for diff-tree",
            "url": "https://github.com/ollama/ollama/pull/3958",
            "state": "MERGED",
            "createdAt": "2024-04-26T20:57:30Z",
            "mergedAt": "2024-04-26T21:17:56Z",
            "closedAt": "2024-04-26T21:17:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "the diff-tree previously compared the head ref (the latest commit in the PR) against the base ref (the latest commit in the target branch). if the target branch is updated, this comparison will include the new files in the target as well which is wrong.\r\n\r\ninstead, find and compare the head ref against the merge base of the head and base refs. this should ensure only the changes added in the pr are evaluated",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Also look at cwd as a root for windows runners",
            "url": "https://github.com/ollama/ollama/pull/3959",
            "state": "MERGED",
            "createdAt": "2024-04-26T21:05:19Z",
            "mergedAt": "2024-04-26T23:14:08Z",
            "closedAt": "2024-04-26T23:14:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update the setup command to use llama3.",
            "url": "https://github.com/ollama/ollama/pull/3962",
            "state": "MERGED",
            "createdAt": "2024-04-26T21:44:49Z",
            "mergedAt": "2024-04-26T22:41:01Z",
            "closedAt": "2024-04-26T22:41:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Updates the command to run ollama upon installation to use llama3:\r\n`ollama run llama3`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix gemma, command-r layer weights",
            "url": "https://github.com/ollama/ollama/pull/3964",
            "state": "MERGED",
            "createdAt": "2024-04-26T22:01:41Z",
            "mergedAt": "2024-04-26T22:23:33Z",
            "closedAt": "2024-04-26T22:23:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 4,
            "body": "some models (gemma, command-r) do not have output tensors. instead the token_embd tensor are offloaded in its place",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix target in gen_windows.ps1",
            "url": "https://github.com/ollama/ollama/pull/3966",
            "state": "MERGED",
            "createdAt": "2024-04-26T22:11:15Z",
            "mergedAt": "2024-04-26T22:36:54Z",
            "closedAt": "2024-04-26T22:36:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "Replaces #3963 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fine grain control over windows generate steps",
            "url": "https://github.com/ollama/ollama/pull/3968",
            "state": "MERGED",
            "createdAt": "2024-04-26T22:37:39Z",
            "mergedAt": "2024-04-26T23:03:38Z",
            "closedAt": "2024-04-26T23:03:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 8,
            "body": "This will speed up CI which already tries to only build static for unit tests",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove Digest (for now)",
            "url": "https://github.com/ollama/ollama/pull/3970",
            "state": "MERGED",
            "createdAt": "2024-04-27T03:59:55Z",
            "mergedAt": "2024-04-27T04:14:28Z",
            "closedAt": "2024-04-27T04:14:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 140,
            "body": "The Digest type needs more thought and is not necessary at the moment.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add support for building on Windows ARM64",
            "url": "https://github.com/ollama/ollama/pull/3972",
            "state": "MERGED",
            "createdAt": "2024-04-27T05:38:44Z",
            "mergedAt": "2024-04-28T21:52:58Z",
            "closedAt": "2024-04-28T21:52:59Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 24,
            "deletions": 17,
            "body": "Part of #2589\r\n\r\n- Builds only the cpu runner for ARM64 Also, the existing CMake recipe already enables NEON and Armv8.2 extensions when ARM64 is detected.\r\n- I'll create another PR with build instructions. The main trick is that MSY2 has the CLANGARM64 environment that provides gcc aliases to Clang.\r\n\r\nMaintainer changes are enabled. Feel free to adjust the changes to your liking. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 7
            }
        }
    },
    {
        "node": {
            "title": "types/model: relax name length constraint from 2 to 1",
            "url": "https://github.com/ollama/ollama/pull/3984",
            "state": "MERGED",
            "createdAt": "2024-04-27T23:45:51Z",
            "mergedAt": "2024-04-28T00:58:41Z",
            "closedAt": "2024-04-28T00:58:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 15,
            "deletions": 11,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Restart server on failure when running Windows app",
            "url": "https://github.com/ollama/ollama/pull/3985",
            "state": "MERGED",
            "createdAt": "2024-04-28T02:49:29Z",
            "mergedAt": "2024-04-29T14:07:52Z",
            "closedAt": "2024-04-29T14:07:52Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 27,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: allow _ as starter character in Name parts",
            "url": "https://github.com/ollama/ollama/pull/3991",
            "state": "MERGED",
            "createdAt": "2024-04-28T04:18:55Z",
            "mergedAt": "2024-04-28T04:24:52Z",
            "closedAt": "2024-04-28T04:24:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 11,
            "deletions": 10,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix typos in README.md",
            "url": "https://github.com/ollama/ollama/pull/4007",
            "state": "MERGED",
            "createdAt": "2024-04-28T16:58:10Z",
            "mergedAt": "2024-05-01T17:39:38Z",
            "closedAt": "2024-05-01T17:39:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "- There was a lagging comma instead of a dot\r\n- Some other minor changes",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix concurrency for CPU mode",
            "url": "https://github.com/ollama/ollama/pull/4009",
            "state": "MERGED",
            "createdAt": "2024-04-28T20:48:26Z",
            "mergedAt": "2024-04-28T21:20:28Z",
            "closedAt": "2024-04-28T21:20:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 87,
            "deletions": 12,
            "body": "Prior refactoring passes on #3418 accidentally removed the logic to bypass VRAM checks for CPU loads.  This adds that back, along with test coverage.\r\n\r\nThis also fixes loaded map access in the unit test to be behind the mutex which was likely the cause of various flakes in the tests.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md to include ollama-r library",
            "url": "https://github.com/ollama/ollama/pull/4012",
            "state": "MERGED",
            "createdAt": "2024-04-29T01:16:58Z",
            "mergedAt": "2024-05-07T16:52:30Z",
            "closedAt": "2024-05-07T16:52:30Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add ollama-r library",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix copying model to itself",
            "url": "https://github.com/ollama/ollama/pull/4019",
            "state": "MERGED",
            "createdAt": "2024-04-29T03:45:26Z",
            "mergedAt": "2024-04-29T03:47:49Z",
            "closedAt": "2024-04-29T03:47:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove old comment",
            "url": "https://github.com/ollama/ollama/pull/4020",
            "state": "MERGED",
            "createdAt": "2024-04-29T03:46:05Z",
            "mergedAt": "2024-04-29T03:52:26Z",
            "closedAt": "2024-04-29T03:52:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix/issue 3736: When runners are closing or expiring. Scheduler is getting dirty VRAM size readings.",
            "url": "https://github.com/ollama/ollama/pull/4031",
            "state": "MERGED",
            "createdAt": "2024-04-29T15:51:52Z",
            "mergedAt": "2024-05-01T19:13:26Z",
            "closedAt": "2024-05-01T19:13:26Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 18,
            "deletions": 8,
            "body": "Issue: When the Ollama `Scheduler` requests a runner to stop (kill), the `Scheduler` reads the available VRAM and gets a size that includes the terminating runner. This results in offloading to the CPU and slower execution. Each time a new model is swapped in, the new runner reads the previous runner's memory allocation. This affects the new runner's VRAM allocation estimate.\r\nFix: When stopping a runner, wait for the process to exit so that the memory is free before `Scheduler` checks the amount of VRAM available.\r\n\r\nIssue: After a runner finishes a request, an expiration timer is assigned based on the session duration. Subsequent requests will renew the expiration timer after each request has finished. If a request happens to take too long and the timer fires, the runner will be scheduled to be unloaded.  If concurrently, a new pending may get an incorrect measure of VRAM, resulting in offloading to the CPU and slower execution.  Runners are expiring in the middle of heavy use, which results in the same model closing and reloading.  The reloading gets a dirty VRAM measurement because the previous runner is not fully closed before the new runner is created. The concurrency of the pending and completed Go routines.  The pending continues on the unloaded event, which can be \"any\" unloaded event.  \r\nFix: Clear the timer so it does not fire when reusing runners.  Only assign the timer when the runner has finished.  Clear any assigned timers when closing runners.  An active runner should not have an expire timer.  ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Fix relative path lookup",
            "url": "https://github.com/ollama/ollama/pull/4035",
            "state": "MERGED",
            "createdAt": "2024-04-29T23:01:00Z",
            "mergedAt": "2024-04-29T23:08:06Z",
            "closedAt": "2024-04-29T23:08:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp",
            "url": "https://github.com/ollama/ollama/pull/4036",
            "state": "MERGED",
            "createdAt": "2024-04-29T23:10:57Z",
            "mergedAt": "2024-04-30T03:18:48Z",
            "closedAt": "2024-04-30T03:18:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 5,
            "body": "Bump llama.cpp to b2761",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update langchainpy.md",
            "url": "https://github.com/ollama/ollama/pull/4037",
            "state": "MERGED",
            "createdAt": "2024-04-30T01:02:01Z",
            "mergedAt": "2024-04-30T03:19:06Z",
            "closedAt": "2024-04-30T03:19:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 4,
            "body": "Updated the code a bit since it was showing deprecation messages for me.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: reduce Name.Filepath allocs from 5 to 2",
            "url": "https://github.com/ollama/ollama/pull/4039",
            "state": "MERGED",
            "createdAt": "2024-04-30T05:14:34Z",
            "mergedAt": "2024-04-30T18:09:19Z",
            "closedAt": "2024-04-30T18:09:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 21,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: add Guix package manager in README.",
            "url": "https://github.com/ollama/ollama/pull/4040",
            "state": "MERGED",
            "createdAt": "2024-04-30T07:40:33Z",
            "mergedAt": "2024-05-09T18:10:24Z",
            "closedAt": "2024-05-09T18:10:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: store accurate model parameter size",
            "url": "https://github.com/ollama/ollama/pull/4058",
            "state": "MERGED",
            "createdAt": "2024-04-30T18:43:29Z",
            "mergedAt": "2024-05-07T21:41:54Z",
            "closedAt": "2024-05-07T21:41:54Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 48,
            "deletions": 6,
            "body": "- add test for number formatting\r\n- fix bug where 1B and 1M were not stored correctly\r\n- display 2 decimal points for million param sizes\r\n- display 1 decimal point for billion param sizes\r\n\r\nThis human conversion is displayed as the parameter size on ollama.com, so it should be in the standard format that model parameter sizes are measured.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "rename parser to model/file",
            "url": "https://github.com/ollama/ollama/pull/4059",
            "state": "MERGED",
            "createdAt": "2024-04-30T18:57:35Z",
            "mergedAt": "2024-05-03T20:01:22Z",
            "closedAt": "2024-05-03T20:01:22Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 6
            },
            "additions": 151,
            "deletions": 124,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: reintroduce Digest",
            "url": "https://github.com/ollama/ollama/pull/4065",
            "state": "MERGED",
            "createdAt": "2024-04-30T22:53:16Z",
            "mergedAt": "2024-04-30T23:38:03Z",
            "closedAt": "2024-04-30T23:38:03Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 89,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add CUDA Driver API for GPU discovery",
            "url": "https://github.com/ollama/ollama/pull/4067",
            "state": "MERGED",
            "createdAt": "2024-04-30T23:44:18Z",
            "mergedAt": "2024-05-06T20:30:27Z",
            "closedAt": "2024-05-06T20:30:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 342,
            "deletions": 5,
            "body": "We're seeing some corner cases with cudart which might be resolved by switching to the driver API which comes bundled with the driver package.\r\n\r\n\r\nI've verified this works on Windows, linux x86 host and container, and linux arm (jetson)\r\n\r\nFixes #4008 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "metal: add 512MiB to minimum memory, no partial offloading overhead",
            "url": "https://github.com/ollama/ollama/pull/4068",
            "state": "MERGED",
            "createdAt": "2024-05-01T01:56:07Z",
            "mergedAt": "2024-05-01T15:46:03Z",
            "closedAt": "2024-05-01T15:46:03Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chore: fix typo in docs/development.md",
            "url": "https://github.com/ollama/ollama/pull/4073",
            "state": "MERGED",
            "createdAt": "2024-05-01T09:47:42Z",
            "mergedAt": "2024-05-01T19:39:11Z",
            "closedAt": "2024-05-01T19:39:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "fix typo when reading docs",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update readme description",
            "url": "https://github.com/ollama/ollama/pull/4083",
            "state": "CLOSED",
            "createdAt": "2024-05-01T14:45:17Z",
            "mergedAt": null,
            "closedAt": "2024-05-09T22:01:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "add some introduction about the project",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add instructions to easily install specific versions on faq.md",
            "url": "https://github.com/ollama/ollama/pull/4084",
            "state": "MERGED",
            "createdAt": "2024-05-01T15:09:37Z",
            "mergedAt": "2024-06-09T17:49:04Z",
            "closedAt": "2024-06-09T17:49:04Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "This PR adds a new question to faq.md on how to install specific versions of ollama using `VER_PARAM` env variable. \r\n\r\nMight be useful when trying to install release candidates versions or switching between versions easily, and I could not find any info on how to do that in the current docs.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Use LF for line endings",
            "url": "https://github.com/ollama/ollama/pull/4085",
            "state": "MERGED",
            "createdAt": "2024-05-01T18:42:28Z",
            "mergedAt": "2024-05-01T19:02:45Z",
            "closedAt": "2024-05-01T19:02:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "This fixes builds using Docker for Windows",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add preflight OPTIONS handling and update CORS config",
            "url": "https://github.com/ollama/ollama/pull/4086",
            "state": "MERGED",
            "createdAt": "2024-05-01T19:14:45Z",
            "mergedAt": "2024-05-08T20:14:00Z",
            "closedAt": "2024-05-08T20:14:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "Couple of tweaks to our CORS configuration and how we handle `OPTIONS` requests. This update is geared towards making our service more compatible with clients originally designed to work with OpenAI, where sending an `Authorization` header is common.\r\n\r\n#### Details of Changes\r\n1. **Handling OPTIONS Requests**: I added a quick return for `OPTIONS` requests in our `allowedHostsMiddleware`. This means we're now ending these preflight requests with a 204 (No Content) status right off the bat.\r\n\r\n2. **Updating CORS for Authorization Headers**: Since some of the Ollama clients automatically send an `Authorization` header (because they're set up for OpenAI), I've updated our CORS config to accept these headers. This is needed for making sure these clients can interact with our service without hitting CORS.\r\n\r\n####  Security\r\nSince we're not currently using the `Authorization` header for our own authentication, allowing this header doesn't open us up to new security risks as long as we don't have auth. \r\n\r\nEnabling the `OPTIONS` method is mainly about letting browsers do their preflight check when they see that `Authorization` header. It's pretty standard and doesn't pose a direct risk by itself as far as I am aware.\r\n\r\nresolves #4001\r\nresolves #3983 \r\nresolves https://github.com/ollama/ollama-js/issues/80",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "types/model: fix name for hostport",
            "url": "https://github.com/ollama/ollama/pull/4087",
            "state": "MERGED",
            "createdAt": "2024-05-01T19:15:25Z",
            "mergedAt": "2024-05-01T19:42:07Z",
            "closedAt": "2024-05-01T19:42:07Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 55,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: destination invalid",
            "url": "https://github.com/ollama/ollama/pull/4089",
            "state": "MERGED",
            "createdAt": "2024-05-01T19:39:17Z",
            "mergedAt": "2024-05-01T19:46:35Z",
            "closedAt": "2024-05-01T19:46:35Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 7,
            "body": "update cmd help too",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Support Fedoras standard ROCm location",
            "url": "https://github.com/ollama/ollama/pull/4090",
            "state": "MERGED",
            "createdAt": "2024-05-01T22:48:10Z",
            "mergedAt": "2024-05-06T21:33:41Z",
            "closedAt": "2024-05-06T21:33:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 10,
            "deletions": 8,
            "body": "Fixes #3877 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix line ending",
            "url": "https://github.com/ollama/ollama/pull/4108",
            "state": "MERGED",
            "createdAt": "2024-05-02T21:54:18Z",
            "mergedAt": "2024-05-02T21:55:15Z",
            "closedAt": "2024-05-02T21:55:15Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 47,
            "deletions": 47,
            "body": "replace CRLF with LF\r\n\r\nCRLF leaves the file in a perpetually dirty state on non-windows systems without a way to reset",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add information on model tagging to `import.md`.",
            "url": "https://github.com/ollama/ollama/pull/4109",
            "state": "CLOSED",
            "createdAt": "2024-05-02T23:17:24Z",
            "mergedAt": null,
            "closedAt": "2024-06-08T23:32:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 1,
            "body": "I added a section to the import documentation on model tagging and cleaned up the quantization reference section a smidge.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/4111",
            "state": "MERGED",
            "createdAt": "2024-05-03T00:33:51Z",
            "mergedAt": "2024-05-05T21:45:32Z",
            "closedAt": "2024-05-05T21:45:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "Includes a proxy plugin for ollama to work like github copilot.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update 'llama2' -> 'llama3' in most places",
            "url": "https://github.com/ollama/ollama/pull/4116",
            "state": "MERGED",
            "createdAt": "2024-05-03T03:12:22Z",
            "mergedAt": "2024-05-03T19:25:04Z",
            "closedAt": "2024-05-03T19:25:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 21
            },
            "additions": 94,
            "deletions": 102,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "\ud83d\udc4c IMPROVE: add portkey library for production tools",
            "url": "https://github.com/ollama/ollama/pull/4119",
            "state": "MERGED",
            "createdAt": "2024-05-03T07:02:22Z",
            "mergedAt": "2024-05-06T17:25:23Z",
            "closedAt": "2024-05-06T17:25:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: add support for flash_attn",
            "url": "https://github.com/ollama/ollama/pull/4120",
            "state": "MERGED",
            "createdAt": "2024-05-03T07:28:07Z",
            "mergedAt": "2024-05-20T20:36:03Z",
            "closedAt": "2024-05-20T20:36:03Z",
            "reviews": {
                "totalCount": 27
            },
            "files": {
                "totalCount": 2
            },
            "additions": 28,
            "deletions": 3,
            "body": "- Add Flash Attention support #4051\r\n\r\nOnly enabled by default on a supported CUDA version or Metal is detected, configurable via params and the API.\r\n\r\nCredit to @wanderingmeow who [took my broken idea and made it work](https://github.com/ollama/ollama/issues/4051#issuecomment-2092430887) \ud83c\udf89 \r\n\r\nFixes #4051",
            "participants": {
                "totalCount": 9
            },
            "comments": {
                "totalCount": 34
            }
        }
    },
    {
        "node": {
            "title": "Feat: Add `OLLAMA_LOAD_TIMEOUT` env variable",
            "url": "https://github.com/ollama/ollama/pull/4123",
            "state": "CLOSED",
            "createdAt": "2024-05-03T09:47:50Z",
            "mergedAt": null,
            "closedAt": "2024-05-23T21:10:59Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 14,
            "deletions": 1,
            "body": "Closes #3940 \r\n\r\nFor certain hardware setups and models, the offloading to the GPU can take a lot of time and the user can hit a timeout. This PR makes the timeout configurable via the `OLLAMA_LOAD_TIMEOUT` env variable, to be provided in seconds.\r\n\r\n@dhiltgen I added a subsection in the FAQ, since I was not sure where to document the env variable. Let me know if this is the right place.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Soften timeouts on sched unit tests",
            "url": "https://github.com/ollama/ollama/pull/4129",
            "state": "MERGED",
            "createdAt": "2024-05-03T16:09:07Z",
            "mergedAt": "2024-05-03T18:10:26Z",
            "closedAt": "2024-05-03T18:10:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 9,
            "body": "This gives us more headroom on the scheduler tests to tamp down some flakes.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Skip PhysX cudart library",
            "url": "https://github.com/ollama/ollama/pull/4135",
            "state": "MERGED",
            "createdAt": "2024-05-03T18:56:55Z",
            "mergedAt": "2024-05-06T20:34:00Z",
            "closedAt": "2024-05-06T20:34:00Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "For some reason this library gives incorrect GPU information, so skip it\r\n\r\n\r\nI'm not convinced yet this is the optimal fix, but queuing this up in case we get ready to cut a new release and haven't found a better solution yet.\r\n\r\nFixes #4008 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Explain the 2 different windows download options",
            "url": "https://github.com/ollama/ollama/pull/4141",
            "state": "MERGED",
            "createdAt": "2024-05-03T21:10:51Z",
            "mergedAt": "2024-05-04T19:50:16Z",
            "closedAt": "2024-05-04T19:50:16Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 0,
            "body": "I've confirmed that the service is functional when run under nssm.\r\n\r\nFixes #4047 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "omit prompt and generate settings from final response",
            "url": "https://github.com/ollama/ollama/pull/4143",
            "state": "MERGED",
            "createdAt": "2024-05-03T23:22:38Z",
            "mergedAt": "2024-05-04T00:39:49Z",
            "closedAt": "2024-05-04T00:39:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 2,
            "body": "if the input is large, it might overrun the response buffer. there's no need to return the prompt since the caller has it already",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Make maximum pending request configurable",
            "url": "https://github.com/ollama/ollama/pull/4144",
            "state": "MERGED",
            "createdAt": "2024-05-03T23:37:51Z",
            "mergedAt": "2024-05-05T17:53:44Z",
            "closedAt": "2024-05-05T17:53:44Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 154,
            "deletions": 23,
            "body": "Bump the maximum queued requests to 512 (from 10)\r\nMake it configurable with a new env var `OLLAMA_MAX_QUEUE`\r\nReturn a 503 when the server is too busy instead of more generic 500.\r\n\r\nFixes #4124 \r\n\r\nWith the added integration test, here are some quick memory stats on linux:\r\n- Just starting ollama RSS 429.0m\r\n- Load orca-mini: RSS 456.8m. (just the Go process, not the child runner)\r\n- During my stress test where I push >512 connections: RSS 489.0m\r\n\r\n\r\n\r\n\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix lint warnings",
            "url": "https://github.com/ollama/ollama/pull/4145",
            "state": "MERGED",
            "createdAt": "2024-05-03T23:44:35Z",
            "mergedAt": "2024-05-03T23:53:17Z",
            "closedAt": "2024-05-03T23:53:17Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: format go code",
            "url": "https://github.com/ollama/ollama/pull/4149",
            "state": "MERGED",
            "createdAt": "2024-05-04T09:36:38Z",
            "mergedAt": "2024-05-05T23:08:09Z",
            "closedAt": "2024-05-05T23:08:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 73,
            "deletions": 73,
            "body": "Hi, I find some go code is not formatted, So I run `gofmt -w .` to format them. ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add GPU usage",
            "url": "https://github.com/ollama/ollama/pull/4153",
            "state": "MERGED",
            "createdAt": "2024-05-04T17:10:08Z",
            "mergedAt": "2024-05-08T23:39:11Z",
            "closedAt": "2024-05-08T23:39:11Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 40,
            "deletions": 20,
            "body": "Help users understand how much of the model fit into their GPU without having to resort to inspecting the server log\r\n\r\nA few examples from different systems and models\r\n\r\n```\r\neval rate:            4.40 tokens/s\r\ngpu usage:            1 GPU (14/27 layers) 3.2 GB (2.0 GB GPU)\r\n\r\neval rate:            6.64 tokens/s\r\ngpu usage:            1 GPU (27/27 layers) 3.2 GB\r\n\r\neval rate:            18.44 tokens/s\r\ngpu usage:            2 GPUs (27/33 layers) 27 GB (24 GB GPU)\r\n\r\neval rate:            19.58 tokens/s\r\ngpu usage:            CPU (0/27 layers) 3.2 GB\r\n```\r\n\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Centralize server config handling",
            "url": "https://github.com/ollama/ollama/pull/4154",
            "state": "MERGED",
            "createdAt": "2024-05-04T18:48:27Z",
            "mergedAt": "2024-05-06T00:08:26Z",
            "closedAt": "2024-05-06T00:08:26Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 12
            },
            "additions": 235,
            "deletions": 162,
            "body": "This moves all the env var reading into one central module and logs the loaded config once at startup which should help in troubleshooting user server logs\r\n\r\nExample server output\r\n```\r\n% ollama serve 2>&1 | head -1\r\n2024/05/05 14:56:27 routes.go:989: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix `no slots available` error with concurrent requests",
            "url": "https://github.com/ollama/ollama/pull/4160",
            "state": "MERGED",
            "createdAt": "2024-05-05T02:43:34Z",
            "mergedAt": "2024-05-06T21:22:53Z",
            "closedAt": "2024-05-06T21:22:53Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 115,
            "deletions": 112,
            "body": "This fixes a few issues with queuing requests:\r\n\r\n- Always wait for the server.cpp ready state to avoid \"server busy\" errors this should fix `no slots available`\r\n\r\nFixes #4159",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Allocate a large enough kv cache for all parallel requests",
            "url": "https://github.com/ollama/ollama/pull/4162",
            "state": "MERGED",
            "createdAt": "2024-05-05T04:43:06Z",
            "mergedAt": "2024-05-05T22:59:32Z",
            "closedAt": "2024-05-05T22:59:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "This fixes `opts.NumCtx` being assigned correctly",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix llava models not working after first request",
            "url": "https://github.com/ollama/ollama/pull/4164",
            "state": "MERGED",
            "createdAt": "2024-05-05T07:42:46Z",
            "mergedAt": "2024-05-06T03:50:31Z",
            "closedAt": "2024-05-06T03:50:31Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 32,
            "deletions": 1,
            "body": "Temporary fix for https://github.com/ollama/ollama/issues/4163, although a bit more work is required to understand the root cause",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update libraries for langchain_community + llama3 changed from llama2",
            "url": "https://github.com/ollama/ollama/pull/4174",
            "state": "MERGED",
            "createdAt": "2024-05-05T16:46:34Z",
            "mergedAt": "2024-05-05T23:07:04Z",
            "closedAt": "2024-05-05T23:07:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "Changed:\r\n- run ->invoke for updated lib \r\n- updated langchain libraries for non-depreciated\r\n- updated llama2 to llama3",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "validate the format of the digest when getting the model path",
            "url": "https://github.com/ollama/ollama/pull/4175",
            "state": "MERGED",
            "createdAt": "2024-05-05T18:36:35Z",
            "mergedAt": "2024-05-05T18:46:12Z",
            "closedAt": "2024-05-05T18:46:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 84,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix rare nil pointer dereference when model unloads",
            "url": "https://github.com/ollama/ollama/pull/4187",
            "state": "MERGED",
            "createdAt": "2024-05-06T00:04:36Z",
            "mergedAt": "2024-05-06T00:18:27Z",
            "closedAt": "2024-05-06T00:18:27Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 10,
            "body": "While testing concurrency I noticed a segfault happen occasionally when loading, canceling, and loading the same model repeatedly over and over again with a script like this:\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# Command to run\r\nCOMMAND=\"ollama run llama3 hello\"\r\n\r\n# Number of times to run the command concurrently\r\nN=100\r\n\r\n# Running the command N times concurrently\r\nfor i in $(seq 1 $N); do\r\n    $COMMAND &\r\ndone\r\n```\r\n\r\nThe error looked like this:\r\n\r\n\r\n```\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x101298ae0]\r\n\r\ngoroutine 51 [running]:\r\ngithub.com/ollama/ollama/server.(*runnerRef).needsReload(0x14000ffc5a0, {0x1018b17a0, 0x1400017d8b0}, 0x140003e20f0)\r\n\t/Users/jmorgan/git/ollama/server/sched.go:472 +0x150\r\ngithub.com/ollama/ollama/server.(*Scheduler).processPending(0x1400017d900, {0x1018b17a0, 0x1400017d8b0})\r\n\t/Users/jmorgan/git/ollama/server/sched.go:143 +0x3d0\r\ngithub.com/ollama/ollama/server.(*Scheduler).Run.func1()\r\n\t/Users/jmorgan/git/ollama/server/sched.go:120 +0x28\r\ncreated by github.com/ollama/ollama/server.(*Scheduler).Run in goroutine 1\r\n\t/Users/jmorgan/git/ollama/server/sched.go:119 +0xc4\r\n```\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "User our bundled libraries (cuda) instead of the host library",
            "url": "https://github.com/ollama/ollama/pull/4188",
            "state": "MERGED",
            "createdAt": "2024-05-06T00:47:21Z",
            "mergedAt": "2024-05-06T21:41:05Z",
            "closedAt": "2024-05-06T21:41:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 31,
            "deletions": 15,
            "body": "Trying to live off the land for cuda libraries was not the right strategy.  We need to use the version we compiled against to ensure things work properly.\r\n\r\nThis is most likely going to break Jetson v11 systems, but it turns out the change to favor host cuda libraries is breaking\r\nquite a few users.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Skip scheduling cancelled requests and always reload if unloading",
            "url": "https://github.com/ollama/ollama/pull/4189",
            "state": "MERGED",
            "createdAt": "2024-05-06T02:09:53Z",
            "mergedAt": "2024-05-06T21:22:25Z",
            "closedAt": "2024-05-06T21:22:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 0,
            "body": "This should fix the issue seen in https://github.com/ollama/ollama/pull/4187. The issue was caused by a new request coming in for the same model _while_ it was being unloaded. I do wonder if there's a better approach here: if we know another request is coming in for a model we're loading, we shouldn't unload it on a context cancel since we'll need it anyways.\r\n\r\nThis change also discards any incoming requests that have already been cancelled so they aren't scheduled\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix golangci workflow not enable gofmt and goimports",
            "url": "https://github.com/ollama/ollama/pull/4190",
            "state": "MERGED",
            "createdAt": "2024-05-06T02:18:23Z",
            "mergedAt": "2024-05-07T16:49:40Z",
            "closedAt": "2024-05-07T16:49:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 4,
            "body": "Hi, this pr tries to fix golangci not enable `gofmt` and `goimport` in github workflows.\r\n\r\nAll workflows have passed. But I notice `* text eol=lf`  was removed in commit [9164b0161](https://github.com/ollama/ollama/commit/9164b0161bcb24e543cba835a8863b80af2c0c21) which v0.1.33 was released. So I still need help from maintainers because `.github/workflows/release.yaml` is not tested. By now I can not trigger release workflow in my own repo https://github.com/alwqx/ollama because `.github/workflows/release.yaml` references some environments which I don't have or I don't known how to get my own value:\r\n- MACOS_SIGNING_KEY\r\n- MACOS_SIGNING_KEY_PASSWORD\r\n- APPLE_IDENTITY\r\n- APPLE_PASSWORD\r\n- APPLE_ID\r\n- KEY_CONTAINER\r\n- GOOGLE_SIGNING_CREDENTIALS\r\n- ...\r\n\r\nI hope maintainer/collaborators help test release workflow for this change.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: delete log `HEAD`",
            "url": "https://github.com/ollama/ollama/pull/4194",
            "state": "MERGED",
            "createdAt": "2024-05-06T05:28:47Z",
            "mergedAt": "2024-05-06T17:32:31Z",
            "closedAt": "2024-05-06T17:32:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix stale test logic",
            "url": "https://github.com/ollama/ollama/pull/4208",
            "state": "MERGED",
            "createdAt": "2024-05-06T21:16:57Z",
            "mergedAt": "2024-05-06T21:23:12Z",
            "closedAt": "2024-05-06T21:23:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 4,
            "body": "The model processing was recently changed to be deferred but this test scenario hadn't been adjusted for that change in behavior.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Close server on receiving signal",
            "url": "https://github.com/ollama/ollama/pull/4213",
            "state": "MERGED",
            "createdAt": "2024-05-06T22:51:37Z",
            "mergedAt": "2024-05-06T23:01:37Z",
            "closedAt": "2024-05-06T23:01:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: add minimum based on layer size",
            "url": "https://github.com/ollama/ollama/pull/4215",
            "state": "MERGED",
            "createdAt": "2024-05-07T00:05:47Z",
            "mergedAt": "2024-05-07T16:26:33Z",
            "closedAt": "2024-05-07T16:26:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 7,
            "deletions": 7,
            "body": "adjust minimum memory requirements based on the model being loaded and reduce the static minimum",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Enable concurrency by default",
            "url": "https://github.com/ollama/ollama/pull/4218",
            "state": "MERGED",
            "createdAt": "2024-05-07T01:00:28Z",
            "mergedAt": "2024-07-01T15:32:29Z",
            "closedAt": "2024-07-01T15:32:29Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 7
            },
            "additions": 175,
            "deletions": 73,
            "body": "This adjusts our default settings to enable multiple models and parallel requests to a single model.  Users can still override these by the same env var settings as before.  Parallel has a direct impact on num_ctx, which in turn can have a significant impact on small VRAM GPUs so this change also refines the algorithm so that when parallel is not explicitly set by the user, we try to find a reasonable default that fits the model on their GPU(s).  As before, multiple models will only load concurrently if they fully fit in VRAM.\r\n\r\nCorresponding Doc update to merge after this #5364 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: fix parser for empty values",
            "url": "https://github.com/ollama/ollama/pull/4231",
            "state": "MERGED",
            "createdAt": "2024-05-07T17:00:59Z",
            "mergedAt": "2024-05-07T17:48:32Z",
            "closedAt": "2024-05-07T17:48:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 4,
            "body": "empty values were incorrectly seen as `\\n` after parsing, formatting, and reparsing",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add done_reason to the api",
            "url": "https://github.com/ollama/ollama/pull/4235",
            "state": "MERGED",
            "createdAt": "2024-05-07T18:50:12Z",
            "mergedAt": "2024-05-09T20:30:14Z",
            "closedAt": "2024-05-09T20:30:14Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 44,
            "deletions": 40,
            "body": "When generating content using the chat or generate endpoints it is useful to know the reason the LLM stopped generating.\r\n\r\nThis may be due to 3 reasons currently in our API:\r\n- \"stop\" - The generation hit a stop token.\r\n- \"length\" - The maximum `num_tokens` was reached.\r\n- \"load\" - The request was sent with an empty body to load the model.\r\n\r\nThis change proposes a new `done_reason` parameter on the chat and generate responses.\r\n\r\nMoving forward this change help us give more information to the user. For example we can add timeouts on requests to prevent the server from hanging.\r\n\r\nFollow-up:\r\n- update docs\r\n- update javascript and python client libraries\r\n\r\nRelated: #4230",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Record more GPU information",
            "url": "https://github.com/ollama/ollama/pull/4238",
            "state": "MERGED",
            "createdAt": "2024-05-07T22:01:16Z",
            "mergedAt": "2024-05-09T21:26:58Z",
            "closedAt": "2024-05-09T21:26:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 10
            },
            "additions": 150,
            "deletions": 96,
            "body": "This cleans up the logging for GPU discovery a bit, and can serve as a foundation to report GPU information in a future UX.\r\n\r\nSome example output (without OLLAMA_DEBUG set)\r\n\r\nWindows Cuda:\r\n```\r\ntime=2024-05-07T14:55:04.202-07:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11.3 cuda_v12.3 rocm_v5.7]\"\r\ntime=2024-05-07T14:55:04.310-07:00 level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\r\ntime=2024-05-07T14:55:04.572-07:00 level=INFO source=amd_windows.go:90 msg=\"unsupported Radeon iGPU detected skipping\" id=0 name=\"AMD Radeon(TM) Graphics\" gfx=gfx1036\r\ntime=2024-05-07T14:55:04.635-07:00 level=INFO source=gpu.go:246 msg=\"inference compute\" id=GPU-13b3d4ff-808b-ab50-e395-de65e58aa716 library=cuda compute=8.9 driver=12.3 name=\"NVIDIA GeForce RTX 4090\" total=\"24563.5 MiB\" available=\"23008.0 MiB\"\r\n```\r\n\r\nWindows Radeon:\r\n```\r\ntime=2024-05-07T15:00:22.839-07:00 level=INFO source=amd_windows.go:90 msg=\"unsupported Radeon iGPU detected skipping\" id=0 name=\"AMD Radeon(TM) Graphics\" gfx=gfx1036\r\ntime=2024-05-07T15:00:23.082-07:00 level=INFO source=gpu.go:246 msg=\"inference compute\" id=1 library=rocm compute=gfx1100 driver=0.0 name=\"AMD Radeon RX 7900 XTX\" total=\"24560.0 MiB\" available=\"24432.0 MiB\"\r\n```\r\n\r\nLinux Radeon mismatch gfx without override\r\n```\r\ntime=2024-05-07T21:56:20.080Z level=WARN source=amd_linux.go:290 msg=\"amdgpu is not supported\" gpu=0 gpu_type=gfx1034 library=/usr/share/ollama/lib/rocm supported_types=\"[gfx1030 gfx1100 gfx1101 gfx1102 gfx900 gfx906 gfx908 gfx90a gfx940 gfx941 gfx942]\"\r\ntime=2024-05-07T21:56:20.080Z level=WARN source=amd_linux.go:292 msg=\"See https://github.com/ollama/ollama/blob/main/docs/gpu.md#overrides for HSA_OVERRIDE_GFX_VERSION usage\"\r\ntime=2024-05-07T21:56:20.080Z level=INFO source=amd_linux.go:305 msg=\"no compatible amdgpu devices detected\"\r\ntime=2024-05-07T21:56:20.080Z level=INFO source=gpu.go:246 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"32051.6 MiB\" available=\"271.3 MiB\"\r\n```\r\n\r\nSame system with the override set\r\n```\r\ntime=2024-05-07T21:57:08.855Z level=INFO source=amd_linux.go:298 msg=\"skipping rocm gfx compatibility check\" HSA_OVERRIDE_GFX_VERSION=10.3.0\r\ntime=2024-05-07T21:57:08.855Z level=INFO source=gpu.go:246 msg=\"inference compute\" id=0 library=rocm compute=gfx1034 driver=6.3 name=1002:743f total=\"4080.0 MiB\" available=\"1102.9 MiB\"\r\n```\r\n\r\nDual CUDA setup:\r\n```\r\ntime=2024-05-07T21:51:19.894Z level=INFO source=gpu.go:246 msg=\"inference compute\" id=GPU-19fc4f1e-fbcc-de33-f14a-ae21199420b6 library=cuda compute=8.6 driver=12.4 name=\"NVIDIA GeForce RTX 3060\" total=\"12030.6 MiB\" available=\"11922.6 MiB\"\r\ntime=2024-05-07T21:51:19.894Z level=INFO source=gpu.go:246 msg=\"inference compute\" id=GPU-f3a94ab8-b31d-61ff-9fbb-ce91ac1cdd95 library=cuda compute=8.6 driver=12.4 name=\"NVIDIA GeForce RTX 3060\" total=\"12037.4 MiB\" available=\"8897.4 MiB\"\r\n```\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect noexec and report a better error",
            "url": "https://github.com/ollama/ollama/pull/4241",
            "state": "MERGED",
            "createdAt": "2024-05-07T23:49:02Z",
            "mergedAt": "2024-05-08T22:34:22Z",
            "closedAt": "2024-05-08T22:34:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 0,
            "body": "This will bubble up a much more informative error message if noexec is preventing us from running the subprocess\r\n\r\nFixes #4105 \r\n\r\nRepro scenario:\r\n```\r\nsudo mkdir /tmp2\r\nsudo mount -t tmpfs -o size=1g,noexec tmpfs /tmp2\r\n\r\nOLLAMA_TMPDIR=/tmp2/test1 ollama serve\r\n...\r\ntime=2024-05-07T23:44:54.304Z level=INFO source=sched.go:308 msg=\"NewLlamaServer failed\" model=/home/daniel/.ollama/models/blobs/sha256-66002b78c70a22ab25e16cc9a1736c6cc6335398c7312e3eb33db202350afe66 error=\"unable to start server fork/exec /tmp2/test1/runners/cpu_avx/ollama_llama_server: permission denied.  /tmp2/test1/runners/cpu_avx may have noexec set.  Set OLLAMA_TMPDIR for server to a writable executable directory\"\r\n```\r\n\r\nOn the client that triggered the failure:\r\n```\r\n% ollama run orca-mini hello\r\nError: unable to start server fork/exec /tmp2/test1/runners/cpu_avx/ollama_llama_server: permission denied.  /tmp2/test1/runners/cpu_avx may have noexec set.  Set OLLAMA_TMPDIR for server to a writable executable directory\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "skip if same quantization",
            "url": "https://github.com/ollama/ollama/pull/4244",
            "state": "MERGED",
            "createdAt": "2024-05-08T00:44:44Z",
            "mergedAt": "2024-05-08T02:03:38Z",
            "closedAt": "2024-05-08T02:03:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 30,
            "deletions": 30,
            "body": "this skips quantization of the input and output are the same file types. most of the time, this means if the input and output are both f16",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "skip hidden files in list models handler",
            "url": "https://github.com/ollama/ollama/pull/4247",
            "state": "MERGED",
            "createdAt": "2024-05-08T01:34:13Z",
            "mergedAt": "2024-05-08T02:01:45Z",
            "closedAt": "2024-05-08T02:01:45Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "Hidden files on MacOS (ex: `.DS_Store`) cause the list command to file when not skipped:\r\n```\r\n\u276f ollama ls\r\nError: unqualified name: \r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: fix tag case",
            "url": "https://github.com/ollama/ollama/pull/4261",
            "state": "MERGED",
            "createdAt": "2024-05-08T15:47:42Z",
            "mergedAt": "2024-05-08T18:09:48Z",
            "closedAt": "2024-05-08T18:09:48Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 7,
            "body": "tag is case sensitive so it's incorrect to change it to lower case\r\n\r\nresolves #4250",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Centralize GPU configuration vars",
            "url": "https://github.com/ollama/ollama/pull/4264",
            "state": "MERGED",
            "createdAt": "2024-05-08T19:35:08Z",
            "mergedAt": "2024-06-15T14:33:52Z",
            "closedAt": "2024-06-15T14:33:52Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 3
            },
            "additions": 33,
            "deletions": 6,
            "body": "This should aid in troubleshooting by capturing and reporting the GPU settings at startup in the logs along with all the other server settings.\r\n\r\nFixes #4139 \r\n\r\nExample output setting the ROCm gfx override:\r\n```\r\n2024/05/08 19:33:27 routes.go:993: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION:10.3.0 OLLAMA_DEBUG:true OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "routes: fix show llava models",
            "url": "https://github.com/ollama/ollama/pull/4265",
            "state": "MERGED",
            "createdAt": "2024-05-08T19:43:19Z",
            "mergedAt": "2024-05-08T19:51:21Z",
            "closedAt": "2024-05-08T19:51:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 12,
            "body": "show model file isn't showing the projector because it's set to name `projector` instead of `model`\r\n\r\nalso change the order so adapters/projectors appear ahead of template/system to group with the language model",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Convert directly from llama3",
            "url": "https://github.com/ollama/ollama/pull/4268",
            "state": "MERGED",
            "createdAt": "2024-05-08T23:14:20Z",
            "mergedAt": "2024-05-21T21:43:37Z",
            "closedAt": "2024-05-21T21:43:37Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 12
            },
            "additions": 437,
            "deletions": 306,
            "body": "This change allows you to convert directly from a llama3 derived safetensors model into Ollama.\r\n\r\nIt is currently *missing*:\r\n* pytorch *almost* works however the embeddings layer size is off by the eos/bos tokens\r\n\r\nThis *will* work with most llama3 derivatives if they are using safetensors including `dolphin-2.9-llama3`, nous research's hermes 2 pro, and nvidia's chatqa.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Prune partial downloads",
            "url": "https://github.com/ollama/ollama/pull/4272",
            "state": "MERGED",
            "createdAt": "2024-05-09T01:56:19Z",
            "mergedAt": "2024-05-09T23:35:20Z",
            "closedAt": "2024-05-09T23:35:20Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 21,
            "deletions": 17,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Doc container usage and workaround for nvidia errors",
            "url": "https://github.com/ollama/ollama/pull/4289",
            "state": "MERGED",
            "createdAt": "2024-05-09T15:52:08Z",
            "mergedAt": "2024-05-09T16:27:30Z",
            "closedAt": "2024-05-09T16:27:30Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 92,
            "deletions": 2,
            "body": "Fixes #4242 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Harden subprocess reaping",
            "url": "https://github.com/ollama/ollama/pull/4294",
            "state": "MERGED",
            "createdAt": "2024-05-09T18:16:40Z",
            "mergedAt": "2024-05-09T21:02:16Z",
            "closedAt": "2024-05-09T21:02:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 65,
            "deletions": 59,
            "body": "This carries PR #3702 \r\n\r\nChanges for concurrency to wait for the subprocess to exit inadvertently regressed reaping if we crash during model load.  This fixes that regression and refines the logging around subprocess startup.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "routes: skip invalid filepaths",
            "url": "https://github.com/ollama/ollama/pull/4295",
            "state": "MERGED",
            "createdAt": "2024-05-09T18:24:05Z",
            "mergedAt": "2024-05-09T18:37:34Z",
            "closedAt": "2024-05-09T18:37:34Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "do not block list output",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "log clean up",
            "url": "https://github.com/ollama/ollama/pull/4298",
            "state": "MERGED",
            "createdAt": "2024-05-09T21:53:11Z",
            "mergedAt": "2024-05-09T23:20:57Z",
            "closedAt": "2024-05-09T23:20:57Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 30,
            "deletions": 34,
            "body": "any debug logs are available with `--verbose` which was previously a noop since it's only set  if also compiled with `SERVER_VERBOSE`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Wait for GPU free memory reporting to converge",
            "url": "https://github.com/ollama/ollama/pull/4299",
            "state": "MERGED",
            "createdAt": "2024-05-09T21:56:11Z",
            "mergedAt": "2024-05-09T22:08:57Z",
            "closedAt": "2024-05-09T22:08:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 61,
            "deletions": 3,
            "body": "The GPU drivers take a while to update their free memory reporting, so we need to wait until the values converge with what we're expecting before proceeding to start another runner in order to get an accurate picture.\r\n\r\nPrior to this fix, this can manifest as loading less layers than expected on a subsequent load and seeing slow inference, or worst case, toggling back and forth between GPU and CPU.\r\n\r\nFixes #4253",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add LlamaScript to Community Projects",
            "url": "https://github.com/ollama/ollama/pull/4300",
            "state": "MERGED",
            "createdAt": "2024-05-09T21:58:54Z",
            "mergedAt": "2024-05-09T22:30:49Z",
            "closedAt": "2024-05-09T22:30:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Pull Request for\r\n#4061 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "only forward some env vars",
            "url": "https://github.com/ollama/ollama/pull/4302",
            "state": "MERGED",
            "createdAt": "2024-05-09T22:13:01Z",
            "mergedAt": "2024-05-09T23:21:05Z",
            "closedAt": "2024-05-09T23:21:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 20,
            "body": "only forward select env vars which prevents 1) logging and 2) the subprocess inheriting irrelevant, possibly sensitive, vars",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix race in shutdown logic",
            "url": "https://github.com/ollama/ollama/pull/4304",
            "state": "MERGED",
            "createdAt": "2024-05-09T22:48:49Z",
            "mergedAt": "2024-05-09T22:58:45Z",
            "closedAt": "2024-05-09T22:58:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 5,
            "body": "Ensure the runners are terminated\r\n\r\nFixes #4267 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix typo",
            "url": "https://github.com/ollama/ollama/pull/4305",
            "state": "MERGED",
            "createdAt": "2024-05-09T23:23:48Z",
            "mergedAt": "2024-05-09T23:42:10Z",
            "closedAt": "2024-05-09T23:42:10Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(routes): skip bad manifests",
            "url": "https://github.com/ollama/ollama/pull/4306",
            "state": "MERGED",
            "createdAt": "2024-05-10T00:45:27Z",
            "mergedAt": "2024-05-10T15:58:16Z",
            "closedAt": "2024-05-10T15:58:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump VRAM buffer back up",
            "url": "https://github.com/ollama/ollama/pull/4316",
            "state": "MERGED",
            "createdAt": "2024-05-10T16:16:16Z",
            "mergedAt": "2024-05-10T17:02:35Z",
            "closedAt": "2024-05-10T17:02:35Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 3,
            "body": "Under stress scenarios we're seeing OOMs so this should help stabilize the allocations under heavy concurrency stress.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't clamp ctx size in `PredictServerFit`",
            "url": "https://github.com/ollama/ollama/pull/4317",
            "state": "MERGED",
            "createdAt": "2024-05-10T16:32:57Z",
            "mergedAt": "2024-05-10T17:17:12Z",
            "closedAt": "2024-05-10T17:17:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 6,
            "deletions": 19,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add phi2 mem",
            "url": "https://github.com/ollama/ollama/pull/4320",
            "state": "MERGED",
            "createdAt": "2024-05-10T19:13:55Z",
            "mergedAt": "2024-05-10T19:35:08Z",
            "closedAt": "2024-05-10T19:35:08Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use `--quantize` flag and `quantize` api parameter",
            "url": "https://github.com/ollama/ollama/pull/4321",
            "state": "MERGED",
            "createdAt": "2024-05-10T19:21:05Z",
            "mergedAt": "2024-05-10T20:06:13Z",
            "closedAt": "2024-05-10T20:06:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 17,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add support for IQ quantizarions",
            "url": "https://github.com/ollama/ollama/pull/4322",
            "state": "MERGED",
            "createdAt": "2024-05-10T20:24:55Z",
            "mergedAt": "2024-05-23T20:21:50Z",
            "closedAt": "2024-05-23T20:21:50Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 2
            },
            "additions": 72,
            "deletions": 10,
            "body": "This change allows importing `IQ` type gguf quantization with `ollama create`.\r\n\r\nThis change carries the commit from #3657 while moving its changes around to the refactored project structure.\r\n\r\n```bash\r\n\u276f ./ollama create nous-hermes-2-mistral:IQ_4XS -f /Users/bruce/models/nous-hermes-2-mistral/Modelfile\r\ntransferring model data \r\nusing existing layer sha256:737258efad6ba5cf7232de66715a26cadba67b0e4bdace5cf03cf49d1e4864a0 \r\ncreating new layer sha256:d7285065edcb87b4852f1144dd090812df1b00ade49f74e234066ea9407a14bc \r\ncreating new layer sha256:d8ba2f9a17b3bbdeb5690efaa409b3fcb0b56296a777c7a69c78aa33bbddf182 \r\ncreating new layer sha256:b2c4ee0a7317771fcbe7413c369d72ea911c63e6f52b2b0d6298a5a14c8e4983 \r\nwriting manifest \r\nsuccess \r\n\r\n\u276f ./ollama run nous-hermes-2-mistral:IQ_4XS\r\n>>> write some python\r\n\r\ndef print_fruits(fruits):\r\n    for fruit in fruits:\r\n        print(fruit)\r\n```\r\n\r\nTested with:\r\n`IQ1_S`\r\n`IQ1_M`\r\n`IQ2_M`\r\n`IQ3_XXS`\r\n`IQ3_XS`\r\n`IQ3_S`\r\n`IQ4_NL`\r\n`IQ4_XS`\r\n\r\nresolves #3622 ",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 10
            }
        }
    },
    {
        "node": {
            "title": "Always use the sorted list of GPUs",
            "url": "https://github.com/ollama/ollama/pull/4323",
            "state": "MERGED",
            "createdAt": "2024-05-10T20:54:13Z",
            "mergedAt": "2024-05-10T21:12:15Z",
            "closedAt": "2024-05-10T21:12:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "Make sure the first GPU has the most free space",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update go deps",
            "url": "https://github.com/ollama/ollama/pull/4324",
            "state": "MERGED",
            "createdAt": "2024-05-10T21:04:59Z",
            "mergedAt": "2024-05-11T04:39:27Z",
            "closedAt": "2024-05-11T04:39:27Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 179,
            "deletions": 127,
            "body": "Fixes #4297 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Integration fixes",
            "url": "https://github.com/ollama/ollama/pull/4326",
            "state": "MERGED",
            "createdAt": "2024-05-10T21:20:32Z",
            "mergedAt": "2024-05-10T21:25:59Z",
            "closedAt": "2024-05-10T21:25:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Ollama `ps` command for showing currently loaded models",
            "url": "https://github.com/ollama/ollama/pull/4327",
            "state": "MERGED",
            "createdAt": "2024-05-10T21:50:12Z",
            "mergedAt": "2024-05-14T00:17:36Z",
            "closedAt": "2024-05-14T00:17:36Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 10
            },
            "additions": 193,
            "deletions": 50,
            "body": "This change adds a rudimentary `ps` command which makes use of the new scheduler changes in the server. The UX also \r\n\r\nThe UX for this depends on whether you're using the CPU, GPU, or a hybrid of both and looks like:\r\n\r\n```\r\nNAME            ID              SIZE    PROCESSOR        UNTIL\r\nmistral:latest  61e88e884507    5.4 GB  100% GPU         28 seconds from now\r\n\r\nNAME            ID              SIZE    PROCESSOR        UNTIL\r\nmistral:latest  61e88e884507    5.4 GB  48%/52% CPU/GPU  28 seconds from now\r\n\r\nNAME            ID              SIZE    PROCESSOR        UNTIL\r\nmistral:latest  61e88e884507    5.4 GB  100% CPU         28 seconds from now\r\n```\r\n\r\nAdditionally, there is a new `--keepalive` flag in the REPL which can be used to set how long you want the model to stay resident in memory after the model has finished inference. It takes a duration string (e.g. `3m30s`), however we can switch this to also accept integers similar to the API.\r\n\r\nThis also introduces a new `/api/ps` endpoint which returns back a response similar to the `/api/tags` endpoint albeit with additional information. The size of the running model *will not* match the amount reported from the `/api/tags` endpoint for a given model since it can take additional memory when loaded onto the GPU or as a hybrid.\r\n\r\nPartially addresses #3902 \r\nFixes #4013 \r\nReplaces #2359\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "count memory up to NumGPU if set by user",
            "url": "https://github.com/ollama/ollama/pull/4328",
            "state": "MERGED",
            "createdAt": "2024-05-10T21:51:32Z",
            "mergedAt": "2024-05-14T20:47:45Z",
            "closedAt": "2024-05-14T20:47:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 15,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fall back to CPU runner with zero layers",
            "url": "https://github.com/ollama/ollama/pull/4329",
            "state": "MERGED",
            "createdAt": "2024-05-10T22:10:08Z",
            "mergedAt": "2024-05-10T22:23:16Z",
            "closedAt": "2024-05-10T22:23:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cache and reuse intermediate blobs",
            "url": "https://github.com/ollama/ollama/pull/4330",
            "state": "MERGED",
            "createdAt": "2024-05-10T23:17:44Z",
            "mergedAt": "2024-05-20T20:54:41Z",
            "closedAt": "2024-05-20T20:54:42Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 53,
            "deletions": 18,
            "body": "particularly useful for zipfiles and f16s",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix envconfig unit test",
            "url": "https://github.com/ollama/ollama/pull/4331",
            "state": "MERGED",
            "createdAt": "2024-05-10T23:50:11Z",
            "mergedAt": "2024-05-11T16:16:28Z",
            "closedAt": "2024-05-11T16:16:28Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix `ollama create`'s usage string",
            "url": "https://github.com/ollama/ollama/pull/4362",
            "state": "MERGED",
            "createdAt": "2024-05-11T14:04:53Z",
            "mergedAt": "2024-05-11T21:47:49Z",
            "closedAt": "2024-05-11T21:47:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Since `StringP()` automatically adds the initial value, the initial value description for Modelfile was duplicated.\r\nI have fixed this by removing the redundant default value description from the usage.\r\n\r\nBefore:\r\n```\r\n$ ./ollama create -h\r\nCreate a model from a Modelfile\r\n\r\nUsage:\r\n  ollama create MODEL [flags]\r\n\r\nFlags:\r\n  -f, --file string       Name of the Modelfile (default \"Modelfile\") (default \"Modelfile\")\r\n  -h, --help              help for create\r\n  -q, --quantize string   Quantize model to this level (e.g. q4_0)\r\n\r\nEnvironment Variables:\r\n      OLLAMA_HOST        The host:port or base URL of the Ollama server (e.g. http://localhost:11434)\r\n```\r\n\r\nAfter:\r\n```\r\n$ ./ollama create -h\r\nCreate a model from a Modelfile\r\n\r\nUsage:\r\n  ollama create MODEL [flags]\r\n\r\nFlags:\r\n  -f, --file string       Name of the Modelfile (default \"Modelfile\")\r\n  -h, --help              help for create\r\n  -q, --quantize string   Quantize model to this level (e.g. q4_0)\r\n\r\nEnvironment Variables:\r\n      OLLAMA_HOST        The host:port or base URL of the Ollama server (e.g. http://localhost:11434)\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "case sensitive filepaths",
            "url": "https://github.com/ollama/ollama/pull/4366",
            "state": "MERGED",
            "createdAt": "2024-05-11T20:51:28Z",
            "mergedAt": "2024-05-11T21:12:37Z",
            "closedAt": "2024-05-11T21:12:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 7,
            "body": "TODO: filenames can be case sensitive but filepaths should not. however this needs to be backwards compatible. it currently is not so fix the regression first\r\n\r\nresolves #4346 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update `LlamaScript` to point to new link from Legacy link.",
            "url": "https://github.com/ollama/ollama/pull/4379",
            "state": "MERGED",
            "createdAt": "2024-05-12T15:25:07Z",
            "mergedAt": "2024-05-14T01:08:32Z",
            "closedAt": "2024-05-14T01:08:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Still used Legacy link.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use tokenize/detokenize",
            "url": "https://github.com/ollama/ollama/pull/4380",
            "state": "MERGED",
            "createdAt": "2024-05-12T18:45:53Z",
            "mergedAt": "2024-05-29T19:01:00Z",
            "closedAt": "2024-05-29T19:01:00Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 66,
            "deletions": 242,
            "body": "remove server's infill and system prompt which are unused",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct typos.",
            "url": "https://github.com/ollama/ollama/pull/4387",
            "state": "MERGED",
            "createdAt": "2024-05-13T00:04:01Z",
            "mergedAt": "2024-05-13T01:21:11Z",
            "closedAt": "2024-05-13T01:21:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "ASSSISTANT --> ASSISTANT ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "removed inconsistent punctuation",
            "url": "https://github.com/ollama/ollama/pull/4411",
            "state": "MERGED",
            "createdAt": "2024-05-13T21:28:59Z",
            "mergedAt": "2024-05-13T22:30:46Z",
            "closedAt": "2024-05-13T22:30:46Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "Removed the period in `ollama serve -h`\r\n\r\nResolves https://github.com/ollama/ollama/issues/4410\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document older win10 terminal problems",
            "url": "https://github.com/ollama/ollama/pull/4412",
            "state": "MERGED",
            "createdAt": "2024-05-13T22:10:03Z",
            "mergedAt": "2024-07-05T15:18:22Z",
            "closedAt": "2024-07-05T15:18:22Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 1,
            "body": "We haven't found a workaround, so for now recommend updating.\r\n\r\nFixes #3916 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "check if name exists before create/pull/copy",
            "url": "https://github.com/ollama/ollama/pull/4413",
            "state": "MERGED",
            "createdAt": "2024-05-13T22:28:11Z",
            "mergedAt": "2024-05-29T19:06:58Z",
            "closedAt": "2024-05-29T19:06:58Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 135,
            "deletions": 30,
            "body": "TODO\r\n- [x] tests",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp submodule to `614d3b9`",
            "url": "https://github.com/ollama/ollama/pull/4414",
            "state": "MERGED",
            "createdAt": "2024-05-13T23:23:19Z",
            "mergedAt": "2024-05-16T20:53:09Z",
            "closedAt": "2024-05-16T20:53:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 1,
            "deletions": 25,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update the FAQ to be more clear about windows env variables",
            "url": "https://github.com/ollama/ollama/pull/4415",
            "state": "MERGED",
            "createdAt": "2024-05-14T01:00:41Z",
            "mergedAt": "2024-05-14T01:01:13Z",
            "closedAt": "2024-05-14T01:01:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "don't abort when an invalid model name is used in /save",
            "url": "https://github.com/ollama/ollama/pull/4416",
            "state": "MERGED",
            "createdAt": "2024-05-14T01:42:17Z",
            "mergedAt": "2024-05-14T01:48:28Z",
            "closedAt": "2024-05-14T01:48:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 8,
            "deletions": 2,
            "body": "Fixes #3852",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add load timeout env",
            "url": "https://github.com/ollama/ollama/pull/4419",
            "state": "CLOSED",
            "createdAt": "2024-05-14T01:58:06Z",
            "mergedAt": null,
            "closedAt": "2024-05-23T21:11:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 19,
            "deletions": 1,
            "body": "Fixes: https://github.com/ollama/ollama/issues/4350",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Fixed the API endpoint /api/tags when the model list is empty.",
            "url": "https://github.com/ollama/ollama/pull/4424",
            "state": "MERGED",
            "createdAt": "2024-05-14T08:22:19Z",
            "mergedAt": "2024-05-14T18:18:10Z",
            "closedAt": "2024-05-14T18:18:10Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 1,
            "body": "## Summary\r\n\r\nFixed an issue with the `/api/tags` endpoint where an empty model list was returning `{models: null}`. The endpoint now returns `{models: []}` when no models are available.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add qollama to list of Web & Desktop integrations",
            "url": "https://github.com/ollama/ollama/pull/4426",
            "state": "CLOSED",
            "createdAt": "2024-05-14T09:34:08Z",
            "mergedAt": null,
            "closedAt": "2024-06-06T07:34:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "QOllama is a Qt-based client for [ollama](https://github.com/ollama/ollama), providing a user-friendly interface for interacting with the model and managing chat history. It supports cross-platform functionality, ensuring a seamless experience on Windows, macOS, and Linux.\r\n\r\nGoto QOllama: [https://github.com/farleyrunkel/qollama](https://github.com/farleyrunkel/qollama)\r\n\r\n| Windows | MacOS | Linux |\r\n| :---: | :---: | :---: |\r\n| ![image](https://github.com/farleyrunkel/qollama/raw/main/resources/images/qollama-mac.png) | ![image](https://github.com/farleyrunkel/qollama/raw/main/resources/images/qollama.png) | todo |\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Remove VRAM convergence check for windows",
            "url": "https://github.com/ollama/ollama/pull/4430",
            "state": "MERGED",
            "createdAt": "2024-05-14T16:48:30Z",
            "mergedAt": "2024-05-14T17:59:07Z",
            "closedAt": "2024-05-14T17:59:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "The APIs we query are optimistic on free space, and windows pages VRAM, so we don't have to wait to see reported usage recover on unload",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "re-add system context",
            "url": "https://github.com/ollama/ollama/pull/4435",
            "state": "MERGED",
            "createdAt": "2024-05-14T18:31:31Z",
            "mergedAt": "2024-05-14T18:38:20Z",
            "closedAt": "2024-05-14T18:38:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "Fixes #4383",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "return on part done",
            "url": "https://github.com/ollama/ollama/pull/4436",
            "state": "MERGED",
            "createdAt": "2024-05-14T19:41:45Z",
            "mergedAt": "2024-05-16T00:16:25Z",
            "closedAt": "2024-05-16T00:16:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "only copy as much as we're expecting to receive to prevent runaway downloads",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix keepalive for non-interactive mode",
            "url": "https://github.com/ollama/ollama/pull/4438",
            "state": "MERGED",
            "createdAt": "2024-05-14T20:49:06Z",
            "mergedAt": "2024-05-14T22:17:05Z",
            "closedAt": "2024-05-14T22:17:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix typo in modelfile generation",
            "url": "https://github.com/ollama/ollama/pull/4439",
            "state": "MERGED",
            "createdAt": "2024-05-14T22:32:45Z",
            "mergedAt": "2024-05-14T22:34:29Z",
            "closedAt": "2024-05-14T22:34:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Sanitize the env var debug log",
            "url": "https://github.com/ollama/ollama/pull/4459",
            "state": "MERGED",
            "createdAt": "2024-05-15T21:43:25Z",
            "mergedAt": "2024-05-15T21:58:55Z",
            "closedAt": "2024-05-15T21:58:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 2,
            "body": "Only dump env vars we care about in the logs",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix the cpu estimatedTotal memory + get the expiry time for loading models",
            "url": "https://github.com/ollama/ollama/pull/4461",
            "state": "MERGED",
            "createdAt": "2024-05-15T22:29:49Z",
            "mergedAt": "2024-05-15T22:43:16Z",
            "closedAt": "2024-05-15T22:43:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Port cuda/rocm skip build vars to linux",
            "url": "https://github.com/ollama/ollama/pull/4462",
            "state": "MERGED",
            "createdAt": "2024-05-15T22:59:12Z",
            "mergedAt": "2024-05-15T23:27:47Z",
            "closedAt": "2024-05-15T23:27:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Windows already implements these, carry over to linux.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "changed line display to be calculated with runewidth",
            "url": "https://github.com/ollama/ollama/pull/4463",
            "state": "MERGED",
            "createdAt": "2024-05-16T00:16:04Z",
            "mergedAt": "2024-05-16T21:15:09Z",
            "closedAt": "2024-05-16T21:15:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 5,
            "body": "Calculates line length using `runewidth` instead of `len(string)` as `len(string)` produces the number of bytes in that string (often resulting in an incorrect calculation of display length for multi-width runes). Now cuts lines accordingly for multi-byte characters (Russian, latin, etc) as well as word-wraps for multi-width characters (Chinese, Japanese, Korean)\r\n\r\nOld Russian: \r\n<img width=\"560\" alt=\"Screenshot 2024-05-15 at 5 05 42\u202fPM\" src=\"https://github.com/ollama/ollama/assets/76125168/50bda24b-7fff-4cc9-8e85-6f1b4f006a65\">\r\n\r\nNew: Russian\r\n<img width=\"555\" alt=\"Screenshot 2024-05-15 at 5 01 21\u202fPM\" src=\"https://github.com/ollama/ollama/assets/76125168/a664d1f0-c321-4993-bad7-2c2e066e6a10\">\r\n\r\nOld: Chinese\r\n<img width=\"576\" alt=\"Screenshot 2024-05-15 at 5 10 09\u202fPM\" src=\"https://github.com/ollama/ollama/assets/76125168/5d47f924-9467-4f3b-ba1d-515072f875f3\">\r\n\r\nNew: Chinese\r\n<img width=\"572\" alt=\"Screenshot 2024-05-15 at 5 15 12\u202fPM\" src=\"https://github.com/ollama/ollama/assets/76125168/275e39dd-c89f-4545-9323-0aba1c8060e2\">\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Updating software for read me",
            "url": "https://github.com/ollama/ollama/pull/4467",
            "state": "MERGED",
            "createdAt": "2024-05-16T02:10:48Z",
            "mergedAt": "2024-05-16T20:55:14Z",
            "closedAt": "2024-05-16T20:55:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Updating read me file.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Skip max queue test on remote",
            "url": "https://github.com/ollama/ollama/pull/4482",
            "state": "MERGED",
            "createdAt": "2024-05-16T23:26:04Z",
            "mergedAt": "2024-05-16T23:43:48Z",
            "closedAt": "2024-05-16T23:43:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 1,
            "body": "This test needs to be able to adjust the queue size down from our default setting for a reliable test, so it needs to skip on remote test execution mode.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't return error on signal exit",
            "url": "https://github.com/ollama/ollama/pull/4483",
            "state": "MERGED",
            "createdAt": "2024-05-16T23:26:28Z",
            "mergedAt": "2024-05-17T18:41:57Z",
            "closedAt": "2024-05-17T18:41:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix quantize file types",
            "url": "https://github.com/ollama/ollama/pull/4502",
            "state": "MERGED",
            "createdAt": "2024-05-17T18:31:40Z",
            "mergedAt": "2024-05-20T23:09:27Z",
            "closedAt": "2024-05-20T23:09:27Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 18,
            "deletions": 16,
            "body": "this fixes the reported file type (quantization). previously this will report f16 or f32 based on the input file despite going through quantization\r\n\r\nthis changes contains some changes suggested by @pdevine in #4330",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add OLLAMA_NOHISTORY to turn off history in interactive mode",
            "url": "https://github.com/ollama/ollama/pull/4508",
            "state": "MERGED",
            "createdAt": "2024-05-17T23:38:16Z",
            "mergedAt": "2024-05-18T18:51:57Z",
            "closedAt": "2024-05-18T18:51:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 30,
            "deletions": 5,
            "body": "fixes #3002 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Enhanced GPU discovery and multi-gpu support with concurrency",
            "url": "https://github.com/ollama/ollama/pull/4517",
            "state": "MERGED",
            "createdAt": "2024-05-18T23:07:08Z",
            "mergedAt": "2024-06-14T22:35:01Z",
            "closedAt": "2024-06-14T22:35:01Z",
            "reviews": {
                "totalCount": 30
            },
            "files": {
                "totalCount": 31
            },
            "additions": 1814,
            "deletions": 692,
            "body": "Carries (and obsoletes if we move this one forward first) #4266 and #4441 \r\n\r\nThis refines our GPU discovery to split it into bootstrapping where we discover information about the GPUs once at startup, and then incrementally refresh just free space information, instead of fully rediscovering the GPUs over and over.\r\n\r\nFixes #3158 \r\nFixes #4198 \r\nFixes #3765 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Move the parser back + handle utf16 files",
            "url": "https://github.com/ollama/ollama/pull/4533",
            "state": "MERGED",
            "createdAt": "2024-05-20T04:45:53Z",
            "mergedAt": "2024-05-20T18:26:46Z",
            "closedAt": "2024-05-20T18:26:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 84,
            "deletions": 17,
            "body": "This moves the parser back to `parser/` and also adds support for decoding utf16le and utf16be files.\r\n\r\nFixes #4503 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct typo in error message",
            "url": "https://github.com/ollama/ollama/pull/4535",
            "state": "MERGED",
            "createdAt": "2024-05-20T12:34:40Z",
            "mergedAt": "2024-05-21T20:39:02Z",
            "closedAt": "2024-05-21T20:39:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The spelling of the term \"request\" has been corrected, which was previously mistakenly written as \"requeset\" in the error log message.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chore: fix typo in docs",
            "url": "https://github.com/ollama/ollama/pull/4536",
            "state": "MERGED",
            "createdAt": "2024-05-20T12:36:29Z",
            "mergedAt": "2024-05-20T21:19:03Z",
            "closedAt": "2024-05-20T21:19:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "simplify safetensors reading",
            "url": "https://github.com/ollama/ollama/pull/4543",
            "state": "MERGED",
            "createdAt": "2024-05-20T18:18:01Z",
            "mergedAt": "2024-05-21T21:43:55Z",
            "closedAt": "2024-05-21T21:43:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 49,
            "deletions": 81,
            "body": "mapstructure is unnecessary and the safetensors header can be read directly into a struct",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Wire up load progress",
            "url": "https://github.com/ollama/ollama/pull/4547",
            "state": "MERGED",
            "createdAt": "2024-05-20T23:42:06Z",
            "mergedAt": "2024-05-23T21:06:02Z",
            "closedAt": "2024-05-23T21:06:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 61,
            "deletions": 8,
            "body": "This doesn't expose a UX yet, but wires the initial server portion of progress reporting during load\r\n\r\nTODO\r\n- [X] Adjust waitUntilRunning to be smarter and look for stalled loads instead of a dumb 10m timer\r\n- [ ] ~~expose progress in `ollama run`~~  UX can come in a follow up PR\r\n- [ ] ~~expose percent loaded in `ollama ps`~~ UX can come in a follow up PR\r\n\r\nFixes #4350 \r\nReplaces #4123 #4419 \r\n\r\nThis should provide a good balance between slow model loads vs. detecting stalls without taking too long before giving up.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "working on integration of multi-byte and multi-width runes",
            "url": "https://github.com/ollama/ollama/pull/4549",
            "state": "MERGED",
            "createdAt": "2024-05-21T00:25:57Z",
            "mergedAt": "2024-05-28T19:04:03Z",
            "closedAt": "2024-05-28T19:04:03Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 2
            },
            "additions": 305,
            "deletions": 86,
            "body": "Fixed most issues touched on regarding multi-width runes. \r\n\r\nSome notable issues still exist with a combination of `insert` and `remove` commands\r\n\r\nResolves: https://github.com/ollama/ollama/issues/3432 and resolves: https://github.com/ollama/ollama/issues/4156",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "doc updates for the faq/troubleshooting",
            "url": "https://github.com/ollama/ollama/pull/4565",
            "state": "MERGED",
            "createdAt": "2024-05-21T21:55:18Z",
            "mergedAt": "2024-05-21T22:30:09Z",
            "closedAt": "2024-05-21T22:30:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 141,
            "deletions": 142,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add Ctrl + W shortcut",
            "url": "https://github.com/ollama/ollama/pull/4566",
            "state": "MERGED",
            "createdAt": "2024-05-21T23:56:34Z",
            "mergedAt": "2024-05-22T05:49:37Z",
            "closedAt": "2024-05-22T05:49:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Added \"Ctrl + W\" in shortcuts. \"Ctrl + W\" deletes the word before the cursor\r\n\r\nResolves https://github.com/ollama/ollama/issues/4534",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "lint some of the things",
            "url": "https://github.com/ollama/ollama/pull/4570",
            "state": "MERGED",
            "createdAt": "2024-05-22T04:31:33Z",
            "mergedAt": "2024-06-04T20:27:05Z",
            "closedAt": "2024-06-04T20:27:05Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 54
            },
            "additions": 237,
            "deletions": 242,
            "body": "now that ollama uses go1.22, `x/exp/slices` can be replaced with regular `slices`\r\n\r\nenable some useful linters:\r\n- intrange is a 1.22 feature which simplifies `for i := 0; i < n; i++ { }` with `for i := range n { }`\r\n- testifylint to find bad testify assertions\r\n- unconvert to find unnecessary type conversions\r\n- ~usestdlibvars to find values that can be replaced with stdlib vars, e.g. `OPTIONS` with `http.MethodOptions`~\r\n- wastedassign to find unnecessary assignments\r\n- whitespace is find unnecessary line",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chore: update tokenizer.go",
            "url": "https://github.com/ollama/ollama/pull/4571",
            "state": "MERGED",
            "createdAt": "2024-05-22T06:47:45Z",
            "mergedAt": "2024-05-22T07:25:23Z",
            "closedAt": "2024-05-22T07:25:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "PreTokenziers -> PreTokenizers",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use flash attention flag for now",
            "url": "https://github.com/ollama/ollama/pull/4580",
            "state": "MERGED",
            "createdAt": "2024-05-22T23:18:11Z",
            "mergedAt": "2024-05-23T04:52:09Z",
            "closedAt": "2024-05-23T04:52:09Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 19,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Add isolated gpu test to troubleshooting",
            "url": "https://github.com/ollama/ollama/pull/4594",
            "state": "MERGED",
            "createdAt": "2024-05-23T16:34:41Z",
            "mergedAt": "2024-05-30T20:10:55Z",
            "closedAt": "2024-05-30T20:10:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "This should help users isolate problems with their container runtime.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "bump",
            "url": "https://github.com/ollama/ollama/pull/4597",
            "state": "MERGED",
            "createdAt": "2024-05-23T18:23:06Z",
            "mergedAt": "2024-05-23T21:16:26Z",
            "closedAt": "2024-05-23T21:16:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 56,
            "deletions": 8,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Tidy up developer guide a little",
            "url": "https://github.com/ollama/ollama/pull/4598",
            "state": "MERGED",
            "createdAt": "2024-05-23T21:24:41Z",
            "mergedAt": "2024-05-23T22:14:30Z",
            "closedAt": "2024-05-23T22:14:30Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 19,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move envconfig and consolidate env vars",
            "url": "https://github.com/ollama/ollama/pull/4608",
            "state": "MERGED",
            "createdAt": "2024-05-24T06:44:31Z",
            "mergedAt": "2024-05-24T21:57:16Z",
            "closedAt": "2024-05-24T21:57:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 14
            },
            "additions": 81,
            "deletions": 51,
            "body": "This change moves `envconfig` so that it can be read by both the client and the server and adds descriptions to the various environment variables.\r\n\r\nFixes #4361",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "added new community integration (headless-ollama)",
            "url": "https://github.com/ollama/ollama/pull/4612",
            "state": "MERGED",
            "createdAt": "2024-05-24T13:58:18Z",
            "mergedAt": "2024-06-09T01:51:16Z",
            "closedAt": "2024-06-09T01:51:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "ollama makes it wonderfully easy to build desktop apps that rely on local LLMs with its js and python libraries.\r\n\r\n> however, the user's system needs to have ollama already installed for the desktop app to use the libraries and make calls to the LLMs. Making users install ollama client separately isn't good UX tbh. thus, \"headless-ollama\"\r\n\r\nthis repo has pre-run scripts which automatically utilises node runtime to check for the host OS and installs the ollama client and the models needed by the desktop app before the server starts.\r\n\r\nthis is really helpful while building desktop apps where you want everything to be local and self contained within the system.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix download retry issue",
            "url": "https://github.com/ollama/ollama/pull/4619",
            "state": "MERGED",
            "createdAt": "2024-05-24T18:31:46Z",
            "mergedAt": "2024-05-25T00:21:57Z",
            "closedAt": "2024-05-25T00:21:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Partial downloaded chunks currently resume incorrectly as the code tries to always download the full size of the chunk rather than the remaining size.\r\n\r\nFixes #4520",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 7
            }
        }
    },
    {
        "node": {
            "title": "fix q5_0, q5_1",
            "url": "https://github.com/ollama/ollama/pull/4624",
            "state": "MERGED",
            "createdAt": "2024-05-24T23:01:58Z",
            "mergedAt": "2024-05-24T23:11:22Z",
            "closedAt": "2024-05-24T23:11:22Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Report better warning on client closed abort of load",
            "url": "https://github.com/ollama/ollama/pull/4638",
            "state": "MERGED",
            "createdAt": "2024-05-25T16:24:13Z",
            "mergedAt": "2024-05-25T21:32:28Z",
            "closedAt": "2024-05-25T21:32:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "If the client closes the connection before we finish loading the model we abort, so lets make the log message clearer why to help users understand this failure mode",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adds olpaka flutter client",
            "url": "https://github.com/ollama/ollama/pull/4647",
            "state": "MERGED",
            "createdAt": "2024-05-26T11:50:56Z",
            "mergedAt": "2024-05-28T00:22:02Z",
            "closedAt": "2024-05-28T00:22:02Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Ensure `nvidia` and `nvidia_uvm` kernel modules are loaded in `install.sh` script and at startup",
            "url": "https://github.com/ollama/ollama/pull/4652",
            "state": "MERGED",
            "createdAt": "2024-05-26T18:53:36Z",
            "mergedAt": "2024-05-26T21:57:17Z",
            "closedAt": "2024-05-26T21:57:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 16,
            "body": "Make sure the `nvidia` and `nvidia_uvm` kernel modules are loaded on install.\r\n\r\nNvidia has a daemon that takes care of that `nvidia-persistenced` for restarts, so add both to its config file as well\r\n\r\nLastly, make sure the kernel modules are loaded when re-running the install script, even if drivers are already installed.\r\n\r\nFixes https://github.com/ollama/ollama/issues/4563",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm/server.go: Fix 2 minor typos",
            "url": "https://github.com/ollama/ollama/pull/4661",
            "state": "MERGED",
            "createdAt": "2024-05-27T11:48:32Z",
            "mergedAt": "2024-05-28T00:21:10Z",
            "closedAt": "2024-05-28T00:21:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix OLLAMA_LLM_LIBRARY with wrong map name and add more env vars to help message ",
            "url": "https://github.com/ollama/ollama/pull/4663",
            "state": "MERGED",
            "createdAt": "2024-05-27T12:52:50Z",
            "mergedAt": "2024-05-30T16:36:51Z",
            "closedAt": "2024-05-30T16:36:51Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 3,
            "body": "1. The first commit fix the the wrong name of `OLLAMA_LLM_LIBRARY` in `map[string]EnvVar`\r\n2. The second commit add more env vars to help message of `ollama serve --help` to let user know what env var we can use when start the server.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "small fix in chatbot example to get stream of tokens",
            "url": "https://github.com/ollama/ollama/pull/4671",
            "state": "MERGED",
            "createdAt": "2024-05-27T21:55:31Z",
            "mergedAt": "2024-05-28T00:19:21Z",
            "closedAt": "2024-05-28T00:19:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "It seems like without this line it is not getting the answer as a stream and we have to wait for the whole output to be generated before printing it.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add OllamaSpring Project to Readme",
            "url": "https://github.com/ollama/ollama/pull/4672",
            "state": "MERGED",
            "createdAt": "2024-05-28T02:55:13Z",
            "mergedAt": "2024-05-28T02:58:27Z",
            "closedAt": "2024-05-28T02:58:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add OllamaSpring Project to Readme",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Give the final model loading more time",
            "url": "https://github.com/ollama/ollama/pull/4682",
            "state": "MERGED",
            "createdAt": "2024-05-28T15:56:47Z",
            "mergedAt": "2024-05-28T16:36:03Z",
            "closedAt": "2024-05-28T16:36:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 2,
            "body": "On some systems, 1 minute isn't sufficient to finish the load after it hits 100%\r\n\r\nFixes #4098 #4636 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix nvidia detection in install script",
            "url": "https://github.com/ollama/ollama/pull/4683",
            "state": "MERGED",
            "createdAt": "2024-05-28T16:57:49Z",
            "mergedAt": "2024-05-28T16:59:37Z",
            "closedAt": "2024-05-28T16:59:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: skip blob verification for already verified blobs",
            "url": "https://github.com/ollama/ollama/pull/4712",
            "state": "MERGED",
            "createdAt": "2024-05-29T22:26:02Z",
            "mergedAt": "2024-06-05T23:39:11Z",
            "closedAt": "2024-06-05T23:39:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 18,
            "deletions": 14,
            "body": "WIP.\r\n\r\nThis should also include a way to force verification via a flag or env or other means. I'll let the bike shedding begin on the _best_ way to handle that. :)",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "proper utf16 support",
            "url": "https://github.com/ollama/ollama/pull/4715",
            "state": "MERGED",
            "createdAt": "2024-05-30T04:50:54Z",
            "mergedAt": "2024-06-10T18:41:29Z",
            "closedAt": "2024-06-10T18:41:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 58,
            "deletions": 32,
            "body": "instead of relying on unreadable runes which can be appear for other reasons, check the header and adjust the scanner and decoder",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: update to add LLocal.in to web & desktop integrations",
            "url": "https://github.com/ollama/ollama/pull/4719",
            "state": "MERGED",
            "createdAt": "2024-05-30T11:14:51Z",
            "mergedAt": "2024-06-04T21:43:59Z",
            "closedAt": "2024-06-04T21:43:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "- LLocal.in is an easy to use Ollama desktop client\r\n- supports almost all of the api end points, ollama provides",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fixed japanese characters deleted at end of line",
            "url": "https://github.com/ollama/ollama/pull/4728",
            "state": "MERGED",
            "createdAt": "2024-05-30T17:38:35Z",
            "mergedAt": "2024-05-30T23:25:13Z",
            "closedAt": "2024-05-30T23:25:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 1,
            "body": "Added check to prevent erasing character at end of line\r\n\r\nResolves: https://github.com/ollama/ollama/issues/4714",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp submodule to `5921b8f0`",
            "url": "https://github.com/ollama/ollama/pull/4731",
            "state": "MERGED",
            "createdAt": "2024-05-30T21:40:26Z",
            "mergedAt": "2024-05-30T23:20:22Z",
            "closedAt": "2024-05-30T23:20:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 22,
            "deletions": 25,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "added IsValidNamespace function",
            "url": "https://github.com/ollama/ollama/pull/4733",
            "state": "MERGED",
            "createdAt": "2024-05-30T23:03:53Z",
            "mergedAt": "2024-05-31T21:08:45Z",
            "closedAt": "2024-05-31T21:08:45Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 31,
            "deletions": 0,
            "body": "added function to the package for purpose of validating new usernames on the website",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "partial offloading: allow flash attention and disable mmap",
            "url": "https://github.com/ollama/ollama/pull/4734",
            "state": "MERGED",
            "createdAt": "2024-05-30T23:39:10Z",
            "mergedAt": "2024-05-30T23:58:01Z",
            "closedAt": "2024-05-30T23:58:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 21,
            "deletions": 18,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "vocab only for tokenize",
            "url": "https://github.com/ollama/ollama/pull/4736",
            "state": "MERGED",
            "createdAt": "2024-05-30T23:51:00Z",
            "mergedAt": "2024-05-31T00:21:00Z",
            "closedAt": "2024-05-31T00:21:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "tensors are unneeded for tokenize/detokenize",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "only generate on relevant changes",
            "url": "https://github.com/ollama/ollama/pull/4737",
            "state": "MERGED",
            "createdAt": "2024-05-30T23:54:28Z",
            "mergedAt": "2024-05-31T00:17:50Z",
            "closedAt": "2024-05-31T00:17:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 4,
            "body": "relevant change include changes to c++, generate scripts or the llama.cpp submodule",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use `int32_t` for call to tokenize",
            "url": "https://github.com/ollama/ollama/pull/4738",
            "state": "MERGED",
            "createdAt": "2024-05-31T04:01:55Z",
            "mergedAt": "2024-05-31T04:43:30Z",
            "closedAt": "2024-05-31T04:43:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 19,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Jetson cuda variants for arm",
            "url": "https://github.com/ollama/ollama/pull/4741",
            "state": "CLOSED",
            "createdAt": "2024-05-31T04:56:03Z",
            "mergedAt": null,
            "closedAt": "2024-08-08T00:26:39Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 19
            },
            "additions": 275,
            "deletions": 158,
            "body": "This adds new variants for arm64 specific to Jetson platforms\r\n\r\nFixes #2408 #4693 #5100 #4861\r\n\r\nUpdated to layer on #5631 to reduce the payload size so we're not at risk of exceeding the limits.\r\n\r\nAfter extracting the tgz:\r\n```\r\n% ls -F\r\ncuda/  cuda_jetpack5/  cuda_jetpack6/  ollama*  ollama-linux-arm64.tgz\r\n% ls -lh ollama-linux-arm64.tgz\r\n-rw-r--r-- 1 daniel daniel 1.2G Jul 12 09:09 ollama-linux-arm64.tgz\r\n% ls -lh ollama\r\n-rwxr-xr-x 1 daniel daniel 245M Jul 12 08:58 ollama\r\n% du -sh cuda*\r\n356M\tcuda\r\n519M\tcuda_jetpack5\r\n552M\tcuda_jetpack6\r\n```\r\n\r\n`\"Dynamic LLM libraries [cuda_jetpack6 cuda_v11 cpu cuda_jetpack5]\"`\r\n\r\n```\r\ntime=2024-07-12T09:16:54.654-07:00 level=INFO source=server.go:383 msg=\"starting llama server\" cmd=\"/tmp/ollama2043741824/runners/cuda_jetpack5/ollama_llama_server --model /home/daniel/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --verbose --parallel 4 --port 45659\"\r\ntime=2024-07-12T09:16:54.654-07:00 level=DEBUG source=server.go:398 msg=subprocess environment=\"[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin LD_LIBRARY_PATH=/home/daniel/tmp/cuda_jetpack5:/tmp/ollama2043741824/runners/cuda_jetpack5:/tmp/ollama2043741824/runners CUDA_VISIBLE_DEVICES=GPU-de921cca-84a7-545a-ac50-34a5746dc088]\"\r\n...\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: Orin, compute capability 8.7, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\n...\r\n```",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "refactor convert",
            "url": "https://github.com/ollama/ollama/pull/4756",
            "state": "MERGED",
            "createdAt": "2024-05-31T18:46:52Z",
            "mergedAt": "2024-08-01T21:16:31Z",
            "closedAt": "2024-08-01T21:16:31Z",
            "reviews": {
                "totalCount": 18
            },
            "files": {
                "totalCount": 29
            },
            "additions": 2494,
            "deletions": 1659,
            "body": "the goal is to build a single, well defined interface to convert a model as well as interfaces for input formats (e.g. safetensors, pytorch), model architectures (e.g. llama, gemma), and model tokenizers\r\n\r\nthis change makes some significant changes to the conversion process:\r\n\r\n1. implement a single function call for conversion `convert.Convert(string, io.WriteSeeker)` which abstracts the many operation required for successful conversion\r\n2. implement a new `convert.Converter` interface which each model conversion shall implement\r\n3. decouple vocabulary parsing from model\r\n4. add special vocabulary detection for both tokenizer.model and tokenizer.json based vocabularies\r\n5. update the tensor writing interface for better compatibility with non-trivial conversions\r\n\r\nas a test of this new interface, implement mixtral conversion as an extension to llama conversion\r\n\r\nTODO:\r\n- [ ] write short tests for tokenizer, model KVs, minimal tensor\r\n\r\nResolves: https://github.com/ollama/ollama/issues/5255",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "revert tokenize ffi",
            "url": "https://github.com/ollama/ollama/pull/4761",
            "state": "MERGED",
            "createdAt": "2024-06-01T00:25:44Z",
            "mergedAt": "2024-06-01T01:54:21Z",
            "closedAt": "2024-06-01T01:54:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 144,
            "deletions": 72,
            "body": "this change reverts the series of changes introduced to call tokenize/detokenize. there's a bug on windows specifically where it'll segfault loading deepseek-llm's pretokenizer regexp. the most likely candidate is unicode support differences in mingw used by cgo and msvc used by the subprocess",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add embed model command and fix question invoke",
            "url": "https://github.com/ollama/ollama/pull/4766",
            "state": "MERGED",
            "createdAt": "2024-06-01T12:51:34Z",
            "mergedAt": "2024-06-04T05:20:48Z",
            "closedAt": "2024-06-04T05:20:48Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "I was following the tutorial but i couldn't run it because embedding model was not available, so i had to download the embedding model using `ollama pull nomic-embed-text`. Also the code where we are asking the LLM the question, we are not printing anything so fixed that too.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "detect chat template from KV",
            "url": "https://github.com/ollama/ollama/pull/4800",
            "state": "MERGED",
            "createdAt": "2024-06-03T18:56:21Z",
            "mergedAt": "2024-06-06T23:17:18Z",
            "closedAt": "2024-06-06T23:17:18Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 26
            },
            "additions": 428,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "human numbers formats to 3 digits now",
            "url": "https://github.com/ollama/ollama/pull/4802",
            "state": "CLOSED",
            "createdAt": "2024-06-04T00:29:19Z",
            "mergedAt": null,
            "closedAt": "2024-06-05T00:03:58Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 27,
            "deletions": 17,
            "body": "reformatted humannumbers to always output three digits\r\nadded another case for >= trillion\r\n\r\nWe decided to move these changes to ollama.com to not conflict with current storage.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: use `envconfig.ModelsDir` directly",
            "url": "https://github.com/ollama/ollama/pull/4821",
            "state": "MERGED",
            "createdAt": "2024-06-04T20:54:45Z",
            "mergedAt": "2024-07-03T22:36:11Z",
            "closedAt": "2024-07-03T22:36:11Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 5,
            "deletions": 20,
            "body": "This removes noop helper function.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 12
            }
        }
    },
    {
        "node": {
            "title": "API PS Documentation",
            "url": "https://github.com/ollama/ollama/pull/4822",
            "state": "MERGED",
            "createdAt": "2024-06-04T23:10:54Z",
            "mergedAt": "2024-06-05T18:06:53Z",
            "closedAt": "2024-06-05T18:06:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 46,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs(tools): add gollama",
            "url": "https://github.com/ollama/ollama/pull/4829",
            "state": "MERGED",
            "createdAt": "2024-06-05T08:30:35Z",
            "mergedAt": "2024-06-05T21:13:39Z",
            "closedAt": "2024-06-05T21:13:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "- Add [gollama](https://github.com/sammcj/gollama) (model management tool) to CLI/TUI tools.\r\n\r\nDisclaimer - I did just hack this up over the weekend, but me and a few friends are actually finding it really useful so I thought I'd submit it.\r\n\r\nDon't want to self-promote, or if it's not up to quality feel free to reject!",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Separate ListResponse and ModelResponse for api/tags vs api/ps",
            "url": "https://github.com/ollama/ollama/pull/4842",
            "state": "MERGED",
            "createdAt": "2024-06-05T18:03:08Z",
            "mergedAt": "2024-06-06T17:11:45Z",
            "closedAt": "2024-06-06T17:11:45Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 30,
            "deletions": 14,
            "body": "/api/tags was returning \"0001-01-01T00:00:00Z\" for 'expires_at'\r\n/api/ps was returning \"0001-01-01T00:00:00Z\" for 'modified_at'\r\n\r\n- Removes these fields from the respective endpoints\r\n\r\n/api/ps was omitting 'size_vram' when it was 0\r\n\r\n- ensures that size_vram is always returned\r\n\r\nAdded assertion in test case, and tested locally both with curl and CLI",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add conversion for microsoft phi 3 mini/medium 4k, 128",
            "url": "https://github.com/ollama/ollama/pull/4845",
            "state": "CLOSED",
            "createdAt": "2024-06-05T21:35:45Z",
            "mergedAt": null,
            "closedAt": "2024-07-01T22:39:36Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Error handling load_single_document() in ingest.py",
            "url": "https://github.com/ollama/ollama/pull/4852",
            "state": "MERGED",
            "createdAt": "2024-06-06T09:45:12Z",
            "mergedAt": "2024-06-09T17:41:08Z",
            "closedAt": "2024-06-09T17:41:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 7,
            "body": "load_single_document() handles\r\n- corrupt files\r\n- empty (zero byte) files\r\n- unsupported file extensions",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md with LangChainRust",
            "url": "https://github.com/ollama/ollama/pull/4854",
            "state": "MERGED",
            "createdAt": "2024-06-06T10:45:28Z",
            "mergedAt": "2024-06-09T00:29:36Z",
            "closedAt": "2024-06-09T00:29:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "LangChain-Rust now supports Ollama (via [Ollama-rs](https://github.com/pepperoni21/ollama-rs)).",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Rocm v6 bump",
            "url": "https://github.com/ollama/ollama/pull/4874",
            "state": "MERGED",
            "createdAt": "2024-06-06T17:44:29Z",
            "mergedAt": "2024-06-15T14:38:32Z",
            "closedAt": "2024-06-15T14:38:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Rocm gfx900 workaround",
            "url": "https://github.com/ollama/ollama/pull/4875",
            "state": "MERGED",
            "createdAt": "2024-06-06T17:53:13Z",
            "mergedAt": "2024-06-15T14:38:58Z",
            "closedAt": "2024-06-15T14:38:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 18,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "API app/browser access",
            "url": "https://github.com/ollama/ollama/pull/4879",
            "state": "MERGED",
            "createdAt": "2024-06-06T18:56:08Z",
            "mergedAt": "2024-06-06T22:19:03Z",
            "closedAt": "2024-06-06T22:19:03Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 0,
            "body": "Fixes #4791 \r\nFixes #3799 \r\nFixes #4388 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Extend api/show and ollama show to return more model info",
            "url": "https://github.com/ollama/ollama/pull/4881",
            "state": "MERGED",
            "createdAt": "2024-06-06T21:48:30Z",
            "mergedAt": "2024-06-19T21:19:02Z",
            "closedAt": "2024-06-19T21:19:02Z",
            "reviews": {
                "totalCount": 26
            },
            "files": {
                "totalCount": 5
            },
            "additions": 243,
            "deletions": 30,
            "body": "Building off of #3899 \r\nResolves #3570, #2732, #3899",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "llm: update llama.cpp commit to `7c26775`",
            "url": "https://github.com/ollama/ollama/pull/4896",
            "state": "MERGED",
            "createdAt": "2024-06-07T04:35:53Z",
            "mergedAt": "2024-06-17T19:56:16Z",
            "closedAt": "2024-06-17T19:56:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 31,
            "deletions": 29,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add ability to skip oneapi generate",
            "url": "https://github.com/ollama/ollama/pull/4909",
            "state": "MERGED",
            "createdAt": "2024-06-07T15:33:28Z",
            "mergedAt": "2024-06-07T21:07:15Z",
            "closedAt": "2024-06-07T21:07:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "This follows the same pattern for cuda and rocm to allow disabling the build even when we detect the dependent libraries\r\n\r\nRelated to #4511 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix create model when template detection errors",
            "url": "https://github.com/ollama/ollama/pull/4910",
            "state": "MERGED",
            "createdAt": "2024-06-07T16:06:19Z",
            "mergedAt": "2024-06-07T18:07:39Z",
            "closedAt": "2024-06-07T18:07:39Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 7
            },
            "additions": 77,
            "deletions": 55,
            "body": "this change fixes a bug where a template detection error will cause create to error instead of not including a template",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "convert bert model from safetensors",
            "url": "https://github.com/ollama/ollama/pull/4917",
            "state": "MERGED",
            "createdAt": "2024-06-07T21:56:33Z",
            "mergedAt": "2024-08-21T18:48:29Z",
            "closedAt": "2024-08-21T18:48:29Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 7
            },
            "additions": 344,
            "deletions": 15,
            "body": "add a `moreParser` interface which converters can implement to signal a need for more configuration parsing\r\n\r\nfix a bug in the tokenizer.json parsing where vocab size might exceed intended count if added_token.json contains tokens already defined\r\n\r\nfix a bug in cmd where create will flatten the directory structure potentially creating conflicting files",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update import.md",
            "url": "https://github.com/ollama/ollama/pull/4921",
            "state": "MERGED",
            "createdAt": "2024-06-07T23:45:27Z",
            "mergedAt": "2024-06-10T18:41:10Z",
            "closedAt": "2024-06-10T18:41:10Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 64,
            "deletions": 135,
            "body": "update import docs with recently added features",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix parsing big endian gguf",
            "url": "https://github.com/ollama/ollama/pull/4938",
            "state": "MERGED",
            "createdAt": "2024-06-08T19:34:13Z",
            "mergedAt": "2024-06-10T16:38:12Z",
            "closedAt": "2024-06-10T16:38:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 19,
            "deletions": 9,
            "body": "most gguf files will be little endian but a big endian file should not panic",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - Apple Vision Pro",
            "url": "https://github.com/ollama/ollama/pull/4949",
            "state": "MERGED",
            "createdAt": "2024-06-09T13:10:21Z",
            "mergedAt": "2024-09-05T05:30:19Z",
            "closedAt": "2024-09-05T05:30:20Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "Added Enchanted with Apple Vision Pro support",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/4957",
            "state": "MERGED",
            "createdAt": "2024-06-09T21:29:54Z",
            "mergedAt": "2024-09-05T20:10:44Z",
            "closedAt": "2024-09-05T20:10:44Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add AiLama to the list of community apps in Extensions & Plugins\r\n\r\nThis is a duplicate pull request of #4481, but resolves conflicts ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix: skip removing layers that no longer exist",
            "url": "https://github.com/ollama/ollama/pull/4965",
            "state": "MERGED",
            "createdAt": "2024-06-10T18:18:33Z",
            "mergedAt": "2024-06-10T18:40:03Z",
            "closedAt": "2024-06-10T18:40:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 37,
            "deletions": 1,
            "body": "some models, such as `wizardcoder:34b-python`, incorrectly includes the config layer as an item in layers. this causes `RemoveLayers` to try to remove the same layer more than once, failing the second time since it's already removed\r\n\r\n```json\r\n{\r\n  \"schemaVersion\": 2,\r\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\r\n  \"config\": {\r\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\r\n    \"digest\": \"sha256:a168bedb9a09640289c5174690a6221adae48b75dc431a219923f052ef20d0af\",\r\n    \"size\": 456\r\n  },\r\n  \"layers\": [\r\n    {\r\n      \"mediaType\": \"application/vnd.ollama.image.model\",\r\n      \"digest\": \"sha256:c8ad30822293b3c24f02265a42e2879b9725bd19a54d048fbe2c38487ee0ea84\",\r\n      \"size\": 19052059872\r\n    },\r\n    {\r\n      \"mediaType\": \"application/vnd.ollama.image.template\",\r\n      \"digest\": \"sha256:2d836d77287d85ac3d2ea87f4d765db6aaabc98543442072111b3d9831cdf9f1\",\r\n      \"size\": 61\r\n    },\r\n    {\r\n      \"mediaType\": \"application/vnd.ollama.image.system\",\r\n      \"digest\": \"sha256:8fadb9ad1206f2f130b54004893a2a7f76b1ff41a78049d69d797df2ee67fe94\",\r\n      \"size\": 106\r\n    },\r\n    {\r\n      \"mediaType\": \"application/vnd.ollama.image.params\",\r\n      \"digest\": \"sha256:bf6237d30a42b25b196a7a178dc566e113cf2f193aa11e7302c6d61880be6028\",\r\n      \"size\": 30\r\n    },\r\n    {\r\n      \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\r\n      \"digest\": \"sha256:a168bedb9a09640289c5174690a6221adae48b75dc431a219923f052ef20d0af\",\r\n      \"size\": 456\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nresolves #4898 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove :latest from list and show",
            "url": "https://github.com/ollama/ollama/pull/4968",
            "state": "CLOSED",
            "createdAt": "2024-06-10T23:40:14Z",
            "mergedAt": null,
            "closedAt": "2024-08-12T17:27:09Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 16,
            "deletions": 9,
            "body": "<img width=\"504\" alt=\"Screenshot 2024-06-10 at 4 45 15\u202fPM\" src=\"https://github.com/ollama/ollama/assets/65097070/e977df55-22d8-4284-81cf-7cd1015a8398\">\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: \"Skip searching for network devices\"",
            "url": "https://github.com/ollama/ollama/pull/4972",
            "state": "MERGED",
            "createdAt": "2024-06-11T08:12:25Z",
            "mergedAt": "2024-06-15T00:04:41Z",
            "closedAt": "2024-06-15T00:04:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "On an Ubuntu 24.04 computer with vmware installed, the sudo lshw command will get stuck. \"Network interfaces\" is always displayed",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Ollama-hpp to Community Libraries in README.",
            "url": "https://github.com/ollama/ollama/pull/4983",
            "state": "MERGED",
            "createdAt": "2024-06-11T18:13:11Z",
            "mergedAt": "2024-06-11T18:15:05Z",
            "closedAt": "2024-06-11T18:15:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Ollama-hpp is a header-only C++ implementation of the Ollama API. Adding to the README for Community Libraries at Pat's suggestion.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "llm: fix seed value not being applied to requests",
            "url": "https://github.com/ollama/ollama/pull/4986",
            "state": "MERGED",
            "createdAt": "2024-06-11T20:40:11Z",
            "mergedAt": "2024-06-11T21:24:41Z",
            "closedAt": "2024-06-11T21:24:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 9,
            "body": "Fixes https://github.com/ollama/ollama/issues/4660\r\nFixes https://github.com/ollama/ollama/issues/2773\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert \"Merge pull request #4938 from ollama/mxyng/fix-byte-order\"",
            "url": "https://github.com/ollama/ollama/pull/4987",
            "state": "MERGED",
            "createdAt": "2024-06-11T22:56:34Z",
            "mergedAt": "2024-06-11T23:04:20Z",
            "closedAt": "2024-06-11T23:04:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 19,
            "body": "This reverts commit f5f245cc154580fa7b4052c001d2a7e3d771cfb8, reversing changes made to 94d37fdcae30ddeb6c9f65c8707004f5ec9eaf33.\r\n\r\nthis change broke gguf v2 which is incorrectly detected as big endian",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: multiple templates when creating from model",
            "url": "https://github.com/ollama/ollama/pull/5004",
            "state": "MERGED",
            "createdAt": "2024-06-12T19:00:13Z",
            "mergedAt": "2024-06-12T21:39:29Z",
            "closedAt": "2024-06-12T21:39:29Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 33,
            "deletions": 26,
            "body": "multiple templates may appear in a model if a model is created from another model that 1) has an autodetected template and 2) defines a custom template\r\n\r\nthis fixes the bug by not detecting chat template when inheriting from another model",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: /v1/models and /v1/models/{model} compatibility",
            "url": "https://github.com/ollama/ollama/pull/5007",
            "state": "MERGED",
            "createdAt": "2024-06-12T20:59:37Z",
            "mergedAt": "2024-07-02T18:50:56Z",
            "closedAt": "2024-07-02T18:50:56Z",
            "reviews": {
                "totalCount": 16
            },
            "files": {
                "totalCount": 6
            },
            "additions": 386,
            "deletions": 13,
            "body": "This PR adds compatibility with the /v1/models and /v1/models/{model} endpoint for listing models.\r\n\r\nE.g.\r\n\r\n`curl http://localhost:11434/v1/models`\r\n\r\n```\r\n{\r\n  \"object\": \"list\",\r\n  \"data\": [\r\n    {\r\n      \"id\": \"mario:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1718141294,\r\n      \"owned_by\": \"ollama\"\r\n    },\r\n    {\r\n      \"id\": \"nomic-embed-text:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1718054969,\r\n      \"owned_by\": \"ollama\"\r\n    },\r\n    {\r\n      \"id\": \"llava:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1718049682,\r\n      \"owned_by\": \"ollama\"\r\n    },\r\n    {\r\n      \"id\": \"mistral:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1717609491,\r\n      \"owned_by\": \"ollama\"\r\n    },\r\n    {\r\n      \"id\": \"llama3:latest\",\r\n      \"object\": \"model\",\r\n      \"created\": 1717451603,\r\n      \"owned_by\": \"ollama\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nResolves #2430 \r\nResolves #2476 \r\n\r\nIncludes #5028 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "move OLLAMA_HOST to envconfig",
            "url": "https://github.com/ollama/ollama/pull/5009",
            "state": "MERGED",
            "createdAt": "2024-06-12T22:17:39Z",
            "mergedAt": "2024-06-12T22:48:16Z",
            "closedAt": "2024-06-12T22:48:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 6
            },
            "additions": 119,
            "deletions": 103,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert \"proper utf16 support\"",
            "url": "https://github.com/ollama/ollama/pull/5025",
            "state": "MERGED",
            "createdAt": "2024-06-13T17:24:20Z",
            "mergedAt": "2024-06-13T17:31:25Z",
            "closedAt": "2024-06-13T17:31:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 32,
            "deletions": 58,
            "body": "This reverts commit 66ab48772f4f41f3f27fb93e15ef0cf756bda3d0.\r\n\r\nthis change broke utf-8 scanning of multi-byte runes\r\n\r\nresolves #5055 by reverting the offending change #4715",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "server: remove jwt decoding error",
            "url": "https://github.com/ollama/ollama/pull/5027",
            "state": "MERGED",
            "createdAt": "2024-06-13T18:15:13Z",
            "mergedAt": "2024-06-13T18:21:15Z",
            "closedAt": "2024-06-13T18:21:15Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: /v1/models/{model} compatibility",
            "url": "https://github.com/ollama/ollama/pull/5028",
            "state": "MERGED",
            "createdAt": "2024-06-13T18:29:28Z",
            "mergedAt": "2024-07-02T18:40:48Z",
            "closedAt": "2024-07-02T18:40:48Z",
            "reviews": {
                "totalCount": 16
            },
            "files": {
                "totalCount": 5
            },
            "additions": 121,
            "deletions": 6,
            "body": "Adds compatibility for `/v1/models/{model}`\r\n\r\nE.g\r\n`curl http://localhost:11434/v1/models/llama3`\r\n\r\n```\r\n{\r\n    \"id\": \"llama3\",\r\n    \"object\": \"model\",\r\n    \"created\": 1718141294,\r\n    \"owned_by\": \"library\"\r\n}\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add OLLAMA_MODELS to envconfig",
            "url": "https://github.com/ollama/ollama/pull/5029",
            "state": "MERGED",
            "createdAt": "2024-06-13T19:06:21Z",
            "mergedAt": "2024-06-13T19:52:04Z",
            "closedAt": "2024-06-13T19:52:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 8
            },
            "additions": 47,
            "deletions": 11,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: multibyte utf16",
            "url": "https://github.com/ollama/ollama/pull/5031",
            "state": "MERGED",
            "createdAt": "2024-06-13T20:08:58Z",
            "mergedAt": "2024-06-13T20:14:55Z",
            "closedAt": "2024-06-13T20:14:55Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 54,
            "deletions": 45,
            "body": "follow up to #5025 and #4715 which fixes multibyte runes for utf16",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Actually skip PhysX on windows",
            "url": "https://github.com/ollama/ollama/pull/5032",
            "state": "MERGED",
            "createdAt": "2024-06-13T20:17:42Z",
            "mergedAt": "2024-06-13T20:26:09Z",
            "closedAt": "2024-06-13T20:26:09Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Fixes #4984 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add ModifiedAt Field to /api/show",
            "url": "https://github.com/ollama/ollama/pull/5033",
            "state": "MERGED",
            "createdAt": "2024-06-13T20:53:22Z",
            "mergedAt": "2024-06-16T03:53:56Z",
            "closedAt": "2024-06-16T03:53:57Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 36,
            "deletions": 21,
            "body": "Changed `model` variable name to `m` due to `ParseName` function from `model `package\r\n\r\nE.g.\r\n\r\n...\r\n\r\n```\r\n  \"template\": \"[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\",\r\n  \"details\": {\r\n    \"parent_model\": \"\",\r\n    \"format\": \"gguf\",\r\n    \"family\": \"llama\",\r\n    \"families\": [\r\n      \"llama\",\r\n      \"clip\"\r\n    ],\r\n    \"parameter_size\": \"7B\",\r\n    \"quantization_level\": \"Q4_0\"\r\n  },\r\n  \"modified_at\": \"2024-06-10T13:01:22.096005938-07:00\"\r\n}\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Re-introduce the `llama` package",
            "url": "https://github.com/ollama/ollama/pull/5034",
            "state": "MERGED",
            "createdAt": "2024-06-13T21:06:18Z",
            "mergedAt": "2024-10-08T15:53:54Z",
            "closedAt": "2024-10-08T15:53:55Z",
            "reviews": {
                "totalCount": 30
            },
            "files": {
                "totalCount": 289
            },
            "additions": 166143,
            "deletions": 166,
            "body": "This PR brings back the `llama` package, making it possible to call llama.cpp and ggml APIs from Go directly via CGo. This has a few advantages:\r\n\r\n1. C APIs can be called directly from Go without needing to use the previous \"server\" REST API\r\n2. On macOS and for CPU builds on Linux and Windows, Ollama can be built without a `go generate ./...` step, making it easy to get up and running to hack on parts of Ollama that don't require fast inference\r\n3. Faster build times for AVX,AVX2,CUDA and ROCM (a full build of all runners takes <5 min on a fast CPU)\r\n4. No git submodule making it easier to clone and build from source\r\n\r\nThis is a big PR, but much of it is vendor code except for:\r\n\r\n1. `llama.go` CGo bindings\r\n2. `example/`: a simple example of running inference\r\n3. `runner/`: a subprocess server designed to replace the `llm/ext_server` package\r\n4. `Makefile` an as minimal as possible `Makefile` to build the `runner` package for different targets (cpu, avx, avx2, cuda, rocm)\r\n\r\nThe easiest way to try out the PR:\r\n\r\n```\r\ncd llama\r\nmake -j\r\n```\r\n\r\nWhich will produce `ollama_runner` binaries based on the current platform.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update 40xx gpu compat matrix",
            "url": "https://github.com/ollama/ollama/pull/5036",
            "state": "MERGED",
            "createdAt": "2024-06-14T00:04:06Z",
            "mergedAt": "2024-06-14T00:10:33Z",
            "closedAt": "2024-06-14T00:10:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "More parallelism on windows generate",
            "url": "https://github.com/ollama/ollama/pull/5037",
            "state": "MERGED",
            "createdAt": "2024-06-14T00:14:36Z",
            "mergedAt": "2024-06-15T15:03:05Z",
            "closedAt": "2024-06-15T15:03:06Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 51,
            "deletions": 21,
            "body": "Make the build faster\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adds an uninstall script to the installer",
            "url": "https://github.com/ollama/ollama/pull/5043",
            "state": "CLOSED",
            "createdAt": "2024-06-14T08:17:31Z",
            "mergedAt": null,
            "closedAt": "2024-09-05T05:14:16Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 66,
            "deletions": 0,
            "body": "A new script called ollama_uninstall.sh gets created as part of the installation process on Linux. Running this will remove the ollama installation.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "openai: do not set temperature to 0 when setting seed",
            "url": "https://github.com/ollama/ollama/pull/5045",
            "state": "MERGED",
            "createdAt": "2024-06-14T16:24:20Z",
            "mergedAt": "2024-06-14T20:43:56Z",
            "closedAt": "2024-06-14T20:43:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 4,
            "body": "`tempearture` was previously set to 0 for reproducible outputs when setting seed, however this is not required\r\n\r\nNote https://github.com/ollama/ollama/issues/4990 is still an open issue on Nvidia/AMD GPUs\r\n\r\nFixes https://github.com/ollama/ollama/issues/5044",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: longer timeout in `TestRequests`",
            "url": "https://github.com/ollama/ollama/pull/5046",
            "state": "MERGED",
            "createdAt": "2024-06-14T16:37:12Z",
            "mergedAt": "2024-06-14T16:48:25Z",
            "closedAt": "2024-06-14T16:48:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "@dhiltgen this seems like a band-aid - is there something deeper we should fix in this test?",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Cuda v12",
            "url": "https://github.com/ollama/ollama/pull/5049",
            "state": "MERGED",
            "createdAt": "2024-06-14T20:56:22Z",
            "mergedAt": "2024-08-19T18:14:24Z",
            "closedAt": "2024-08-19T18:14:24Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 23
            },
            "additions": 447,
            "deletions": 217,
            "body": "This builds upon the new linux packaging model in #5631 to support building 2 different CUDA runners: v11 for support going back to CC 5.0, and v12 for CC 6.0 and up GPUs.  This allows us to start enabling new features such as `GGML_CUDA_USE_GRAPHS` which require cuda v12 support without dropping support for older GPUs.\r\n\r\nFixes #4958\r\nFixes #5737\r\nFixes #2361\r\nFixes #6144\r\n\r\n\r\nResulting sizes:\r\n```\r\n% ls -lh dist/*.xz\r\n-rw-r--r--  1 daniel  staff   1.4G Aug 12 11:43 dist/ollama-linux-amd64.tar.xz\r\n-rw-r--r--  1 daniel  staff   1.5G Aug 12 12:11 dist/ollama-linux-arm64.tar.xz\r\n```\r\n```\r\ntime=2024-07-12T20:24:36.369Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60101 cpu]\"\r\n```\r\n\r\n",
            "participants": {
                "totalCount": 7
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "add model capabilities",
            "url": "https://github.com/ollama/ollama/pull/5051",
            "state": "MERGED",
            "createdAt": "2024-06-14T21:29:14Z",
            "mergedAt": "2024-07-02T21:26:07Z",
            "closedAt": "2024-07-02T21:26:07Z",
            "reviews": {
                "totalCount": 21
            },
            "files": {
                "totalCount": 31
            },
            "additions": 353,
            "deletions": 191,
            "body": "detect completion capability by looking at model KVs. with this change, ollama correctly detects a model like [jina/jina-embeddings-v2-small-en](https://ollama.com/jina/jina-embeddings-v2-small-en) is an embedding model (as opposed to a text completion model)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "gpu: Fix build warning",
            "url": "https://github.com/ollama/ollama/pull/5058",
            "state": "MERGED",
            "createdAt": "2024-06-15T06:28:51Z",
            "mergedAt": "2024-06-15T18:52:36Z",
            "closedAt": "2024-06-15T18:52:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fix build warning\r\n```\r\n# github.com/ollama/ollama/gpu\r\ngpu_info_oneapi.c: In function \u2018oneapi_check_vram\u2019:\r\ngpu_info_oneapi.c:163:51: warning: format not a string literal and no format arguments [-Wformat-security]\r\n  163 |   snprintf(&resp->gpu_name[0], GPU_NAME_LEN, props.modelName);\r\n      |                                              ~~~~~^~~~~~~~~~\r\n\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add LSP-AI to README",
            "url": "https://github.com/ollama/ollama/pull/5063",
            "state": "MERGED",
            "createdAt": "2024-06-15T14:10:47Z",
            "mergedAt": "2024-09-05T05:17:34Z",
            "closedAt": "2024-09-05T05:17:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Implement custom github release action",
            "url": "https://github.com/ollama/ollama/pull/5069",
            "state": "MERGED",
            "createdAt": "2024-06-15T18:31:43Z",
            "mergedAt": "2024-06-17T20:59:37Z",
            "closedAt": "2024-06-17T20:59:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 18,
            "deletions": 12,
            "body": "This implements the release logic we want via gh cli to support updating releases with rc tags in place and retain release notes and other community reactions.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move libraries out of users path",
            "url": "https://github.com/ollama/ollama/pull/5072",
            "state": "MERGED",
            "createdAt": "2024-06-15T20:19:05Z",
            "mergedAt": "2024-06-19T16:13:40Z",
            "closedAt": "2024-06-19T16:13:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 41,
            "deletions": 26,
            "body": "We update the PATH on windows to get the CLI mapped, but this has an unintended side effect of causing other apps that may use our bundled DLLs to get terminated when we upgrade.\r\n\r\nFixes #5050 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Implement log rotation for tray app",
            "url": "https://github.com/ollama/ollama/pull/5074",
            "state": "MERGED",
            "createdAt": "2024-06-15T23:40:23Z",
            "mergedAt": "2024-06-19T20:02:24Z",
            "closedAt": "2024-06-19T20:02:24Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 86,
            "deletions": 9,
            "body": "Fixes #4770 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "gpu: add env var for detecting Intel oneapi gpus",
            "url": "https://github.com/ollama/ollama/pull/5076",
            "state": "MERGED",
            "createdAt": "2024-06-16T01:53:48Z",
            "mergedAt": "2024-06-17T00:09:05Z",
            "closedAt": "2024-06-17T00:09:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 35,
            "deletions": 26,
            "body": "Fixes https://github.com/ollama/ollama/issues/5073 until we can find the root cause",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add some more debugging logs for intel discovery",
            "url": "https://github.com/ollama/ollama/pull/5080",
            "state": "MERGED",
            "createdAt": "2024-06-16T14:43:54Z",
            "mergedAt": "2024-06-16T21:42:42Z",
            "closedAt": "2024-06-16T21:42:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 3,
            "body": "Also removes an unused overall count variable\r\n\r\nUntil we can find a repro to fully root cause the crash, this may help narrow the search space.\r\n\r\nRelated to #5073 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix a build warning again",
            "url": "https://github.com/ollama/ollama/pull/5096",
            "state": "MERGED",
            "createdAt": "2024-06-17T10:10:51Z",
            "mergedAt": "2024-06-17T18:47:48Z",
            "closedAt": "2024-06-17T18:47:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "With the latest main branch, there is a build warning\r\n```\r\n# github.com/ollama/ollama/gpu\r\nIn file included from gpu_info_oneapi.h:4,\r\n                 from gpu_info_oneapi.c:3:\r\ngpu_info_oneapi.c: In function \u2018oneapi_init\u2019:\r\ngpu_info_oneapi.c:101:27: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 3 has type \u2018zes_driver_handle_t\u2019 {aka \u2018struct _zes_driver_handle_t *\u2019} [-Wformat=]\r\n  101 |     LOG(resp->oh.verbose, \"calling zesDeviceGet %d\\n\", resp->oh.drivers[d]);\r\n      |                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~~~~~~~~~~~~~~\r\n      |                                                                        |\r\n      |                                                                        zes_driver_handle_t {aka struct _zes_driver_handle_t *}\r\ngpu_info.h:33:23: note: in definition of macro \u2018LOG\u2019\r\n   33 |       fprintf(stderr, __VA_ARGS__); \\\r\n      |                       ^~~~~~~~~~~\r\ngpu_info_oneapi.c:101:50: note: format string is defined here\r\n  101 |     LOG(resp->oh.verbose, \"calling zesDeviceGet %d\\n\", resp->oh.drivers[d]);\r\n      |                                                 ~^\r\n      |                                                  |\r\n      |                                                  int\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert powershell jobs, but keep nvcc and cmake parallelism",
            "url": "https://github.com/ollama/ollama/pull/5103",
            "state": "MERGED",
            "createdAt": "2024-06-17T20:50:05Z",
            "mergedAt": "2024-06-17T21:23:18Z",
            "closedAt": "2024-06-17T21:23:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 20,
            "deletions": 37,
            "body": "It doesn't look like the added complexity of trying to parallelize in powershell is worth it, so remove that, but retain the other parallelism flags for cmake and nvcc.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adjust mmap logic for cuda windows for faster model load",
            "url": "https://github.com/ollama/ollama/pull/5105",
            "state": "MERGED",
            "createdAt": "2024-06-17T22:57:41Z",
            "mergedAt": "2024-06-18T00:07:31Z",
            "closedAt": "2024-06-18T00:07:31Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 96,
            "deletions": 15,
            "body": "On Windows, recent llama.cpp changes make mmap slower in most cases, so default to off.  This also implements a tri-state for use_mmap so we can detect the difference between a user provided value of true/false, or unspecified.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Tighten up memory prediction logging",
            "url": "https://github.com/ollama/ollama/pull/5106",
            "state": "MERGED",
            "createdAt": "2024-06-18T02:11:21Z",
            "mergedAt": "2024-06-18T16:24:38Z",
            "closedAt": "2024-06-18T16:24:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 66,
            "deletions": 44,
            "body": "Prior to this change, we logged the memory prediction multiple times as the scheduler iterates to find a suitable configuration, which can be confusing since only the last log before the server starts is actually valid. This now logs once just before starting the server on the final configuration. It also reports what library instead of always saying \"offloading to gpu\" when using CPU.\r\n\r\nA few examples (non-debug regular logging level):\r\n```\r\ntime=2024-06-17T19:07:15.507-07:00 level=INFO source=types.go:98 msg=\"inference compute\" id=0 library=metal compute=\"\" driver=0.0 name=\"\" total=\"96.0 GiB\" available=\"96.0 GiB\"\r\n[GIN] 2024/06/17 - 19:07:33 | 200 |     313.875\u00b5s |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/06/17 - 19:07:33 | 200 |    2.473333ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-06-17T19:07:33.438-07:00 level=INFO source=memory.go:303 msg=\"offload to metal\" layers.requested=-1 layers.model=27 layers.offload=27 layers.split=\"\" memory.available=\"[96.0 GiB]\" memory.required.full=\"3.2 GiB\" memory.required.partial=\"3.2 GiB\" memory.required.kv=\"650.0 MiB\" memory.required.allocations=\"[3.2 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"2.2 GiB\" memory.weights.nonrepeating=\"103.8 MiB\" memory.graph.full=\"157.0 MiB\" memory.graph.partial=\"157.0 MiB\"\r\ntime=2024-06-17T19:07:33.439-07:00 level=INFO source=server.go:359 msg=\"starting llama server\" cmd=\"/var/folders/hs/0tcx8spd1vv390h0j6jq5vq80000gn/T/ollama3083603568/runners/metal/ollama_llama_server --model /Users/daniel/.ollama/models/blobs/sha256-66002b78c70a22ab25e16cc9a1736c6cc6335398c7312e3eb33db202350afe66 --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 27 --parallel 1 --port 63538\"\r\n```\r\n\r\n```\r\ntime=2024-06-18T02:09:26.404Z level=WARN source=gpu.go:225 msg=\"CPU does not have minimum vector extensions, GPU inference disabled\" required=avx detected=\"no vector extensions\"\r\ntime=2024-06-18T02:09:26.405Z level=INFO source=types.go:98 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"31.3 GiB\" available=\"30.4 GiB\"\r\n[GIN] 2024/06/18 - 02:09:36 | 200 |     662.875\u00b5s |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/06/18 - 02:09:36 | 200 |    3.819958ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-06-18T02:09:36.898Z level=INFO source=memory.go:303 msg=\"offload to cpu\" layers.requested=-1 layers.model=27 layers.offload=0 layers.split=\"\" memory.available=\"[30.4 GiB]\" memory.required.full=\"2.7 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"650.0 MiB\" memory.required.allocations=\"[2.7 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"2.2 GiB\" memory.weights.nonrepeating=\"103.8 MiB\" memory.graph.full=\"157.0 MiB\" memory.graph.partial=\"177.2 MiB\"\r\ntime=2024-06-18T02:09:36.905Z level=INFO source=server.go:359 msg=\"starting llama server\" cmd=\"/tmp/ollama284292296/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-66002b78c70a22ab25e16cc9a1736c6cc6335398c7312e3eb33db202350afe66 --ctx-size 2048 --batch-size 512 --embedding --log-disable --parallel 1 --port 43251\"\r\n```\r\n\r\n```\r\ntime=2024-06-17T19:02:51.398-07:00 level=INFO source=types.go:98 msg=\"inference compute\" id=GPU-1c750365-54dc-7082-7c6b-9dd953a68ab6 library=cuda compute=6.1 driver=12.3 name=\"NVIDIA GeForce GTX 1060 6GB\" total=\"5.9 GiB\" available=\"5.7 GiB\"\r\n[GIN] 2024/06/17 - 19:02:57 | 200 |      28.835\u00b5s |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/06/17 - 19:02:57 | 200 |     453.399\u00b5s |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-06-17T19:02:58.233-07:00 level=INFO source=memory.go:303 msg=\"offload to cuda\" layers.requested=-1 layers.model=27 layers.offload=27 layers.split=\"\" memory.available=\"[5.7 GiB]\" memory.required.full=\"3.1 GiB\" memory.required.partial=\"3.1 GiB\" memory.required.kv=\"650.0 MiB\" memory.required.allocations=\"[3.1 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"2.2 GiB\" memory.weights.nonrepeating=\"103.8 MiB\" memory.graph.full=\"157.0 MiB\" memory.graph.partial=\"177.2 MiB\"\r\ntime=2024-06-17T19:02:58.233-07:00 level=INFO source=server.go:359 msg=\"starting llama server\" cmd=\"/tmp/ollama3201791839/runners/cuda_v11/ollama_llama_server --model /home/daniel/.ollama/models/blobs/sha256-66002b78c70a22ab25e16cc9a1736c6cc6335398c7312e3eb33db202350afe66 --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 27 --parallel 1 --port 43155\"\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Handle models with divergent layer sizes",
            "url": "https://github.com/ollama/ollama/pull/5117",
            "state": "MERGED",
            "createdAt": "2024-06-18T18:06:13Z",
            "mergedAt": "2024-06-18T18:36:51Z",
            "closedAt": "2024-06-18T18:36:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "The recent refactoring of the memory prediction assumed all layers are the same size, but for some models (like deepseek-coder-v2) this is not the case, so our predictions were significantly off.\r\n\r\n\r\nWithout the fix:\r\n```\r\ntime=2024-06-18T11:03:42.708-07:00 level=INFO source=memory.go:303 msg=\"offload to metal\" layers.requested=-1 layers.model=28 layers.offload=28 layers.split=\"\" memory.available=\"[96.0 GiB]\" memory.required.full=\"2.4 GiB\" memory.required.partial=\"2.4 GiB\" memory.required.kv=\"432.0 MiB\" memory.required.allocations=\"[2.4 GiB]\" memory.weights.total=\"1.6 GiB\" memory.weights.repeating=\"1.4 GiB\" memory.weights.nonrepeating=\"164.1 MiB\" memory.graph.full=\"72.0 MiB\" memory.graph.partial=\"72.0 MiB\"\r\n```\r\n\r\nWith the fix:\r\n```\r\ntime=2024-06-18T11:02:47.707-07:00 level=INFO source=memory.go:309 msg=\"offload to metal\" layers.requested=-1 layers.model=28 layers.offload=28 layers.split=\"\" memory.available=\"[96.0 GiB]\" memory.required.full=\"9.2 GiB\" memory.required.partial=\"9.2 GiB\" memory.required.kv=\"432.0 MiB\" memory.required.allocations=\"[9.2 GiB]\" memory.weights.total=\"8.4 GiB\" memory.weights.repeating=\"8.3 GiB\" memory.weights.nonrepeating=\"164.1 MiB\" memory.graph.full=\"72.0 MiB\" memory.graph.partial=\"72.0 MiB\"\r\n```\r\n\r\nPartial fix for #5113 but we'll need additional graph updates...",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add a few missing server settings and sort the list",
            "url": "https://github.com/ollama/ollama/pull/5119",
            "state": "CLOSED",
            "createdAt": "2024-06-18T18:28:48Z",
            "mergedAt": null,
            "closedAt": "2024-07-29T21:26:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 4,
            "body": "Fixes #5093 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "deepseek v2 graph",
            "url": "https://github.com/ollama/ollama/pull/5121",
            "state": "MERGED",
            "createdAt": "2024-06-18T20:18:43Z",
            "mergedAt": "2024-06-18T23:30:58Z",
            "closedAt": "2024-06-18T23:30:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 0,
            "body": "Fixes #5113\r\nFixes #4799 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove Digest",
            "url": "https://github.com/ollama/ollama/pull/5122",
            "state": "MERGED",
            "createdAt": "2024-06-18T20:31:51Z",
            "mergedAt": "2024-06-19T03:28:11Z",
            "closedAt": "2024-06-19T03:28:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 89,
            "body": "The Digest type in its current form is awkward to work with and presents challenges with regard to how it serializes via String using the '-' prefix.\r\n\r\nWe currently only use this in ollama.com, so we'll move our specific needs around digest parsing and validation there.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Wire up windows AMD driver reporting",
            "url": "https://github.com/ollama/ollama/pull/5124",
            "state": "MERGED",
            "createdAt": "2024-06-18T23:24:42Z",
            "mergedAt": "2024-07-10T19:50:23Z",
            "closedAt": "2024-07-10T19:50:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 13,
            "body": "This seems to be ROCm version, not actually driver version, but it may be useful for toggling logic for VRAM reporting in the future\r\n\r\n\r\nBefore:\r\n```\r\ntime=2024-06-18T15:56:19.574-07:00 level=INFO source=types.go:98 msg=\"inference compute\" id=1 library=rocm compute=gfx1100 driver=0.0 name=\"AMD Radeon RX 7900 XTX\" total=\"24.0 GiB\" available=\"23.9 GiB\"\r\n```\r\n\r\nAfter:\r\n```\r\ntime=2024-06-18T16:15:27.513-07:00 level=INFO source=types.go:98 msg=\"inference compute\" id=1 library=rocm compute=gfx1100 driver=5.7 name=\"AMD Radeon RX 7900 XTX\" total=\"24.0 GiB\" available=\"23.9 GiB\"\r\n```\r\n\r\nfwiw, this version string doesn't seem to be wired to the actual driver version.    On this same test system:\r\n```\r\nGet-WmiObject Win32_VideoController | format-table Name, Description,VideoProcessor,DriverVersion\r\n\r\nName                    Description             VideoProcessor                         DriverVersion\r\n----                    -----------             --------------                         -------------\r\nAMD Radeon(TM) Graphics AMD Radeon(TM) Graphics AMD Radeon Graphics Processor (0x164E) 31.0.24019.1006\r\nAMD Radeon RX 7900 XTX  AMD Radeon RX 7900 XTX  AMD Radeon Graphics Processor (0x744C) 31.0.24019.1006\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump latest fedora cuda repo to 39",
            "url": "https://github.com/ollama/ollama/pull/5125",
            "state": "MERGED",
            "createdAt": "2024-06-19T00:15:15Z",
            "mergedAt": "2024-06-20T18:27:24Z",
            "closedAt": "2024-06-20T18:27:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes #5062 \r\n\r\nFedora39 is now the latest.\r\n\r\nhttps://developer.download.nvidia.com/compute/cuda/repos/ ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update message processing",
            "url": "https://github.com/ollama/ollama/pull/5126",
            "state": "MERGED",
            "createdAt": "2024-06-19T00:32:04Z",
            "mergedAt": "2024-07-09T16:20:44Z",
            "closedAt": "2024-07-09T16:20:44Z",
            "reviews": {
                "totalCount": 38
            },
            "files": {
                "totalCount": 7
            },
            "additions": 677,
            "deletions": 709,
            "body": "this change changes the way messages are processed before handing off to the llm. there are a few areas worth mentioning:\r\n\r\n1. messages are now a first class component of the template. template rendering will only falling back to the previous iterative template if messages is unsupported by the template. however, new models _should_ implement the previous prompt/response template for compatibility with older ollama versions\r\n2. the generate endpoint has been updated to use messages for prompt templating but the end result should be the same\r\n3. the chat endpoint has been updated to preprocess incoming messages\r\n\r\n    - continuous messages of the same role are joined into a single message, separated with two newlines\r\n    - content and image data can be interleaved by sending messages with alternating fields, e.g. \r\n        ```\r\n        [\r\n            {\"role\": \"user\", \"content\": \"Consider the following images:\"},\r\n            {\"role\": \"user\", \"images\": [\"<base64 image data>\", \"<base64 image data>\"]},\r\n            {\"role\": \"user\", \"content\": \"What is the difference between the two images?\"}\r\n        ]\r\n        ```\r\n    - system messages are aggregated and prepended to the _last_ user message\r\n",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Introduce `/api/embed` endpoint supporting batch embedding",
            "url": "https://github.com/ollama/ollama/pull/5127",
            "state": "MERGED",
            "createdAt": "2024-06-19T00:35:06Z",
            "mergedAt": "2024-07-15T19:14:24Z",
            "closedAt": "2024-07-15T19:14:24Z",
            "reviews": {
                "totalCount": 58
            },
            "files": {
                "totalCount": 8
            },
            "additions": 453,
            "deletions": 31,
            "body": "Resolves #4224\r\nCloses #3642\r\nMentioned in #962",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Fix levelzero empty symbol detect",
            "url": "https://github.com/ollama/ollama/pull/5128",
            "state": "MERGED",
            "createdAt": "2024-06-19T01:30:27Z",
            "mergedAt": "2024-06-19T15:33:16Z",
            "closedAt": "2024-06-19T15:33:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 27,
            "deletions": 36,
            "body": "we notice many users report ollama crash on Windows platform when ollama try to discover Intel GPUs after introducing Intel GPUs support.\r\nthis issue may occur on some older Intel CPUs which before 11Gen. \r\nthis is due to the driver library of iGPUs before 11Gen processors don't have the symbols which initial level-zero needed, like `zesInit` , and there is also a bug in `oneapi_init`, `l[i].p` is a pointer to pointer, func ptr stored in `*l[i].p` so we should detect whether `*l[i].p` is null ptr to check whether we load the func successfully.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update requirements.txt",
            "url": "https://github.com/ollama/ollama/pull/5139",
            "state": "MERGED",
            "createdAt": "2024-06-19T11:07:36Z",
            "mergedAt": "2024-09-12T01:56:56Z",
            "closedAt": "2024-09-12T01:56:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "With chromadb==0.4.7, ingest.py still fails with \r\n`Cannot submit more than 5,461 embeddings at once. Please submit your embeddings in batches of size 5,461 or less.`\r\n\r\nSee \r\n- https://github.com/ollama/ollama/issues/4476\r\n- https://github.com/ollama/ollama/issues/2572\r\n- https://github.com/ollama/ollama/issues/533",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix bad symbol load detection",
            "url": "https://github.com/ollama/ollama/pull/5145",
            "state": "MERGED",
            "createdAt": "2024-06-19T15:56:58Z",
            "mergedAt": "2024-06-19T16:12:33Z",
            "closedAt": "2024-06-19T16:12:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 3,
            "deletions": 3,
            "body": "pointer deref's weren't correct on a few libraries, which explains some crashes on older systems or miswired symlinks for discovery libraries.\r\n\r\nFixes #4982 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Put back temporary intel GPU env var",
            "url": "https://github.com/ollama/ollama/pull/5146",
            "state": "MERGED",
            "createdAt": "2024-06-19T15:58:50Z",
            "mergedAt": "2024-06-19T16:12:45Z",
            "closedAt": "2024-06-19T16:12:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 35,
            "deletions": 26,
            "body": "Until we merge #4876 lets keep the opt-in env var to avoid confusion in the binary releases if we discover an Intel GPU but don't actually have the runner built in.\r\n\r\nThis reverts commit 755b4e4fc291366595ed7bfb37c2a91ff5834df8.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove confusing log message",
            "url": "https://github.com/ollama/ollama/pull/5147",
            "state": "MERGED",
            "createdAt": "2024-06-19T18:18:08Z",
            "mergedAt": "2024-06-19T19:50:31Z",
            "closedAt": "2024-06-19T19:50:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 17,
            "body": "chat template isn't being used so there's no reason to validate",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update Show Docs",
            "url": "https://github.com/ollama/ollama/pull/5149",
            "state": "MERGED",
            "createdAt": "2024-06-19T20:38:14Z",
            "mergedAt": "2024-06-21T22:52:10Z",
            "closedAt": "2024-06-21T22:52:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update OpenAI Compatibility Docs with /v1/models",
            "url": "https://github.com/ollama/ollama/pull/5151",
            "state": "MERGED",
            "createdAt": "2024-06-19T22:09:40Z",
            "mergedAt": "2024-08-01T22:48:44Z",
            "closedAt": "2024-08-01T22:48:44Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: skip os.removeAll() if PID does not exist",
            "url": "https://github.com/ollama/ollama/pull/5188",
            "state": "MERGED",
            "createdAt": "2024-06-20T15:54:26Z",
            "mergedAt": "2024-06-20T17:40:59Z",
            "closedAt": "2024-06-20T17:40:59Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 19,
            "deletions": 12,
            "body": "previously deleted all directories in $TMPDIR starting with ollama. Added a \"continue\" to skip the directory removal if a PID doesn't exist. We do this to prevent accidentally deleting directories in tmpdir that share the ollama name but aren't created by us for processes\r\n\r\nresolves: https://github.com/ollama/ollama/issues/5129",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove Quotes from Parameters in Ollama Show",
            "url": "https://github.com/ollama/ollama/pull/5190",
            "state": "CLOSED",
            "createdAt": "2024-06-20T16:08:03Z",
            "mergedAt": null,
            "closedAt": "2024-08-11T22:19:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 10,
            "body": "Resolves #5183 \r\n\r\nBefore:\r\n<img width=\"492\" alt=\"Screenshot 2024-06-20 at 9 07 56\u202fAM\" src=\"https://github.com/ollama/ollama/assets/65097070/0c6e8f89-854b-4069-a387-dc191766ee70\">\r\n\r\nAfter:\r\n\r\n<img width=\"502\" alt=\"Screenshot 2024-06-20 at 9 07 17\u202fAM\" src=\"https://github.com/ollama/ollama/assets/65097070/f6d69913-1bb1-47cf-b8a9-d0e6dc3c6e70\">\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "handle asymmetric embedding KVs",
            "url": "https://github.com/ollama/ollama/pull/5192",
            "state": "MERGED",
            "createdAt": "2024-06-20T16:47:12Z",
            "mergedAt": "2024-06-20T17:46:24Z",
            "closedAt": "2024-06-20T17:46:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 35,
            "deletions": 9,
            "body": "KV size assumed a symmetric K and V embedding sizes which isn't always the case, e.g. deepseek v2\r\n\r\nsmoke tested memory usage against llama2, llama3, gemma, phi3, qwen2, and deepseek v2",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Correct Ollama Show Precision of Parameter",
            "url": "https://github.com/ollama/ollama/pull/5193",
            "state": "CLOSED",
            "createdAt": "2024-06-20T17:44:34Z",
            "mergedAt": null,
            "closedAt": "2024-09-05T02:49:55Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 64,
            "deletions": 6,
            "body": "Resolves #5184\r\n\r\nThe HumanNumber function is only used elsewhere on line 434 of `images.go`, changing it could introduce precision inconsistencies with pulled vs created models though. Can create a separate function.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Refine mmap default logic on linux",
            "url": "https://github.com/ollama/ollama/pull/5194",
            "state": "MERGED",
            "createdAt": "2024-06-20T18:08:40Z",
            "mergedAt": "2024-06-20T18:44:08Z",
            "closedAt": "2024-06-20T18:44:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 12,
            "body": "If we try to use mmap when the model is larger than the system free space, loading is slower than the no-mmap approach.\r\n\r\nThis should resolve multiple issues where model loads stalled for longer than 5 minutes on some systems and caused our timeout to trigger.  When those users forced use_mmap=false, things sped up significantly, so this should handle it automatically.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "include modelfile messages",
            "url": "https://github.com/ollama/ollama/pull/5196",
            "state": "MERGED",
            "createdAt": "2024-06-20T21:12:40Z",
            "mergedAt": "2024-07-31T17:18:17Z",
            "closedAt": "2024-07-31T17:18:17Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 39,
            "deletions": 43,
            "body": "this change moves the CLI behaviour of prepending Modelfile messages to chat conversations to the server. This allows API calls to easily use these fields. It also extends this to /api/generate requests where messages will only be prepended if context is empty\r\n\r\nmissing some tests\r\n\r\nresolves https://github.com/ollama/ollama-python/issues/117\r\nresolves https://github.com/ollama/ollama-js/issues/109",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix use_mmap parsing for modelfiles",
            "url": "https://github.com/ollama/ollama/pull/5205",
            "state": "MERGED",
            "createdAt": "2024-06-21T19:28:16Z",
            "mergedAt": "2024-06-21T23:30:36Z",
            "closedAt": "2024-06-21T23:30:36Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 76,
            "deletions": 0,
            "body": "Add the new tristate parsing logic for the code path for modelfiles, as well as a unit test.\r\n\r\nFixes #5198 \r\n\r\n\r\nManually confirmed as well:\r\n```\r\n% cat use_mmap.modelfile\r\nFROM library/llama2\r\nPARAMETER use_mmap false\r\n% ollama create foo -f ./use_mmap.modelfile\r\ntransferring model data\r\nusing existing layer sha256:8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246\r\nusing existing layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b\r\nusing existing layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d\r\nusing existing layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988\r\nusing existing layer sha256:fa304d6750612c207b8705aca35391761f29492534e90b30575e4980d6ca82f6\r\nusing existing layer sha256:04f36b167ddc372657a153cf129aa32f1f6d5bd4440ce92c5cb4f76f7844417e\r\nwriting manifest\r\nsuccess\r\n```\r\n\r\nWhere before it would respond with:\r\n```\r\n% ollama create foo -f ./use_mmap.modelfile\r\ntransferring model data\r\nError: invalid int value [false]\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: quantization with template",
            "url": "https://github.com/ollama/ollama/pull/5206",
            "state": "MERGED",
            "createdAt": "2024-06-21T20:30:52Z",
            "mergedAt": "2024-06-21T20:44:35Z",
            "closedAt": "2024-06-21T20:44:35Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 10,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add insert support to generate endpoint",
            "url": "https://github.com/ollama/ollama/pull/5207",
            "state": "MERGED",
            "createdAt": "2024-06-22T00:25:01Z",
            "mergedAt": "2024-07-16T21:37:32Z",
            "closedAt": "2024-07-16T21:37:32Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 6
            },
            "additions": 155,
            "deletions": 27,
            "body": "this change is triggered by the presence of \"suffix\", particularly useful for code completion tasks\r\n\r\nexamples templates\r\n\r\ncodellama merged code completion and fill-in-middle template\r\n```\r\n{{- if .Suffix }}<PRE> {{ .Prompt }} <SUF>{{ .Suffix }} <MID>\r\n{{- else }}{{ Prompt }}\r\n{{- end }}\r\n```\r\n\r\ndeepseek coder v2 merged messages #5126 and fill-in-middle template\r\n```\r\n{{- if .Suffix }}<\uff5cfim\u2581begin\uff5c>{{ .Prompt }}\r\n<\uff5cfim\u2581hole\uff5c>\r\n{{ .Suffix }}<\uff5cfim\u2581end\uff5c>\r\n{{- else }}\r\n{{- range .Messages }}\r\n{{- if eq .Role \"user\" }}\r\n{{- if and (eq (index $.Messages (sub (len $.Messages) 1)) .) $.System }}{{ $.System }}{{ \"\\n\\n\" }}\r\n{{- end }}User: {{ .Content }}{{ \"\\n\\n\" }}\r\n{{- else if eq .Role \"assistant\" }}Assistant: {{ .Content }}<\uff5cend\u2581of\u2581sentence\uff5c>\r\n{{- end }}\r\n{{- end }}Assistant:\r\n{{- end }}\r\n```\r\n\r\nexample request\r\n\r\n```\r\ncurl 127.0.0.1:11434/api/generate -d '{\r\n    \"model\": \"deepseek-coder-v2\",\r\n    \"prompt\": \"def add(\",\r\n    \"suffix\": \"return c\",\r\n    \"stream\": false,\r\n    \"options\": {\r\n        \"temperature\": 0\r\n    }\r\n}'\r\n```\r\n\r\nmodels that do no support `suffix` will return error `model does not support [suffix]`\r\n\r\nan example fill-in-middle model is [mike/deepseek-coder-v2](https://ollama.com/mike/deepseek-coder-v2)\r\n\r\nresolves #496 \r\nresolves #3869 \r\nresolves #5403 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Support image input for OpenAI chat compatibility",
            "url": "https://github.com/ollama/ollama/pull/5208",
            "state": "MERGED",
            "createdAt": "2024-06-22T00:31:49Z",
            "mergedAt": "2024-07-14T05:07:45Z",
            "closedAt": "2024-07-14T05:07:45Z",
            "reviews": {
                "totalCount": 21
            },
            "files": {
                "totalCount": 2
            },
            "additions": 119,
            "deletions": 6,
            "body": "Supports passing in base64 encoded image into image_url.\r\n\r\nE.g.\r\n\r\n```\r\ncurl http://localhost:11434/v1/chat/completions \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"model\": \"llava\",\r\n    \"messages\": [\r\n      {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n          {\r\n            \"type\": \"text\",\r\n            \"text\": \"What'\\''s in this image?\"\r\n          },\r\n          {\r\n            \"type\": \"image_url\",\r\n            \"image_url\": {\r\n               \"url\": \"'$image'\"\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    ],\r\n    \"max_tokens\": 300\r\n  }' | jq\r\n```\r\n\r\n```\r\n{\r\n  \"id\": \"chatcmpl-659\",\r\n  \"object\": \"chat.completion\",\r\n  \"created\": 1719016156,\r\n  \"model\": \"llava\",\r\n  \"system_fingerprint\": \"fp_ollama\",\r\n  \"choices\": [\r\n    {\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"content\": \" The image shows a cute cartoon of an animal. It appears to be a dog or similar creature, styled with exaggerated features typical in internet memes. The character has big eyes, a round face, and its arms are raised in the air, as if waving or giving a thumbs-up gesture. There's also some motion blur that gives the impression of movement, suggesting the animal might be jumping or dancing. This kind of image is often used in digital communication to convey emotions or add a playful element to text messages. \"\r\n      },\r\n      \"finish_reason\": \"stop\"\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 1,\r\n    \"completion_tokens\": 112,\r\n    \"total_tokens\": 113\r\n  }\r\n}\r\n```\r\n\r\nResolves #3690, #5304",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 8
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: v1/completions compatibility",
            "url": "https://github.com/ollama/ollama/pull/5209",
            "state": "MERGED",
            "createdAt": "2024-06-22T00:46:53Z",
            "mergedAt": "2024-07-02T23:01:45Z",
            "closedAt": "2024-07-02T23:01:45Z",
            "reviews": {
                "totalCount": 15
            },
            "files": {
                "totalCount": 3
            },
            "additions": 353,
            "deletions": 3,
            "body": "`curl http://localhost:11434/v1/completions \\\r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"llama3\",\r\n        \"prompt\": \"Say this is a test\",\r\n        \"temperature\": 0\r\n    }' | jq`\r\n\r\n```\r\n{\r\n  \"id\": \"cmpl-210\",\r\n  \"object\": \"text_completion\",\r\n  \"created\": 1719010238,\r\n  \"model\": \"llama3\",\r\n  \"system_fingerprint\": \"fp_ollama\",\r\n  \"choices\": [\r\n    {\r\n      \"text\": \"It looks like you're testing to see if I'm paying attention! Well, I am! You said \\\"Say this is a test\\\" - that's a classic phrase often used in tests or assessments. Am I right?\",\r\n      \"index\": 0,\r\n      \"finish_reason\": \"stop\"\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 0,\r\n    \"completion_tokens\": 46,\r\n    \"total_tokens\": 46\r\n  }\r\n}\r\n```\r\n\r\nResolves #3027 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/5214",
            "state": "MERGED",
            "createdAt": "2024-06-22T15:08:49Z",
            "mergedAt": "2024-07-01T02:00:58Z",
            "closedAt": "2024-07-01T02:00:58Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Added Mesop example to web & desktop",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix use_mmap for modefiles",
            "url": "https://github.com/ollama/ollama/pull/5243",
            "state": "MERGED",
            "createdAt": "2024-06-23T20:02:23Z",
            "mergedAt": "2024-07-03T20:59:42Z",
            "closedAt": "2024-07-03T20:59:42Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 3
            },
            "additions": 63,
            "deletions": 93,
            "body": "PR #5205 was incomplete and missed handling numeric json values.  This switches to a pointer type to represent undefined as nil.\r\n\r\nFixes #5198\r\n\r\n```\r\n% cat use_mmap.modelfile\r\nFROM library/llama2\r\nPARAMETER use_mmap false\r\n% ollama create test -f ./use_mmap.modelfile\r\ntransferring model data\r\nusing existing layer sha256:8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246\r\n...\r\nwriting manifest\r\nsuccess\r\n% ollama run test\r\n>>> Send a message (/? for help)\r\n% grep \"starting llama server\" server.log\r\ntime=2024-06-23T12:59:32.057-07:00 level=INFO source=server.go:363 msg=\"starting llama server\" cmd=\"/tmp/ollama4152649118/runners/cpu_avx2/ollama_llama_server --model /home/daniel/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 2048 --batch-size 512 --embedding --log-disable --no-mmap --parallel 1 --port 34091\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: speed up gguf decoding by a lot",
            "url": "https://github.com/ollama/ollama/pull/5246",
            "state": "MERGED",
            "createdAt": "2024-06-24T00:07:14Z",
            "mergedAt": "2024-06-25T04:47:52Z",
            "closedAt": "2024-06-25T04:47:52Z",
            "reviews": {
                "totalCount": 16
            },
            "files": {
                "totalCount": 13
            },
            "additions": 263,
            "deletions": 69,
            "body": "Previously, some costly things were causing the loading of GGUF files\r\nand their metadata and tensor information to be VERY slow:\r\n\r\n  * Too many allocations when decoding strings\r\n  * Hitting disk for each read of each key and value, resulting in a\r\n    not-okay amount of syscalls/disk I/O.\r\n\r\nThe show API is now down to 33ms from 800ms+ for llama3 on a macbook pro\r\nm3.\r\n\r\nThis commit also prevents collecting large arrays of values when\r\ndecoding GGUFs (if desired). When such keys are encountered, their\r\nvalues are null, and are encoded as such in JSON.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "cmd: defer stating model info until necessary",
            "url": "https://github.com/ollama/ollama/pull/5248",
            "state": "MERGED",
            "createdAt": "2024-06-24T05:00:57Z",
            "mergedAt": "2024-06-25T03:14:03Z",
            "closedAt": "2024-06-25T03:14:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 46,
            "deletions": 70,
            "body": "This commit changes the 'ollama run' command to defer fetching model information until it really needs it. That is, when in interactive mode.\r\n\r\nThis positively impacts the performance of the command:\r\n\r\n    ; time ./before run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./before run llama3 'hi'  0.02s user 0.01s system 2% cpu 1.168 total\r\n    ; time ./before run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./before run llama3 'hi'  0.02s user 0.01s system 2% cpu 1.220 total\r\n    ; time ./before run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./before run llama3 'hi'  0.02s user 0.01s system 2% cpu 1.217 total\r\n    ; time ./after run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./after run llama3 'hi'  0.02s user 0.01s system 4% cpu 0.652 total\r\n    ; time ./after run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./after run llama3 'hi'  0.01s user 0.01s system 5% cpu 0.498 total\r\n    ; time ./after run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\r\n\r\n    ./after run llama3 'hi'  0.01s user 0.01s system 3% cpu 0.479 total\r\n    ; time ./after run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./after run llama3 'hi'  0.02s user 0.01s system 5% cpu 0.507 total\r\n    ; time ./after run llama3 'hi'\r\n    Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n    ./after run llama3 'hi'  0.02s user 0.01s system 5% cpu 0.507 total",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix tests",
            "url": "https://github.com/ollama/ollama/pull/5261",
            "state": "MERGED",
            "createdAt": "2024-06-24T23:20:33Z",
            "mergedAt": "2024-06-24T23:38:57Z",
            "closedAt": "2024-06-24T23:38:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 19,
            "deletions": 31,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Docs for `api/embed`",
            "url": "https://github.com/ollama/ollama/pull/5282",
            "state": "MERGED",
            "createdAt": "2024-06-25T20:56:28Z",
            "mergedAt": "2024-07-22T20:37:08Z",
            "closedAt": "2024-07-22T20:37:08Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 76,
            "deletions": 8,
            "body": "Waiting on #5127 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "tools",
            "url": "https://github.com/ollama/ollama/pull/5284",
            "state": "MERGED",
            "createdAt": "2024-06-25T21:26:54Z",
            "mergedAt": "2024-07-16T01:03:38Z",
            "closedAt": "2024-07-16T01:03:38Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 16
            },
            "additions": 621,
            "deletions": 52,
            "body": "```\r\ncurl -s 127.0.0.1:11434/api/chat -d '{\r\n  \"model\": \"mike/mistral\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"What's the weather like today in Paris?\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"tool_calls\": [\r\n        {\r\n          \"id\": \"89a1e453-0bce-4de3-a456-c54bed09c520\",\r\n          \"type\": \"function\",\r\n          \"function\": {\r\n            \"name\": \"get_current_weather\",\r\n            \"arguments\": {\r\n              \"location\": \"Paris, France\",\r\n              \"format\": \"celsius\"\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"role\": \"tool\",\r\n      \"tool_call_id\": \"89a1e453-0bce-4de3-a456-c54bed09c520\",\r\n      \"content\": \"22\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"The weather in Paris is 22 degrees celsius.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"What's the weather like today in San Francisco and Toronto?\"\r\n    }\r\n  ],\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"get_current_weather\",\r\n        \"description\": \"Get the current weather\",\r\n        \"parameters\": {\r\n          \"type\": \"object\",\r\n          \"properties\": {\r\n            \"location\": {\r\n              \"type\": \"string\",\r\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\r\n            },\r\n            \"format\": {\r\n              \"type\": \"string\",\r\n              \"enum\": [\r\n                \"celsius\",\r\n                \"fahrenheit\"\r\n              ],\r\n              \"description\": \"The temperature unit to use. Infer this from the users location.\"\r\n            }\r\n          },\r\n          \"required\": [\r\n            \"location\",\r\n            \"format\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"stream\": false,\r\n  \"options\": {\r\n    \"temperature\": 0\r\n  }\r\n}'\r\n```",
            "participants": {
                "totalCount": 10
            },
            "comments": {
                "totalCount": 11
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: /v1/embeddings compatibility",
            "url": "https://github.com/ollama/ollama/pull/5285",
            "state": "MERGED",
            "createdAt": "2024-06-25T22:47:36Z",
            "mergedAt": "2024-07-16T20:36:08Z",
            "closedAt": "2024-07-16T20:36:09Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 3
            },
            "additions": 184,
            "deletions": 0,
            "body": "In anticipation of #5127 \r\nResolves #2416 \r\n\r\n```\r\ncurl http://localhost:11434/v1/embeddings \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"input\": [\"hello\", \"hi\"],\r\n    \"model\": \"all-minilm\"\r\n  }'\r\n```\r\n\r\n```\r\n{\r\n  \"object\": \"list\",\r\n  \"data\": [\r\n    {\r\n      \"object\": \"embedding\",\r\n      \"embedding\": [\r\n        ...\r\n      ],\r\n      \"index\": 0\r\n    },\r\n    {\r\n      \"object\": \"embedding\",\r\n      \"embedding\": [\r\n        ...\r\n      ],\r\n     \"index\": 1\r\n    }\r\n  ],\r\n  \"model\": \"all-minilm\"\r\n}\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Ollama Show: Check for Projector Type",
            "url": "https://github.com/ollama/ollama/pull/5307",
            "state": "MERGED",
            "createdAt": "2024-06-26T18:22:07Z",
            "mergedAt": "2024-06-28T18:30:17Z",
            "closedAt": "2024-06-28T18:30:17Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 9,
            "deletions": 3,
            "body": "Fixes #5289 \r\n\r\n<img width=\"410\" alt=\"Screenshot 2024-06-26 at 11 21 57\u202fAM\" src=\"https://github.com/ollama/ollama/assets/65097070/4ae18164-e5c2-453b-91d4-de54569b8e11\">\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update OpenAI Compatibility Docs with /v1/models/{model}",
            "url": "https://github.com/ollama/ollama/pull/5309",
            "state": "MERGED",
            "createdAt": "2024-06-26T20:17:09Z",
            "mergedAt": "2024-08-01T22:58:13Z",
            "closedAt": "2024-08-01T22:58:13Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update OpenAI Compatibility Docs with Image Chat Support",
            "url": "https://github.com/ollama/ollama/pull/5310",
            "state": "MERGED",
            "createdAt": "2024-06-26T21:04:55Z",
            "mergedAt": "2024-08-02T20:05:57Z",
            "closedAt": "2024-08-02T20:05:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 64,
            "deletions": 4,
            "body": "Referencing #5208 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update OpenAI Compatibility Docs with /v1/completions",
            "url": "https://github.com/ollama/ollama/pull/5311",
            "state": "MERGED",
            "createdAt": "2024-06-26T21:31:03Z",
            "mergedAt": "2024-08-02T20:16:23Z",
            "closedAt": "2024-08-02T20:16:23Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 54,
            "deletions": 3,
            "body": "Referencing #5209 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI Compatibility: Correct Finish Reason Documentation ",
            "url": "https://github.com/ollama/ollama/pull/5312",
            "state": "MERGED",
            "createdAt": "2024-06-26T21:36:25Z",
            "mergedAt": "2024-06-28T16:58:14Z",
            "closedAt": "2024-06-28T16:58:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "`finish_reason` is not bound to only being `stop`\r\n\r\n```\r\ncurl http://localhost:11434/v1/chat/completions \\\r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"llama3\",\r\n        \"messages\": [\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are a helpful assistant.\"\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": \"Hello!\"\r\n            }\r\n        ],\r\n       \"max_tokens\": 1\r\n    }' | jq\r\n\r\n{\r\n  \"id\": \"chatcmpl-20\",\r\n  \"object\": \"chat.completion\",\r\n  \"created\": 1719437652,\r\n  \"model\": \"llama3\",\r\n  \"system_fingerprint\": \"fp_ollama\",\r\n  \"choices\": [\r\n    {\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"content\": \"Hello\"\r\n      },\r\n      \"finish_reason\": \"length\"\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 23,\r\n    \"completion_tokens\": 1,\r\n    \"total_tokens\": 24\r\n  }\r\n}\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "zip: prevent extracting files into parent dirs",
            "url": "https://github.com/ollama/ollama/pull/5314",
            "state": "MERGED",
            "createdAt": "2024-06-26T23:49:44Z",
            "mergedAt": "2024-06-27T04:38:21Z",
            "closedAt": "2024-06-27T04:38:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 133,
            "deletions": 22,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: architecture patch",
            "url": "https://github.com/ollama/ollama/pull/5316",
            "state": "MERGED",
            "createdAt": "2024-06-27T04:10:16Z",
            "mergedAt": "2024-06-27T04:38:13Z",
            "closedAt": "2024-06-27T04:38:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 305,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update readme for gemma 2",
            "url": "https://github.com/ollama/ollama/pull/5333",
            "state": "MERGED",
            "createdAt": "2024-06-27T16:43:43Z",
            "mergedAt": "2024-06-27T16:45:16Z",
            "closedAt": "2024-06-27T16:45:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: trim spaces for FROM argument, don't trim inside of quotes",
            "url": "https://github.com/ollama/ollama/pull/5336",
            "state": "MERGED",
            "createdAt": "2024-06-27T18:00:23Z",
            "mergedAt": "2024-07-01T23:32:46Z",
            "closedAt": "2024-07-01T23:32:46Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 2
            },
            "additions": 69,
            "deletions": 5,
            "body": "Run trimspaces on arguements before unquoting. \r\n\r\nCloses: https://github.com/ollama/ollama/issues/4998",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "gemma2 graph",
            "url": "https://github.com/ollama/ollama/pull/5340",
            "state": "MERGED",
            "createdAt": "2024-06-27T19:23:22Z",
            "mergedAt": "2024-06-27T21:26:49Z",
            "closedAt": "2024-06-27T21:26:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Include Show Info in Interactive Mode",
            "url": "https://github.com/ollama/ollama/pull/5342",
            "state": "MERGED",
            "createdAt": "2024-06-27T21:13:48Z",
            "mergedAt": "2024-06-28T20:15:52Z",
            "closedAt": "2024-06-28T20:15:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 22,
            "body": "Before:\r\n<img width=\"226\" alt=\"Screenshot 2024-06-27 at 2 13 32\u202fPM\" src=\"https://github.com/ollama/ollama/assets/65097070/b8153a26-9474-42f4-aa98-c8fc576b27e6\">\r\n\r\nAfter:\r\n<img width=\"495\" alt=\"Screenshot 2024-06-27 at 2 13 12\u202fPM\" src=\"https://github.com/ollama/ollama/assets/65097070/168699de-9d57-4a6d-9275-db3ad1e93c54\">\r\n\r\nResolves #5281 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Set default timeout to 600",
            "url": "https://github.com/ollama/ollama/pull/5345",
            "state": "CLOSED",
            "createdAt": "2024-06-27T22:00:25Z",
            "mergedAt": null,
            "closedAt": "2024-07-11T21:42:00Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Resolves #5084, #5081 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Deprecate `name` from ps response, `model` from ls response",
            "url": "https://github.com/ollama/ollama/pull/5363",
            "state": "CLOSED",
            "createdAt": "2024-06-28T20:08:31Z",
            "mergedAt": null,
            "closedAt": "2024-08-12T17:28:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 2,
            "deletions": 7,
            "body": "Keeping name for ls as removing could be severely breaking, could keep both name and model",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document concurrent behavior and settings",
            "url": "https://github.com/ollama/ollama/pull/5364",
            "state": "MERGED",
            "createdAt": "2024-06-28T20:16:56Z",
            "mergedAt": "2024-07-01T16:49:49Z",
            "closedAt": "2024-07-01T16:49:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 0,
            "body": "Merge after #4218 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "convert gemma2",
            "url": "https://github.com/ollama/ollama/pull/5365",
            "state": "MERGED",
            "createdAt": "2024-06-28T20:54:43Z",
            "mergedAt": "2024-08-21T18:48:43Z",
            "closedAt": "2024-08-21T18:48:43Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 13
            },
            "additions": 132,
            "deletions": 46,
            "body": "resolves #6426 ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Do not shift context for sliding window models",
            "url": "https://github.com/ollama/ollama/pull/5368",
            "state": "MERGED",
            "createdAt": "2024-06-29T00:49:26Z",
            "mergedAt": "2024-06-29T02:39:31Z",
            "closedAt": "2024-06-29T02:39:31Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 37,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Return Correct Prompt Eval Count Regardless of Cache Prompt",
            "url": "https://github.com/ollama/ollama/pull/5371",
            "state": "MERGED",
            "createdAt": "2024-06-29T04:04:54Z",
            "mergedAt": "2024-07-03T20:46:23Z",
            "closedAt": "2024-07-03T20:46:23Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Resolves #5370 \r\nResolves #2068 \r\nResolves https://github.com/ollama/ollama-js/issues/66",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update gpu.md",
            "url": "https://github.com/ollama/ollama/pull/5382",
            "state": "MERGED",
            "createdAt": "2024-06-29T13:50:21Z",
            "mergedAt": "2024-07-01T01:48:51Z",
            "closedAt": "2024-07-01T01:48:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Runs fine on a NVIDIA GeForce GTX 1050 Ti 4 GB GDDR5. \r\n\r\nThe 1050s without Ti have 3 GB GDDR5 and 2 GB GDDR5, that's the main difference. See: https://www.nvidia.com/en-us/geforce/10-series/",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix case for NumCtx",
            "url": "https://github.com/ollama/ollama/pull/5410",
            "state": "MERGED",
            "createdAt": "2024-07-01T16:44:37Z",
            "mergedAt": "2024-07-01T16:54:21Z",
            "closedAt": "2024-07-01T16:54:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 9,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "err on insecure path",
            "url": "https://github.com/ollama/ollama/pull/5420",
            "state": "MERGED",
            "createdAt": "2024-07-01T23:03:05Z",
            "mergedAt": "2024-07-02T21:03:23Z",
            "closedAt": "2024-07-02T21:03:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 25,
            "deletions": 7,
            "body": "`archive/zip` errors on insecure file paths which seems reasonable",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: add unsupported architecture message for linux/windows",
            "url": "https://github.com/ollama/ollama/pull/5421",
            "state": "MERGED",
            "createdAt": "2024-07-01T23:05:55Z",
            "mergedAt": "2024-07-01T23:32:14Z",
            "closedAt": "2024-07-01T23:32:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 0,
            "body": "Running unsupported models on linux/windows outputs\r\n`Error: llama runner process has terminated: signal: aborted (core dumped)`\r\n\r\nNew error message:\r\n`Error: this model is not supported by your version of Ollama. You may need to upgrade`\r\n\r\nResolves: https://github.com/ollama/ollama/issues/4889",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Centos 7 EOL broke mirrors",
            "url": "https://github.com/ollama/ollama/pull/5438",
            "state": "MERGED",
            "createdAt": "2024-07-02T16:23:13Z",
            "mergedAt": "2024-07-02T16:28:00Z",
            "closedAt": "2024-07-02T16:28:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 0,
            "body": "As of July 1st 2024: Could not resolve host: mirrorlist.centos.org This is expected due to EOL dates.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Switch ARM64 container image base to rocky 8",
            "url": "https://github.com/ollama/ollama/pull/5439",
            "state": "MERGED",
            "createdAt": "2024-07-02T17:24:21Z",
            "mergedAt": "2024-07-02T18:01:15Z",
            "closedAt": "2024-07-02T18:01:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "The centos 7 arm mirrors have disappeared due to the EOL 2 days ago, and the vault sed workaround which works for x86 doesn't work for arm.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update named templates",
            "url": "https://github.com/ollama/ollama/pull/5440",
            "state": "MERGED",
            "createdAt": "2024-07-02T18:23:38Z",
            "mergedAt": "2024-07-09T16:36:32Z",
            "closedAt": "2024-07-09T16:36:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 75
            },
            "additions": 611,
            "deletions": 27,
            "body": "update named templates with messages falling back to previous template if messages isn't available for compat",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: createBlob with copy on disk if local server",
            "url": "https://github.com/ollama/ollama/pull/5441",
            "state": "CLOSED",
            "createdAt": "2024-07-02T19:16:45Z",
            "mergedAt": null,
            "closedAt": "2024-08-28T18:36:56Z",
            "reviews": {
                "totalCount": 20
            },
            "files": {
                "totalCount": 7
            },
            "additions": 344,
            "deletions": 1,
            "body": "This PR let's users with a local server to bypass the blob upload and directly copy to models directory in server. \r\n\r\nResolves: https://github.com/ollama/ollama/issues/4600\r\n\r\nChanges:\r\nadded `Authorization` to api package to pass in Authorization headers\r\nchanged `KeyPath` and `PublicKey` methods to return objects instead of strings\r\n\r\nTODO:\r\nclean\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Add windows radeon concurrency note",
            "url": "https://github.com/ollama/ollama/pull/5442",
            "state": "MERGED",
            "createdAt": "2024-07-02T19:46:39Z",
            "mergedAt": "2024-07-02T19:47:47Z",
            "closedAt": "2024-07-02T19:47:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add conversion for microsoft phi 3 mini/medium 4k, 128k",
            "url": "https://github.com/ollama/ollama/pull/5443",
            "state": "MERGED",
            "createdAt": "2024-07-02T20:35:53Z",
            "mergedAt": "2024-08-12T22:47:58Z",
            "closedAt": "2024-08-12T22:47:58Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 8
            },
            "additions": 375,
            "deletions": 12,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update faq",
            "url": "https://github.com/ollama/ollama/pull/5446",
            "state": "MERGED",
            "createdAt": "2024-07-02T22:02:41Z",
            "mergedAt": "2024-08-23T21:05:59Z",
            "closedAt": "2024-08-23T21:05:59Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 2,
            "body": "clarify https proxy faq\r\n\r\nrelated #4834 \r\nrelated #1337 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Only set default keep_alive on initial model load",
            "url": "https://github.com/ollama/ollama/pull/5447",
            "state": "MERGED",
            "createdAt": "2024-07-02T22:35:29Z",
            "mergedAt": "2024-07-03T22:34:38Z",
            "closedAt": "2024-07-03T22:34:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 70,
            "deletions": 71,
            "body": "This change fixes the handling of keep_alive so that if client request omits the setting, we only set this on initial load.  Once the model is loaded, if new requests leave this unset, we'll keep whatever keep_alive was there.\r\n\r\nFixes #5272 \r\n\r\n```\r\n% ollama run llama3 --keepalive 1h hello\r\nHello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n\r\n% ollama ps\r\nNAME         \tID          \tSIZE  \tPROCESSOR\tUNTIL\r\nllama3:latest\t365c0bd3c000\t6.7 GB\t100% GPU \t59 minutes from now\r\n% curl http://localhost:11434/api/generate -d '{\r\n  \"model\": \"llama3\",\r\n  \"prompt\": \"hi\",\r\n  \"stream\": false\r\n}' > /dev/null\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   594  100   534  100    60    797     89 --:--:-- --:--:-- --:--:--   886\r\n% ollama ps\r\nNAME         \tID          \tSIZE  \tPROCESSOR\tUNTIL\r\nllama3:latest\t365c0bd3c000\t6.7 GB\t100% GPU \t59 minutes from now\r\n```\r\n\r\nCompare against https://github.com/ollama/ollama/issues/5272#issuecomment-2204491896 showing the incorrect behavior before.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use model template by default",
            "url": "https://github.com/ollama/ollama/pull/5448",
            "state": "MERGED",
            "createdAt": "2024-07-02T23:30:59Z",
            "mergedAt": "2024-07-02T23:48:06Z",
            "closedAt": "2024-07-02T23:48:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 5,
            "body": "this fixes a bug introduced in https://github.com/ollama/ollama/pull/5051 which causes generate requests to not use the model template\r\n\r\ninstead of many more dramatic changes, this change mirrors with the previous behaviour using the existing control flow",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Better nvidia GPU discovery logging",
            "url": "https://github.com/ollama/ollama/pull/5465",
            "state": "MERGED",
            "createdAt": "2024-07-03T17:38:42Z",
            "mergedAt": "2024-07-03T20:12:22Z",
            "closedAt": "2024-07-03T20:12:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 51,
            "deletions": 23,
            "body": "Refine the way we log GPU discovery to improve the non-debug output, and report more actionable log messages when possible to help users troubleshoot on their own.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix clip model loading with unicode paths",
            "url": "https://github.com/ollama/ollama/pull/5466",
            "state": "MERGED",
            "createdAt": "2024-07-03T19:38:27Z",
            "mergedAt": "2024-07-05T15:16:58Z",
            "closedAt": "2024-07-05T15:16:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 42,
            "deletions": 0,
            "body": "On windows, if the model dir contained unicode characters clip models would fail to load.  This fixes the file name handling in clip.cpp to support utf16 on windows.\r\n\r\nFixes #5329 #4365 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix corner cases on tmp cleaner on mac",
            "url": "https://github.com/ollama/ollama/pull/5467",
            "state": "MERGED",
            "createdAt": "2024-07-03T20:10:59Z",
            "mergedAt": "2024-07-03T20:39:36Z",
            "closedAt": "2024-07-03T20:39:36Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 37,
            "deletions": 22,
            "body": "When ollama is running a long time, tmp cleaners can remove the runners.  This tightens up a few corner cases on arm macs where we failed with \"server cpu not listed in available servers map[]\"",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Prevent loading models larger than total memory",
            "url": "https://github.com/ollama/ollama/pull/5469",
            "state": "MERGED",
            "createdAt": "2024-07-03T22:16:39Z",
            "mergedAt": "2024-07-05T15:22:20Z",
            "closedAt": "2024-07-05T15:22:20Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 38,
            "deletions": 0,
            "body": "Users may not realize the shiny new model they're trying to load fits on their disk, but can't load into system+GPU memory.  Today we crash, but with this fix, we'll give them a better error message before even trying to load it.\r\n\r\nFixes #3837 #4955 \r\n\r\n\r\nVerified by using `stress-ng` to saturate system memory, and loaded a secondary model on another ollama instance to use up GPU memory, then tried to load a model\r\n```\r\n% ollama run gemma:7b\r\nError: requested model (5.5 GiB) is too large for this system (4.5 GiB)\r\n```\r\n\r\nDebug server logs from the manual test\r\n```\r\ntime=2024-07-03T15:12:11.901-07:00 level=DEBUG source=gpu.go:336 msg=\"updating system memory data\" before.total=\"31.3 GiB\" before.free=\"4.0 GiB\" now.total=\"31.3 GiB\" now.free=\"3.9 GiB\"\r\nCUDA driver version: 11.4\r\ntime=2024-07-03T15:12:12.000-07:00 level=DEBUG source=gpu.go:377 msg=\"updating cuda memory data\" gpu=GPU-1c750365-54dc-7082-7c6b-9dd953a68ab6 name=\"NVIDIA GeForce GTX 1060 6GB\" before.total=\"5.9 GiB\" before.free=\"548.9 MiB\" now.total=\"5.9 GiB\" now.free=\"548.9 MiB\" now.used=\"5.4 GiB\"\r\nreleasing cuda driver library\r\ntime=2024-07-03T15:12:12.000-07:00 level=DEBUG source=sched.go:186 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\r\ntime=2024-07-03T15:12:12.033-07:00 level=DEBUG source=memory.go:101 msg=evaluating library=cuda gpu_count=1 available=\"[548.9 MiB]\"\r\ntime=2024-07-03T15:12:12.034-07:00 level=DEBUG source=memory.go:168 msg=\"gpu has too little memory to allocate any layers\" gpu=\"{memInfo:{TotalMemory:6372196352 FreeMemory:575537152} Library:cuda Variant:no vector extensions MinimumMemory:479199232 DependencyPath: EnvWorkarounds:[] UnreliableFreeMemory:false ID:GPU-1c750365-54dc-7082-7c6b-9dd953a68ab6 Name:NVIDIA GeForce GTX 1060 6GB Compute:6.1 DriverMajor:11 DriverMinor:4}\"\r\ntime=2024-07-03T15:12:12.034-07:00 level=DEBUG source=memory.go:296 msg=\"insufficient VRAM to load any model layers\"\r\ntime=2024-07-03T15:12:12.034-07:00 level=WARN source=sched.go:216 msg=\"model request too large for system\" requested=\"5.5 GiB\" system=\"4.5 GiB\"\r\n[GIN] 2024/07/03 - 15:12:12 | 500 |  200.768652ms |      10.16.0.83 | POST     \"/api/chat\"\r\n```\r\n\r\nThe system under test:\r\n```\r\n% free -h; nvidia-smi\r\n               total        used        free      shared  buff/cache   available\r\nMem:            31Gi        26Gi       394Mi       197Mi       4.2Gi       3.9Gi\r\nSwap:             0B          0B          0B\r\nWed Jul  3 15:15:17 2024\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| 33%   33C    P8    13W / 120W |   5467MiB /  6077MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A   1582218      C   ...a_v11/ollama_llama_server     5465MiB |\r\n+-----------------------------------------------------------------------------+\r\n```",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 9
            }
        }
    },
    {
        "node": {
            "title": "Update OpenAI Compatibility Docs with /v1/embeddings",
            "url": "https://github.com/ollama/ollama/pull/5470",
            "state": "MERGED",
            "createdAt": "2024-07-03T22:17:48Z",
            "mergedAt": "2024-08-01T23:00:29Z",
            "closedAt": "2024-08-01T23:00:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 31,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix error detection by limiting model loading error parsing",
            "url": "https://github.com/ollama/ollama/pull/5472",
            "state": "MERGED",
            "createdAt": "2024-07-03T22:42:59Z",
            "mergedAt": "2024-07-04T00:04:30Z",
            "closedAt": "2024-07-04T00:04:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: environ lookup",
            "url": "https://github.com/ollama/ollama/pull/5473",
            "state": "MERGED",
            "createdAt": "2024-07-03T23:08:08Z",
            "mergedAt": "2024-07-31T17:18:05Z",
            "closedAt": "2024-07-31T17:18:05Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 27
            },
            "additions": 514,
            "deletions": 482,
            "body": "current envconfig is configured on import by `init()`. this is problematic and error prone in tests which sometimes override configurations. which means the test must call `envconfig.LoadConfig()` otherwise it'll have side effects\r\n\r\ne.g.\r\n```go\r\nfunc TestRemoveModels(t *testing.T) {\r\n    t.Setenv(\"OLLAMA_MODELS\", t.TempDir())\r\n    // envconfig.LoadConfig()\r\n\r\n    RemoveModels()\r\n}\r\n```\r\n\r\nthis pattern is repeated in many tests but anyone writing a new test can easily miss it\r\n\r\nthis change proposes a new envconfig where configurations are functions rather than variables. this allows dynamic lookup of configurations while keeping the same-ish interface",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp submodule to `d7fd29f`",
            "url": "https://github.com/ollama/ollama/pull/5475",
            "state": "MERGED",
            "createdAt": "2024-07-04T00:03:46Z",
            "mergedAt": "2024-07-05T17:25:58Z",
            "closedAt": "2024-07-05T17:25:58Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 15
            },
            "additions": 150,
            "deletions": 422,
            "body": "Fixes #5157 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix assert on small embedding inputs",
            "url": "https://github.com/ollama/ollama/pull/5491",
            "state": "MERGED",
            "createdAt": "2024-07-05T01:19:10Z",
            "mergedAt": "2024-07-05T15:20:57Z",
            "closedAt": "2024-07-05T15:20:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 60,
            "deletions": 0,
            "body": "Tensors allocated for pooling layers were too small on 2-3 character inputs causing assertions to be fired.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use slot with cached prompt instead of least recently used",
            "url": "https://github.com/ollama/ollama/pull/5492",
            "state": "MERGED",
            "createdAt": "2024-07-05T02:34:53Z",
            "mergedAt": "2024-07-05T16:32:47Z",
            "closedAt": "2024-07-05T16:32:47Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 39,
            "deletions": 1,
            "body": "This chooses the slot with the longest common prompt prefix instead of selecting the least recently used slot \u2013 this maximizes cache time for a single \"conversation\".\r\n\r\nFuture improvements:\r\n- [ ] Clone slots and their cache \r\n- [ ] Avoid requests \"stealing\" slots from each other because they have a small but common prefix\r\n- [ ] Account for context shifts in the cache matching",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "types/model: remove knowledge of digest",
            "url": "https://github.com/ollama/ollama/pull/5500",
            "state": "MERGED",
            "createdAt": "2024-07-05T19:08:01Z",
            "mergedAt": "2024-07-05T20:42:30Z",
            "closedAt": "2024-07-05T20:42:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 14,
            "deletions": 42,
            "body": "This was leading to ambiguity and confusion in ollama.com, and is not used anywhere in ollama at the moment. Once manifests are addressable by digest, we can add this back in, and in a way that is more tailored to the concept of addressing a manifest by digest.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix typo in cgo directives in `llm.go`",
            "url": "https://github.com/ollama/ollama/pull/5501",
            "state": "MERGED",
            "createdAt": "2024-07-05T19:18:20Z",
            "mergedAt": "2024-07-05T19:18:37Z",
            "closedAt": "2024-07-05T19:18:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Always go build in CI generate steps",
            "url": "https://github.com/ollama/ollama/pull/5502",
            "state": "MERGED",
            "createdAt": "2024-07-05T19:27:31Z",
            "mergedAt": "2024-07-05T22:39:11Z",
            "closedAt": "2024-07-05T22:39:11Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "With the recent cgo changes, bugs can sneak through if we don't make sure to `go build` all the permutations",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Workaround broken ROCm p2p copy",
            "url": "https://github.com/ollama/ollama/pull/5503",
            "state": "MERGED",
            "createdAt": "2024-07-05T19:51:14Z",
            "mergedAt": "2024-07-09T22:44:16Z",
            "closedAt": "2024-07-09T22:44:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 1,
            "body": "Enable the build flag for llama.cpp to use CPU copy for multi-GPU scenarios.\r\n\r\n~~Marking draft until I can validate this doesn't cause a significant performance hit for single GPU ROCm scenarios.~~\r\n\r\nUpdate: perf results look good - no regressions detected in my testing.\r\n\r\nFixes #5087 #5450",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Fix cmake build to install dependent dylibs",
            "url": "https://github.com/ollama/ollama/pull/5505",
            "state": "MERGED",
            "createdAt": "2024-07-05T22:25:35Z",
            "mergedAt": "2024-07-05T23:07:01Z",
            "closedAt": "2024-07-05T23:07:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 17,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refine scheduler unit tests for reliability",
            "url": "https://github.com/ollama/ollama/pull/5506",
            "state": "MERGED",
            "createdAt": "2024-07-05T22:31:26Z",
            "mergedAt": "2024-07-20T22:48:40Z",
            "closedAt": "2024-07-20T22:48:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 195,
            "deletions": 130,
            "body": "This breaks up some of the test scenarios to create a more reliable set of tests, as well as adding a little more coverage.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: put back old include dir",
            "url": "https://github.com/ollama/ollama/pull/5507",
            "state": "MERGED",
            "createdAt": "2024-07-05T22:43:28Z",
            "mergedAt": "2024-07-05T23:34:21Z",
            "closedAt": "2024-07-05T23:34:21Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: display transfer model data progress",
            "url": "https://github.com/ollama/ollama/pull/5510",
            "state": "CLOSED",
            "createdAt": "2024-07-05T23:58:52Z",
            "mergedAt": null,
            "closedAt": "2024-07-31T17:16:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 9
            },
            "additions": 367,
            "deletions": 36,
            "body": "displays `transferring model data 24% \u2807\" during transfer data\r\n\r\nrebased on top of https://github.com/ollama/ollama/pull/5441\r\n\r\nhttps://github.com/ollama/ollama/issues/5423",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llm: fix missing dylibs by restoring old build behavior on Linux and macOS",
            "url": "https://github.com/ollama/ollama/pull/5511",
            "state": "MERGED",
            "createdAt": "2024-07-06T01:00:34Z",
            "mergedAt": "2024-07-06T01:48:32Z",
            "closedAt": "2024-07-06T01:48:32Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 21,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "autodetect stop parameters from template",
            "url": "https://github.com/ollama/ollama/pull/5512",
            "state": "MERGED",
            "createdAt": "2024-07-06T01:06:39Z",
            "mergedAt": "2024-07-26T20:48:23Z",
            "closedAt": "2024-07-26T20:48:23Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 21
            },
            "additions": 156,
            "deletions": 4,
            "body": "named template generally have well defined control tokens that can be used to stop generation. it might be possible to parse these directly from the template but given how few there are, it's simpler to set them manually",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update community integration - Painting Droid",
            "url": "https://github.com/ollama/ollama/pull/5514",
            "state": "MERGED",
            "createdAt": "2024-07-06T06:22:00Z",
            "mergedAt": "2024-09-03T20:15:54Z",
            "closedAt": "2024-09-03T20:15:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I've added integration with Ollama to [Painting Droid](https://github.com/mateuszmigas/painting-droid). Users can now ask Llava about stuff they are drawing. More integration is coming to allow Ollama to invoke drawing functions within the app.\r\n\r\n![CleanShot 2024-07-06 at 08 19 58@2x](https://github.com/ollama/ollama/assets/54471371/4032d01b-3be8-4f8f-ada2-2f86eabae068)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "sched: don't error if paging to disk on Windows and macOS",
            "url": "https://github.com/ollama/ollama/pull/5523",
            "state": "MERGED",
            "createdAt": "2024-07-07T01:04:14Z",
            "mergedAt": "2024-07-07T02:01:53Z",
            "closedAt": "2024-07-07T02:01:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 24,
            "deletions": 13,
            "body": "macOS and Windows don't error when paging to disk, so loosen this check for now to not return an error to users that could still run the model (albeit a little slowly). It also stops us from double counting memory on Apple Silicon Macs.\r\n\r\nIn the future, we should still select an upper limit on memory for macOS and Windows to avoid timeouts, etc. This PR is meant to unblock 0.1.49 and doesn't include that yet.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI v1/completions: allow stop token list",
            "url": "https://github.com/ollama/ollama/pull/5551",
            "state": "MERGED",
            "createdAt": "2024-07-08T21:51:21Z",
            "mergedAt": "2024-07-09T21:01:27Z",
            "closedAt": "2024-07-09T21:01:27Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 20,
            "deletions": 5,
            "body": "Resolves #5545 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs",
            "url": "https://github.com/ollama/ollama/pull/5552",
            "state": "MERGED",
            "createdAt": "2024-07-08T22:17:57Z",
            "mergedAt": "2024-07-25T23:26:19Z",
            "closedAt": "2024-07-25T23:26:19Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 292,
            "deletions": 4,
            "body": "part of #5216\r\npart of #5284\r\npart of #5207",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bundle missing CRT libraries",
            "url": "https://github.com/ollama/ollama/pull/5555",
            "state": "MERGED",
            "createdAt": "2024-07-09T01:26:51Z",
            "mergedAt": "2024-07-10T19:50:02Z",
            "closedAt": "2024-07-10T19:50:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "Some users are experienging runner startup errors due to not having these msvc redist libraries on their host\r\n\r\nFixes #4657 \r\n\r\n```\r\n> dumpbin /dependents .\\llm\\build\\windows\\amd64\\cpu\\bin\\ollama_llama_server.exe\r\nMicrosoft (R) COFF/PE Dumper Version 14.29.30154.0\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\r\nDump of file .\\llm\\build\\windows\\amd64\\cpu\\bin\\ollama_llama_server.exe\r\n\r\nFile Type: EXECUTABLE IMAGE\r\n\r\n  Image has the following dependencies:\r\n\r\n    WS2_32.dll\r\n    llama.dll\r\n    ggml.dll\r\n    KERNEL32.dll\r\n    MSVCP140.dll\r\n    VCRUNTIME140.dll\r\n    VCRUNTIME140_1.dll\r\n    api-ms-win-crt-runtime-l1-1-0.dll\r\n    api-ms-win-crt-stdio-l1-1-0.dll\r\n    api-ms-win-crt-filesystem-l1-1-0.dll\r\n    api-ms-win-crt-math-l1-1-0.dll\r\n    api-ms-win-crt-convert-l1-1-0.dll\r\n    api-ms-win-crt-heap-l1-1-0.dll\r\n    api-ms-win-crt-string-l1-1-0.dll\r\n    api-ms-win-crt-time-l1-1-0.dll\r\n    api-ms-win-crt-locale-l1-1-0.dll\r\n```\r\n\r\n```\r\n> dir .\\dist\\windows-amd64\\ollama_runners\\\r\n\r\n\r\n    Directory: C:\\Users\\danie\\code\\ollama\\dist\\windows-amd64\\ollama_runners\r\n\r\n\r\nMode                 LastWriteTime         Length Name\r\n----                 -------------         ------ ----\r\nd-----          7/8/2024   4:14 PM                cpu\r\nd-----          7/8/2024   4:16 PM                cpu_avx\r\nd-----          7/8/2024   4:19 PM                cpu_avx2\r\nd-----          7/8/2024   4:40 PM                cuda_v11.3\r\nd-----          7/8/2024   5:46 PM                rocm_v5.7\r\n-a----          2/6/2024   8:14 PM          33752 api-ms-win-crt-convert-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          30168 api-ms-win-crt-environment-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          31672 api-ms-win-crt-filesystem-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          30680 api-ms-win-crt-heap-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          30168 api-ms-win-crt-locale-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          38632 api-ms-win-crt-math-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          34264 api-ms-win-crt-runtime-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          35776 api-ms-win-crt-stdio-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          35800 api-ms-win-crt-string-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM          32192 api-ms-win-crt-time-l1-1-0.dll\r\n-a----          2/6/2024   8:14 PM         567328 msvcp140.dll\r\n-a----          2/6/2024   8:14 PM          25016 msvcp140_1.dll\r\n-a----          2/6/2024   8:14 PM         186928 msvcp140_2.dll\r\n-a----          2/6/2024   8:14 PM          57392 msvcp140_atomic_wait.dll\r\n-a----          2/6/2024   8:14 PM          21536 msvcp140_codecvt_ids.dll\r\n-a----          2/6/2024   8:14 PM          98336 vcruntime140.dll\r\n-a----          2/6/2024   8:14 PM          38448 vcruntime140_1.dll\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: fix model reloads when setting `OLLAMA_NUM_PARALLEL`",
            "url": "https://github.com/ollama/ollama/pull/5560",
            "state": "MERGED",
            "createdAt": "2024-07-09T04:36:51Z",
            "mergedAt": "2024-07-09T05:32:15Z",
            "closedAt": "2024-07-09T05:32:15Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI Tests: Separate Request and Response Compare Tests",
            "url": "https://github.com/ollama/ollama/pull/5578",
            "state": "MERGED",
            "createdAt": "2024-07-09T18:12:41Z",
            "mergedAt": "2024-07-09T20:48:31Z",
            "closedAt": "2024-07-09T20:48:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 75,
            "deletions": 113,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Statically link c++ and thread lib on windows",
            "url": "https://github.com/ollama/ollama/pull/5579",
            "state": "MERGED",
            "createdAt": "2024-07-09T18:20:50Z",
            "mergedAt": "2024-07-09T19:21:13Z",
            "closedAt": "2024-07-09T19:21:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 7,
            "body": "This makes sure we statically link the c++ and thread library on windows to avoid unnecessary runtime dependencies on non-standard DLLs\r\n\r\n\r\nOn my dev box, I have these `libpthread.dll.a` libraries and my local builds were leaking the dependency\r\n\r\ne.g.:\r\n```\r\n> gci -path C:\\msys64\\ -r -fi 'libpthread.dll.a'\r\n\r\n    Directory: C:\\msys64\\ucrt64\\lib\r\n\r\nMode                 LastWriteTime         Length Name\r\n----                 -------------         ------ ----\r\n-a---           4/27/2024 12:18 PM          94672 libpthread.dll.a\r\n```\r\n\r\nBefore this change (my local builds):\r\n```\r\n> dumpbin /dependents ./ollama.exe\r\nMicrosoft (R) COFF/PE Dumper Version 14.40.33811.0\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\r\nDump of file .\\ollama.exe\r\n\r\nFile Type: EXECUTABLE IMAGE\r\n\r\n  Image has the following dependencies:\r\n\r\n    libstdc++-6.dll\r\n    KERNEL32.dll\r\n    msvcrt.dll\r\n    libwinpthread-1.dll\r\n```\r\n\r\nAfter this change:\r\n```\r\n> dumpbin /dependents .\\ollama.exe\r\nMicrosoft (R) COFF/PE Dumper Version 14.29.30154.0\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\r\nDump of file .\\ollama.exe\r\n\r\nFile Type: EXECUTABLE IMAGE\r\n\r\n  Image has the following dependencies:\r\n\r\n    KERNEL32.dll\r\n    msvcrt.dll\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Detect CUDA OS overhead",
            "url": "https://github.com/ollama/ollama/pull/5580",
            "state": "MERGED",
            "createdAt": "2024-07-09T18:44:26Z",
            "mergedAt": "2024-07-10T19:47:31Z",
            "closedAt": "2024-07-10T19:47:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 29,
            "deletions": 1,
            "body": "This adds logic to detect skew between the driver and\r\nmanagement library which can be attributed to OS overhead\r\nand records that so we can adjust subsequent management\r\nlibrary free VRAM updates and avoid OOM scenarios.\r\n\r\n\r\nFixes #5504 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix context exhaustion integration test for small gpus",
            "url": "https://github.com/ollama/ollama/pull/5583",
            "state": "MERGED",
            "createdAt": "2024-07-09T22:30:40Z",
            "mergedAt": "2024-07-20T22:48:21Z",
            "closedAt": "2024-07-20T22:48:21Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 2,
            "body": "On the smaller GPUs, the initial model load of llama2 took over 30s (the default timeout for the DoGenerate helper)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove `GGML_CUDA_FORCE_MMQ=on` from build",
            "url": "https://github.com/ollama/ollama/pull/5588",
            "state": "MERGED",
            "createdAt": "2024-07-10T03:08:09Z",
            "mergedAt": "2024-07-10T20:17:13Z",
            "closedAt": "2024-07-10T20:17:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Removing this build flag seems to cause mmq to be decided at runtime vs always forcing it on. This stops some cuda calls from failing on V100 and other CC 7.0 cards. This may also help with [RDNA3 cards](https://github.com/ggerganov/llama.cpp/blob/a59f8fdc85e1119d470d8766e29617962549d993/docs/build.md?plain=1#L175)\r\n\r\nFixes https://github.com/ollama/ollama/issues/5571\r\n\r\nChanges to mmq functionality related to this bug: https://github.com/ggerganov/llama.cpp/compare/7c26775...a8db2a9c#diff-b2fe862fcd5119199ae59ea13d1b6a46e0d23e41e727e39d90913f828a5ff66bR78",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Remove duplicate merge glitch",
            "url": "https://github.com/ollama/ollama/pull/5605",
            "state": "MERGED",
            "createdAt": "2024-07-10T16:02:06Z",
            "mergedAt": "2024-07-10T18:47:08Z",
            "closedAt": "2024-07-10T18:47:08Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 4,
            "body": "Fixes #5594 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump ROCm on windows to 6.1.2",
            "url": "https://github.com/ollama/ollama/pull/5607",
            "state": "MERGED",
            "createdAt": "2024-07-10T18:02:49Z",
            "mergedAt": "2024-07-10T19:47:10Z",
            "closedAt": "2024-07-10T19:47:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 17,
            "deletions": 28,
            "body": "This also adjusts our algorithm to favor our bundled ROCm. I've confirmed VRAM reporting still doesn't work properly so we can't yet enable concurrency by default.\r\n\r\nFixes #5599 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Add Suffix to `v1/completions`",
            "url": "https://github.com/ollama/ollama/pull/5611",
            "state": "MERGED",
            "createdAt": "2024-07-10T20:12:48Z",
            "mergedAt": "2024-07-17T03:50:14Z",
            "closedAt": "2024-07-17T03:50:14Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 7,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "chatglm graph",
            "url": "https://github.com/ollama/ollama/pull/5612",
            "state": "MERGED",
            "createdAt": "2024-07-10T20:44:11Z",
            "mergedAt": "2024-07-10T21:18:33Z",
            "closedAt": "2024-07-10T21:18:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 26,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Support Tools",
            "url": "https://github.com/ollama/ollama/pull/5614",
            "state": "MERGED",
            "createdAt": "2024-07-10T21:21:37Z",
            "mergedAt": "2024-07-17T03:52:59Z",
            "closedAt": "2024-07-17T03:52:59Z",
            "reviews": {
                "totalCount": 14
            },
            "files": {
                "totalCount": 1
            },
            "additions": 53,
            "deletions": 4,
            "body": "```\r\ncurl -s 127.0.0.1:11434/v1/chat/completions -d '{\r\n  \"model\": \"mike/mistral\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"What'\\''s the weather like today in Paris?\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"tool_calls\": [\r\n        {\r\n          \"id\": \"89a1e453-0bce-4de3-a456-c54bed09c520\",\r\n          \"type\": \"function\",\r\n          \"function\": {\r\n            \"name\": \"get_current_weather\",\r\n            \"arguments\": \"{\\\"location\\\": \\\"Paris, France\\\", \\\"format\\\": \\\"celsius\\\"}\"\r\n          }\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"role\": \"tool\",\r\n      \"tool_call_id\": \"89a1e453-0bce-4de3-a456-c54bed09c520\",\r\n      \"content\": \"22\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"The weather in Paris is 22 degrees celsius.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"What'\\''s the weather like today in San Francisco and Toronto?\"\r\n    }\r\n  ],\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"get_current_weather\",\r\n        \"description\": \"Get the current weather\",\r\n        \"parameters\": {\r\n          \"type\": \"object\",\r\n          \"properties\": {\r\n            \"location\": {\r\n              \"type\": \"string\",\r\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\r\n            },\r\n            \"format\": {\r\n              \"type\": \"string\",\r\n              \"enum\": [\r\n                \"celsius\",\r\n                \"fahrenheit\"\r\n              ],\r\n              \"description\": \"The temperature unit to use. Infer this from the users location.\"\r\n            }\r\n          },\r\n          \"required\": [\r\n            \"location\",\r\n            \"format\"\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"stream\": false,\r\n  \"temperature\": 0\r\n}' | jq\r\n```\r\n\r\n```\r\n{\r\n  \"id\": \"chatcmpl-920\",\r\n  \"object\": \"chat.completion\",\r\n  \"created\": 1721187203,\r\n  \"model\": \"mike/mistral\",\r\n  \"system_fingerprint\": \"fp_ollama\",\r\n  \"choices\": [\r\n    {\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"content\": \"\",\r\n        \"tool_calls\": [\r\n          {\r\n            \"id\": \"call_q9jtgbw2\",\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n              \"name\": \"get_current_weather\",\r\n              \"arguments\": \"{\\\"format\\\":\\\"celsius\\\",\\\"location\\\":\\\"San Francisco, CA\\\"}\"\r\n            }\r\n          },\r\n          {\r\n            \"id\": \"call_9chg85cd\",\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n              \"name\": \"get_current_weather\",\r\n              \"arguments\": \"{\\\"format\\\":\\\"celsius\\\",\\\"location\\\":\\\"Toronto, Canada\\\"}\"\r\n            }\r\n          }\r\n        ]\r\n      },\r\n      \"finish_reason\": \"stop\"\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 198,\r\n    \"completion_tokens\": 65,\r\n    \"total_tokens\": 263\r\n  }\r\n}\r\n```",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "cmd/llama.cpp: quantize progress",
            "url": "https://github.com/ollama/ollama/pull/5615",
            "state": "CLOSED",
            "createdAt": "2024-07-10T21:25:17Z",
            "mergedAt": null,
            "closedAt": "2024-08-12T18:47:39Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 7
            },
            "additions": 1346,
            "deletions": 4,
            "body": "`quantizing model tensors 68/195 \u2838 `\r\nnow displays quantization progress\r\n\r\nnow tracking here: https://github.com/ollama/ollama/pull/6102\r\n\r\nhttps://github.com/ollama/ollama/issues/5423",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: quant err message",
            "url": "https://github.com/ollama/ollama/pull/5616",
            "state": "MERGED",
            "createdAt": "2024-07-10T22:30:02Z",
            "mergedAt": "2024-07-12T00:24:29Z",
            "closedAt": "2024-07-12T00:24:29Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "`Error: quantization of this model is not supported by your version of Ollama. You may need to upgrade`\r\n\r\nResolves: https://github.com/ollama/ollama/issues/5531",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Update Docs to Include Tools",
            "url": "https://github.com/ollama/ollama/pull/5617",
            "state": "MERGED",
            "createdAt": "2024-07-10T22:39:15Z",
            "mergedAt": "2024-07-25T22:34:06Z",
            "closedAt": "2024-07-25T22:34:06Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update embedded templates",
            "url": "https://github.com/ollama/ollama/pull/5620",
            "state": "MERGED",
            "createdAt": "2024-07-10T23:45:10Z",
            "mergedAt": "2024-07-11T00:16:24Z",
            "closedAt": "2024-07-11T00:16:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 29
            },
            "additions": 214,
            "deletions": 88,
            "body": "add test to ensure legacy and messages template produce the same output. there are some templates which cannot produce the same outputs so those will be skipped",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: remove `/usr/local/cuda/compat` from linker path",
            "url": "https://github.com/ollama/ollama/pull/5621",
            "state": "MERGED",
            "createdAt": "2024-07-11T00:52:31Z",
            "mergedAt": "2024-07-11T03:01:52Z",
            "closedAt": "2024-07-11T03:01:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixes https://github.com/ollama/ollama/issues/5573",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "sched: error on over-allocation of system memory when on Linux",
            "url": "https://github.com/ollama/ollama/pull/5626",
            "state": "MERGED",
            "createdAt": "2024-07-11T04:40:08Z",
            "mergedAt": "2024-07-11T07:53:12Z",
            "closedAt": "2024-07-11T07:53:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 37,
            "body": "Model switching no longer works on CPU-only machines and the scheduler instead errors with `requested model is too large for this system` error: \r\n\r\n```\r\n$ ollama run gemma2\r\nError: requested model (8.4 GiB) is too large for this system (1.9 GiB)\r\n```\r\n\r\nThis PR changes this behavior to only stop a new model from loading if a crash will take place from over-allocating system memory on Linux. It also moves the check until after scheduling has taken place to avoid an error before knowing if another model would be unloaded.\r\n\r\nExample on a 48GB VRAM system with 64GB of system memory\r\n\r\n```\r\n$ ollama run llama3:70b-instruct-fp16\r\nError: requested model requires more system memory (86.8 GiB) than is available (62.5 GiB)\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Refactor linux packaging",
            "url": "https://github.com/ollama/ollama/pull/5631",
            "state": "CLOSED",
            "createdAt": "2024-07-11T14:56:45Z",
            "mergedAt": null,
            "closedAt": "2024-08-17T17:16:45Z",
            "reviews": {
                "totalCount": 18
            },
            "files": {
                "totalCount": 19
            },
            "additions": 249,
            "deletions": 173,
            "body": "This adjusts linux to follow a similar model to windows with a discrete archive (zip/tgz) to cary the primary executable, and dependent libraries. Runners are still carried as payloads inside the main binary.\r\n\r\nAs Darwin has no significant dependent libraries, it still functions as a discrete stand-alone executable carrying the runners as payloads.\r\n\r\nReplaces #5582 \r\n\r\nFixes #5737\r\nFixes #2361 \r\nFixes #6144\r\n\r\n```\r\n% ls -lh dist/ollama-linux-amd64.tgz\r\n-rw-r--r--  1 daniel  staff   1.6G Jul 10 18:14 dist/ollama-linux-amd64.tgz\r\n```\r\n\r\n```\r\n% ls -F\r\ncuda/  ollama*  rocm/\r\n% du -sh .\r\n7.8G\t.\r\n% du -sh *\r\n369M\tcuda\r\n245M\tollama\r\n7.2G\trocm\r\n```\r\n\r\n```\r\n% find /tmp/ollama3466897970/ -type f | xargs ls -lh\r\n-rwxrwxr-x 1 daniel daniel    7 Jul 11 08:00 /tmp/ollama3466897970/ollama.pid\r\n-rwxr-xr-x 1 daniel daniel 808K Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx2/libggml.so\r\n-rwxr-xr-x 1 daniel daniel 1.9M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx2/libllama.so\r\n-rwxr-xr-x 1 daniel daniel 1.8M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx2/ollama_llama_server\r\n-rwxr-xr-x 1 daniel daniel 790K Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx/libggml.so\r\n-rwxr-xr-x 1 daniel daniel 1.9M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx/libllama.so\r\n-rwxr-xr-x 1 daniel daniel 1.8M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu_avx/ollama_llama_server\r\n-rwxr-xr-x 1 daniel daniel 714K Jul 11 08:00 /tmp/ollama3466897970/runners/cpu/libggml.so\r\n-rwxr-xr-x 1 daniel daniel 1.9M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu/libllama.so\r\n-rwxr-xr-x 1 daniel daniel 1.8M Jul 11 08:00 /tmp/ollama3466897970/runners/cpu/ollama_llama_server\r\n-rwxr-xr-x 1 daniel daniel 316M Jul 11 08:00 /tmp/ollama3466897970/runners/cuda_v11/libggml.so\r\n-rwxr-xr-x 1 daniel daniel 1.9M Jul 11 08:00 /tmp/ollama3466897970/runners/cuda_v11/libllama.so\r\n-rwxr-xr-x 1 daniel daniel 1.8M Jul 11 08:00 /tmp/ollama3466897970/runners/cuda_v11/ollama_llama_server\r\n-rwxr-xr-x 1 daniel daniel 298M Jul 11 08:00 /tmp/ollama3466897970/runners/rocm_v60101/libggml.so\r\n-rwxr-xr-x 1 daniel daniel 1.9M Jul 11 08:00 /tmp/ollama3466897970/runners/rocm_v60101/libllama.so\r\n-rwxr-xr-x 1 daniel daniel 1.7M Jul 11 08:00 /tmp/ollama3466897970/runners/rocm_v60101/ollama_llama_server\r\n```\r\n\r\n<details>\r\n<summary>ldd output</summary>\r\n\r\n```\r\n% find /tmp/ollama3466897970/runners -type f | LD_LIBRARY_PATH=/home/daniel/ollama/cuda:/home/daniel/ollama/rocm:/tmp/ollama3466897970/runners/cuda_v11:/tmp/ollama3466897970/runners xargs ldd\r\n/tmp/ollama3466897970/runners/cuda_v11/libggml.so:\r\n\tlinux-vdso.so.1 (0x00007ffe776c2000)\r\n\tlibcudart.so.11.0 (0x00007f5169400000)\r\n\tlibcublas.so.11 (0x00007f5161c00000)\r\n\tlibcublasLt.so.11 (0x00007f5151000000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f514f800000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5169781000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f516977c000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5169777000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f5169772000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f514f5d4000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5169752000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f514f3ac000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f517d482000)\r\n/tmp/ollama3466897970/runners/cuda_v11/ollama_llama_server:\r\n\tlinux-vdso.so.1 (0x00007ffd38567000)\r\n\tlibllama.so => /tmp/ollama3466897970/runners/cuda_v11/libllama.so (0x00007f6668611000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f6654a14000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f66547cf000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f66546e8000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f66546c6000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f66546c1000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6654499000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f66687b3000)\r\n\tlibcudart.so.11.0 (0x00007f6654000000)\r\n\tlibcublas.so.11 (0x00007f664c800000)\r\n\tlibcublasLt.so.11 (0x00007f663bc00000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f663a400000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6654492000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f665448d000)\r\n/tmp/ollama3466897970/runners/cuda_v11/libllama.so:\r\n\tlinux-vdso.so.1 (0x00007fffed5fd000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f69584ac000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6958267000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6958180000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6958160000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6957f38000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f696c24b000)\r\n\tlibcudart.so.11.0 (0x00007f6957c00000)\r\n\tlibcublas.so.11 (0x00007f6950400000)\r\n\tlibcublasLt.so.11 (0x00007f693f800000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f693e000000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6957f31000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6957f2a000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6957f25000)\r\n/tmp/ollama3466897970/runners/cpu_avx/libggml.so:\r\n\tlinux-vdso.so.1 (0x00007fffb28b7000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f183d866000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f183d63a000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f183d61a000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f183d615000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f183d3ed000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f183daaa000)\r\n/tmp/ollama3466897970/runners/cpu_avx/ollama_llama_server:\r\n\tlinux-vdso.so.1 (0x00007ffdac5a0000)\r\n\tlibllama.so => /tmp/ollama3466897970/runners/cuda_v11/libllama.so (0x00007ff44d0a6000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007ff4394a9000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff439264000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff43917d000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff43915b000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff439156000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff438f2e000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007ff44d248000)\r\n\tlibcudart.so.11.0 (0x00007ff438c00000)\r\n\tlibcublas.so.11 (0x00007ff431400000)\r\n\tlibcublasLt.so.11 (0x00007ff420800000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007ff41f000000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff438f27000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff438f22000)\r\n/tmp/ollama3466897970/runners/cpu_avx/libllama.so:\r\n\tlinux-vdso.so.1 (0x00007ffd66ff5000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007fa2366f3000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fa2364ae000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fa2363c7000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fa2363a7000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa23617f000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fa24a490000)\r\n\tlibcudart.so.11.0 (0x00007fa235e00000)\r\n\tlibcublas.so.11 (0x00007fa22e600000)\r\n\tlibcublasLt.so.11 (0x00007fa21da00000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007fa21c200000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fa236178000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fa236171000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fa23616c000)\r\n/tmp/ollama3466897970/runners/rocm_v60101/libggml.so:\r\n\tlinux-vdso.so.1 (0x00007fff7d7fe000)\r\n\tlibhipblas.so.2 => /home/daniel/ollama/rocm/libhipblas.so.2 (0x00007ff4e98b2000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff4e97b2000)\r\n\tlibrocblas.so.4 => /home/daniel/ollama/rocm/librocblas.so.4 (0x00007ff4b5418000)\r\n\tlibamdhip64.so.6 => /home/daniel/ollama/rocm/libamdhip64.so.6 (0x00007ff4b397e000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff4b3750000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff4b3730000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff4b372b000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff4b3503000)\r\n\tlibrocsolver.so.0 => /home/daniel/ollama/rocm/librocsolver.so.0 (0x00007ff460fbf000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007ff4fc33f000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff460fba000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff460fb3000)\r\n\tlibamd_comgr.so.2 => /home/daniel/ollama/rocm/libamd_comgr.so.2 (0x00007ff458287000)\r\n\tlibhsa-runtime64.so.1 => /home/daniel/ollama/rocm/libhsa-runtime64.so.1 (0x00007ff457f9f000)\r\n\tlibnuma.so.1 => /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007ff457f92000)\r\n\tlibrocsparse.so.1 => /home/daniel/ollama/rocm/librocsparse.so.1 (0x00007ff406fd4000)\r\n\tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007ff406fb6000)\r\n\tlibtinfo.so.5 => /home/daniel/ollama/rocm/libtinfo.so.5 (0x00007ff406c00000)\r\n\tlibelf.so.1 => /lib/x86_64-linux-gnu/libelf.so.1 (0x00007ff406f98000)\r\n\tlibrocprofiler-register.so.0 => /home/daniel/ollama/rocm/librocprofiler-register.so.0 (0x00007ff406ebb000)\r\n\tlibdrm.so.2 => /home/daniel/ollama/rocm/libdrm.so.2 (0x00007ff406ea4000)\r\n\tlibdrm_amdgpu.so.1 => /home/daniel/ollama/rocm/libdrm_amdgpu.so.1 (0x00007ff406e97000)\r\n/tmp/ollama3466897970/runners/rocm_v60101/ollama_llama_server:\r\n\tlinux-vdso.so.1 (0x00007ffdfab57000)\r\n\tlibllama.so => /tmp/ollama3466897970/runners/cuda_v11/libllama.so (0x00007f69dd1dd000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f69c95e0000)\r\n\tlibhipblas.so.2 => /home/daniel/ollama/rocm/libhipblas.so.2 (0x00007f69c951b000)\r\n\tlibrocblas.so.4 => /home/daniel/ollama/rocm/librocblas.so.4 (0x00007f6995181000)\r\n\tlibamdhip64.so.6 => /home/daniel/ollama/rocm/libamdhip64.so.6 (0x00007f69936e5000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f69934a0000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f69933b9000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6993399000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6993394000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f699316c000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f69dd37f000)\r\n\tlibcudart.so.11.0 (0x00007f6992e00000)\r\n\tlibcublas.so.11 (0x00007f698b600000)\r\n\tlibcublasLt.so.11 (0x00007f697aa00000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f6979200000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6993165000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f699315e000)\r\n\tlibrocsolver.so.0 => /home/daniel/ollama/rocm/librocsolver.so.0 (0x00007f6926cbc000)\r\n\tlibamd_comgr.so.2 => /home/daniel/ollama/rocm/libamd_comgr.so.2 (0x00007f691df90000)\r\n\tlibhsa-runtime64.so.1 => /home/daniel/ollama/rocm/libhsa-runtime64.so.1 (0x00007f691dca8000)\r\n\tlibnuma.so.1 => /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007f6993151000)\r\n\tlibrocsparse.so.1 => /home/daniel/ollama/rocm/librocsparse.so.1 (0x00007f68cccea000)\r\n\tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f6993133000)\r\n\tlibtinfo.so.5 => /home/daniel/ollama/rocm/libtinfo.so.5 (0x00007f68cca00000)\r\n\tlibelf.so.1 => /lib/x86_64-linux-gnu/libelf.so.1 (0x00007f6993115000)\r\n\tlibrocprofiler-register.so.0 => /home/daniel/ollama/rocm/librocprofiler-register.so.0 (0x00007f6992d23000)\r\n\tlibdrm.so.2 => /home/daniel/ollama/rocm/libdrm.so.2 (0x00007f69930fc000)\r\n\tlibdrm_amdgpu.so.1 => /home/daniel/ollama/rocm/libdrm_amdgpu.so.1 (0x00007f69930ef000)\r\n/tmp/ollama3466897970/runners/rocm_v60101/libllama.so:\r\n\tlinux-vdso.so.1 (0x00007ffdd17dc000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007faadeff1000)\r\n\tlibhipblas.so.2 => /home/daniel/ollama/rocm/libhipblas.so.2 (0x00007faadef2c000)\r\n\tlibrocblas.so.4 => /home/daniel/ollama/rocm/librocblas.so.4 (0x00007faaaab92000)\r\n\tlibamdhip64.so.6 => /home/daniel/ollama/rocm/libamdhip64.so.6 (0x00007faaa90f8000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007faaa8eb1000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007faaa8dca000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007faaa8daa000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007faaa8b82000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007faaf2d9e000)\r\n\tlibcudart.so.11.0 (0x00007faaa8800000)\r\n\tlibcublas.so.11 (0x00007faaa1000000)\r\n\tlibcublasLt.so.11 (0x00007faa90400000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007faa8ec00000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007faaa8b7b000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007faaa8b76000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007faaa8b71000)\r\n\tlibrocsolver.so.0 => /home/daniel/ollama/rocm/librocsolver.so.0 (0x00007faa3c6bc000)\r\n\tlibamd_comgr.so.2 => /home/daniel/ollama/rocm/libamd_comgr.so.2 (0x00007faa33990000)\r\n\tlibhsa-runtime64.so.1 => /home/daniel/ollama/rocm/libhsa-runtime64.so.1 (0x00007faa336a8000)\r\n\tlibnuma.so.1 => /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007faaa8b62000)\r\n\tlibrocsparse.so.1 => /home/daniel/ollama/rocm/librocsparse.so.1 (0x00007fa9e26ea000)\r\n\tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007faaa8b44000)\r\n\tlibtinfo.so.5 => /home/daniel/ollama/rocm/libtinfo.so.5 (0x00007fa9e2400000)\r\n\tlibelf.so.1 => /lib/x86_64-linux-gnu/libelf.so.1 (0x00007faaa8b26000)\r\n\tlibrocprofiler-register.so.0 => /home/daniel/ollama/rocm/librocprofiler-register.so.0 (0x00007faaa8723000)\r\n\tlibdrm.so.2 => /home/daniel/ollama/rocm/libdrm.so.2 (0x00007faaa8b0f000)\r\n\tlibdrm_amdgpu.so.1 => /home/daniel/ollama/rocm/libdrm_amdgpu.so.1 (0x00007faaa8b00000)\r\n/tmp/ollama3466897970/runners/cpu_avx2/libggml.so:\r\n\tlinux-vdso.so.1 (0x00007fff4517d000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbbfefe5000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fbbfedb9000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fbbfed99000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fbbfed94000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbbfeb6c000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbbff22d000)\r\n/tmp/ollama3466897970/runners/cpu_avx2/ollama_llama_server:\r\n\tlinux-vdso.so.1 (0x00007ffc10be7000)\r\n\tlibllama.so => /tmp/ollama3466897970/runners/cuda_v11/libllama.so (0x00007fe6369f2000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007fe622df5000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fe622bb0000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fe622ac9000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fe622aa7000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe622aa2000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe62287a000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fe636b94000)\r\n\tlibcudart.so.11.0 (0x00007fe622400000)\r\n\tlibcublas.so.11 (0x00007fe61ac00000)\r\n\tlibcublasLt.so.11 (0x00007fe60a000000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007fe608800000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fe622873000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe62286e000)\r\n/tmp/ollama3466897970/runners/cpu_avx2/libllama.so:\r\n\tlinux-vdso.so.1 (0x00007fffe2ba2000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f84ecdfc000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f84ecbb7000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f84ecad0000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f84ecab0000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f84ec888000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f8500b99000)\r\n\tlibcudart.so.11.0 (0x00007f84ec400000)\r\n\tlibcublas.so.11 (0x00007f84e4c00000)\r\n\tlibcublasLt.so.11 (0x00007f84d4000000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f84d2800000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f84ec881000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f84ec87a000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f84ec875000)\r\n/tmp/ollama3466897970/runners/cpu/libggml.so:\r\n\tlinux-vdso.so.1 (0x00007ffe3b93f000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f24c8e90000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f24c8c64000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f24c8c44000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f24c8c3f000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f24c8a17000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f24c90c2000)\r\n/tmp/ollama3466897970/runners/cpu/ollama_llama_server:\r\n\tlinux-vdso.so.1 (0x00007ffd0e1fc000)\r\n\tlibllama.so => /tmp/ollama3466897970/runners/cuda_v11/libllama.so (0x00007f15bd29b000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f15a969e000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f15a9459000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f15a9372000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f15a9350000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f15a934b000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f15a9123000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f15bd43d000)\r\n\tlibcudart.so.11.0 (0x00007f15a8e00000)\r\n\tlibcublas.so.11 (0x00007f15a1600000)\r\n\tlibcublasLt.so.11 (0x00007f1590a00000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f158f200000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f15a911c000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f15a9117000)\r\n/tmp/ollama3466897970/runners/cpu/libllama.so:\r\n\tlinux-vdso.so.1 (0x00007fff511f7000)\r\n\tlibggml.so => /tmp/ollama3466897970/runners/cuda_v11/libggml.so (0x00007f5810479000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5810234000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f581014d000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f581012d000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f580ff05000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f5824216000)\r\n\tlibcudart.so.11.0 (0x00007f580fc00000)\r\n\tlibcublas.so.11 (0x00007f5808400000)\r\n\tlibcublasLt.so.11 (0x00007f57f7800000)\r\n\tlibcuda.so.1 => /lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f57f6000000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f580fefe000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f580fef7000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f580fef2000)\r\n```\r\n\r\n</details>",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "llm: avoid loading model if system memory is too small",
            "url": "https://github.com/ollama/ollama/pull/5637",
            "state": "MERGED",
            "createdAt": "2024-07-11T20:17:35Z",
            "mergedAt": "2024-07-11T23:42:57Z",
            "closedAt": "2024-07-11T23:42:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 22,
            "deletions": 13,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "do not automatically aggregate system messages",
            "url": "https://github.com/ollama/ollama/pull/5639",
            "state": "MERGED",
            "createdAt": "2024-07-11T21:40:26Z",
            "mergedAt": "2024-07-12T00:48:50Z",
            "closedAt": "2024-07-12T00:48:50Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 24
            },
            "additions": 124,
            "deletions": 258,
            "body": "add a helper for aggregating system prompts\r\n\r\nrevert embedded templates to use prompt/response templates for better compatibility",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Clean up old files when installing on Windows",
            "url": "https://github.com/ollama/ollama/pull/5645",
            "state": "MERGED",
            "createdAt": "2024-07-12T05:32:49Z",
            "mergedAt": "2024-07-12T05:53:46Z",
            "closedAt": "2024-07-12T05:53:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "app: also clean up tempdir runners on install",
            "url": "https://github.com/ollama/ollama/pull/5646",
            "state": "MERGED",
            "createdAt": "2024-07-12T06:04:05Z",
            "mergedAt": "2024-07-12T19:29:23Z",
            "closedAt": "2024-07-12T19:29:24Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "template: preprocess message and collect system",
            "url": "https://github.com/ollama/ollama/pull/5653",
            "state": "MERGED",
            "createdAt": "2024-07-12T19:02:17Z",
            "mergedAt": "2024-07-12T19:32:34Z",
            "closedAt": "2024-07-12T19:32:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 23,
            "deletions": 67,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove template",
            "url": "https://github.com/ollama/ollama/pull/5655",
            "state": "MERGED",
            "createdAt": "2024-07-12T22:37:10Z",
            "mergedAt": "2024-07-14T03:56:24Z",
            "closedAt": "2024-07-14T03:56:24Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 16,
            "deletions": 50,
            "body": "Remove the broken `/set template` command in the CLI. This is an alternative to #5613 that doesn't add another parameter to the `/api/chat` endpoint.\r\n\r\nGiven the `/set template` command was broken for 6 months and only one person noticed (thank you @protosam) I think it's probably safe to remove this.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix system prompt",
            "url": "https://github.com/ollama/ollama/pull/5662",
            "state": "MERGED",
            "createdAt": "2024-07-13T03:22:40Z",
            "mergedAt": "2024-07-13T04:04:44Z",
            "closedAt": "2024-07-13T04:04:44Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 51,
            "deletions": 30,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix sprintf to snprintf",
            "url": "https://github.com/ollama/ollama/pull/5664",
            "state": "MERGED",
            "createdAt": "2024-07-13T04:54:08Z",
            "mergedAt": "2024-09-03T16:32:59Z",
            "closedAt": "2024-09-03T16:33:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "/Users/au/src/ollama/llm/ext_server/server.cpp:289:9: warning: 'sprintf' is deprecated: This function is provided for compatibility reasons only. Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Kerlig AI, an app for macOS",
            "url": "https://github.com/ollama/ollama/pull/5675",
            "state": "MERGED",
            "createdAt": "2024-07-13T15:16:17Z",
            "mergedAt": "2024-07-13T15:33:46Z",
            "closedAt": "2024-07-13T15:33:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: fix `context`, `load_duration` and `total_duration` fields",
            "url": "https://github.com/ollama/ollama/pull/5676",
            "state": "MERGED",
            "createdAt": "2024-07-13T16:14:57Z",
            "mergedAt": "2024-07-13T16:25:31Z",
            "closedAt": "2024-07-13T16:25:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 46,
            "deletions": 10,
            "body": "Fixes https://github.com/ollama/ollama/issues/5671",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add chat and generate tests with mock runner",
            "url": "https://github.com/ollama/ollama/pull/5684",
            "state": "MERGED",
            "createdAt": "2024-07-14T01:07:47Z",
            "mergedAt": "2024-07-16T16:44:46Z",
            "closedAt": "2024-07-16T16:44:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 679,
            "deletions": 14,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: lowercase roles for compatibility with clients",
            "url": "https://github.com/ollama/ollama/pull/5695",
            "state": "MERGED",
            "createdAt": "2024-07-15T05:44:15Z",
            "mergedAt": "2024-07-15T20:55:57Z",
            "closedAt": "2024-07-15T20:55:57Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 37,
            "deletions": 2,
            "body": "Fixes https://github.com/ollama/ollama/issues/5687. Note: we may first want to make sure that there are no cases where roles may be uppercase.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add sidellama link",
            "url": "https://github.com/ollama/ollama/pull/5702",
            "state": "MERGED",
            "createdAt": "2024-07-15T14:16:19Z",
            "mergedAt": "2024-07-17T17:24:44Z",
            "closedAt": "2024-07-17T17:24:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Enable windows error dialog for subprocess",
            "url": "https://github.com/ollama/ollama/pull/5705",
            "state": "MERGED",
            "createdAt": "2024-07-15T16:31:31Z",
            "mergedAt": "2024-07-26T21:49:34Z",
            "closedAt": "2024-07-26T21:49:34Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 6
            },
            "additions": 32,
            "deletions": 2,
            "body": "Make sure if something goes wrong spawning the process, the user gets\r\nenough info to be able to try to self correct, or at least file a bug\r\nwith details so we can fix it.  Once the process starts, we immediately\r\nchange back to the recommended setting to prevent the blocking dialog.\r\nThis ensures if the model fails to load (OOM, unsupported model type,\r\netc.) the process will exit quickly and we can scan the stdout/stderr\r\nof the subprocess for the reason to report via API.\r\n\r\nExample when I remove the `ggml.dll` side-band and try to run a model\r\n\r\n![Screenshot 2024-07-15 at 9 19 13\u202fAM](https://github.com/user-attachments/assets/307a60fa-fb07-4c53-bfec-fd4e400961cb)\r\n\r\nNote: ollama will be ~stuck until the user clicks the OK button as the subprocess is paused but doesn't exit until they acknowledge, and depending on the nature of the problem, they'll likely get the dialog ~2 times as we attempt fallback runners.  If they didn't click the button within our startup timeout (currently 5min) we'd eventually timeout the model load.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add Metrics to `api\\embed` response",
            "url": "https://github.com/ollama/ollama/pull/5709",
            "state": "MERGED",
            "createdAt": "2024-07-15T22:10:46Z",
            "mergedAt": "2024-07-30T20:12:21Z",
            "closedAt": "2024-07-30T20:12:21Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 6
            },
            "additions": 40,
            "deletions": 16,
            "body": "\"timings\" is returned per request_completion in server.cpp, which must be aggregated to return metrics for a batch of completions.\r\n\r\nsupporting: prompt_eval_count (total number of tokens evaluated), load duration, total duration",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump linux ROCm to 6.1.2",
            "url": "https://github.com/ollama/ollama/pull/5710",
            "state": "MERGED",
            "createdAt": "2024-07-15T22:11:23Z",
            "mergedAt": "2024-07-15T22:32:18Z",
            "closedAt": "2024-07-15T22:32:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "This might help #5708 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Windows arm64 support to official builds",
            "url": "https://github.com/ollama/ollama/pull/5712",
            "state": "MERGED",
            "createdAt": "2024-07-15T23:18:15Z",
            "mergedAt": "2024-09-20T20:09:38Z",
            "closedAt": "2024-09-20T20:09:38Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 8
            },
            "additions": 310,
            "deletions": 44,
            "body": "Wire up CI and build rigging to generate a unified Windows installer with x64 and arm64 payloads.  At install time, the correct binaries will be installed for the platform.\r\n\r\nI was unable to find a combination of hand-picked msvc redist DLLs manually that yielded a working setup on a pristine Windows 11 install, but running the vc_redist installer works reliably, so for arm64, we run the nested installer conditionally.   If it is already installed, that step will be skipped.\r\n\r\nFixes #2589 \r\n\r\nNote:  I've tested most of the CI steps in the PR, but signing isn't yet verified and might require minor fixes on the first release after this merges.\r\n\r\nResulting build artifacts: (Note: current OllamaSetup.exe with only x64 binaries is 273MB)\r\n```\r\n% ls -lh dist/\r\ntotal 932M\r\n-rw-r--r-- 1 daniel 197609  12K Jul 17 09:24 ollama_welcome.ps1\r\n-rwxr-xr-x 1 daniel 197609 291M Jul 17 09:27 OllamaSetup.exe*\r\n-rw-r--r-- 1 daniel 197609 649M Jul 17 09:27 ollama-windows-amd64.zip\r\n-rw-r--r-- 1 daniel 197609  20M Jul 19 15:41 ollama-windows-arm64.zip\r\ndrwxr-xr-x 1 daniel 197609    0 Jul 17 09:24 windows-amd64/\r\n-rwxr-xr-x 1 daniel 197609 5.9M Jul 17 09:24 windows-amd64-app.exe*\r\ndrwxr-xr-x 1 daniel 197609    0 Jul 16 15:53 windows-arm64/\r\n-rwxr-xr-x 1 daniel 197609 5.5M Jul 16 16:12 windows-arm64-app.exe*\r\n% du -sh dist/windows-a*64\r\n2.1G    dist/windows-amd64\r\n37M     dist/windows-arm64\r\n```\r\n\r\nOn a Snapdragon X 12-core laptop:\r\n```\r\n> ollama run --verbose llama3 why is the sky blue\r\n...\r\ntotal duration:       23.6819409s\r\nload duration:        4.738127s\r\nprompt eval count:    16 token(s)\r\nprompt eval duration: 430.297ms\r\nprompt eval rate:     37.18 tokens/s\r\neval count:           348 token(s)\r\neval duration:        18.513796s\r\neval rate:            18.80 tokens/s\r\n```\r\n",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 14
            }
        }
    },
    {
        "node": {
            "title": "server: return empty slice on empty `/api/embed` request",
            "url": "https://github.com/ollama/ollama/pull/5713",
            "state": "MERGED",
            "createdAt": "2024-07-16T00:19:47Z",
            "mergedAt": "2024-07-16T00:39:45Z",
            "closedAt": "2024-07-16T00:39:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 7,
            "deletions": 3,
            "body": "Before:\r\n\r\n```\r\ncurl http://localhost:11434/api/embed \\                                 \r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"input\": \"\",\r\n    \"model\": \"all-minilm\"\r\n  }'\r\n{\"model\":\"all-minilm\"}\r\n```\r\n\r\nAfter:\r\n\r\n```\r\ncurl http://localhost:11434/api/embed \\                                 \r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"input\": \"\",\r\n    \"model\": \"all-minilm\"\r\n  }'\r\n{\"model\":\"all-minilm\",\"embeddings\":[]}\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README.md: Package managers: add Gentoo",
            "url": "https://github.com/ollama/ollama/pull/5714",
            "state": "MERGED",
            "createdAt": "2024-07-16T03:09:17Z",
            "mergedAt": "2024-09-05T16:58:14Z",
            "closedAt": "2024-09-05T16:58:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "server: omit model system prompt if empty",
            "url": "https://github.com/ollama/ollama/pull/5717",
            "state": "MERGED",
            "createdAt": "2024-07-16T04:14:45Z",
            "mergedAt": "2024-07-16T18:09:00Z",
            "closedAt": "2024-07-16T18:09:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The model's system prompt (defined by the `SYSTEM` Modelfile command) will be templated out even if empty currently. This fixes the issue so that it is only templated if not empty.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README: Added AI Studio to the list of UIs",
            "url": "https://github.com/ollama/ollama/pull/5721",
            "state": "MERGED",
            "createdAt": "2024-07-16T09:17:40Z",
            "mergedAt": "2024-07-16T21:24:27Z",
            "closedAt": "2024-07-16T21:24:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I added [AI Studio](https://github.com/MindWorkAI/AI-Studio) to the list of UIs.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix unmarshal type errors",
            "url": "https://github.com/ollama/ollama/pull/5726",
            "state": "MERGED",
            "createdAt": "2024-07-16T18:11:29Z",
            "mergedAt": "2024-07-16T19:12:10Z",
            "closedAt": "2024-07-16T19:12:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 39,
            "deletions": 42,
            "body": "skip unmarshalable types when parsing tool call responses",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: update message processing",
            "url": "https://github.com/ollama/ollama/pull/5729",
            "state": "MERGED",
            "createdAt": "2024-07-16T20:25:11Z",
            "mergedAt": "2024-07-19T18:19:20Z",
            "closedAt": "2024-07-19T18:19:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove unneeded tool calls",
            "url": "https://github.com/ollama/ollama/pull/5730",
            "state": "MERGED",
            "createdAt": "2024-07-16T20:50:20Z",
            "mergedAt": "2024-07-16T21:42:13Z",
            "closedAt": "2024-07-16T21:42:13Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 1,
            "deletions": 15,
            "body": "ID and Type are currently unused so leave them out for now",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove ToolCall from GenerateResponse",
            "url": "https://github.com/ollama/ollama/pull/5732",
            "state": "MERGED",
            "createdAt": "2024-07-16T21:52:21Z",
            "mergedAt": "2024-07-17T17:26:54Z",
            "closedAt": "2024-07-17T17:26:54Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 0,
            "deletions": 8,
            "body": "tools is only applicable for chat",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: validate template",
            "url": "https://github.com/ollama/ollama/pull/5734",
            "state": "MERGED",
            "createdAt": "2024-07-17T00:13:11Z",
            "mergedAt": "2024-07-19T22:24:29Z",
            "closedAt": "2024-07-19T22:24:29Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 3
            },
            "additions": 53,
            "deletions": 3,
            "body": "tries to parse template and returns error if it fails.\r\n\r\nresolves: https://github.com/ollama/ollama/issues/5449",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Make `tool_call` response a `string`",
            "url": "https://github.com/ollama/ollama/pull/5739",
            "state": "MERGED",
            "createdAt": "2024-07-17T02:45:03Z",
            "mergedAt": "2024-07-17T03:14:17Z",
            "closedAt": "2024-07-17T03:14:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 28,
            "deletions": 5,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added code to use swap memory in linux",
            "url": "https://github.com/ollama/ollama/pull/5742",
            "state": "CLOSED",
            "createdAt": "2024-07-17T09:54:10Z",
            "mergedAt": null,
            "closedAt": "2024-09-03T16:54:39Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 30,
            "deletions": 6,
            "body": "",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "stub response",
            "url": "https://github.com/ollama/ollama/pull/5750",
            "state": "MERGED",
            "createdAt": "2024-07-17T17:28:46Z",
            "mergedAt": "2024-07-17T17:39:22Z",
            "closedAt": "2024-07-17T17:39:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "for compatibility, `{{ .Response }}` cannot be in any template control flow structures. therefore any template execution should set an empty Response if one should not be rendered otherwise the output will contain `<no value>` in place of `{{ .Response }}`",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Function Based Testing",
            "url": "https://github.com/ollama/ollama/pull/5752",
            "state": "MERGED",
            "createdAt": "2024-07-17T18:17:00Z",
            "mergedAt": "2024-07-19T18:37:13Z",
            "closedAt": "2024-07-19T18:37:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 266,
            "deletions": 170,
            "body": "Distinguish tests by function, testing requests and error forwarding\r\n\r\ncaptureRequestMiddleware catches the request after it has been converted by the functionality middleware, before hitting a mock endpoint returning 200\r\n\r\nResponseRecorder catches any errors that are returned in the response body immediately by the middleware before reaching the endpoint",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "parse tool call as individual objects",
            "url": "https://github.com/ollama/ollama/pull/5753",
            "state": "MERGED",
            "createdAt": "2024-07-17T18:22:04Z",
            "mergedAt": "2024-07-17T18:47:54Z",
            "closedAt": "2024-07-17T18:47:54Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 76,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "bump go version to 1.22.5 to fix security vulnerabilities in docker",
            "url": "https://github.com/ollama/ollama/pull/5757",
            "state": "MERGED",
            "createdAt": "2024-07-17T21:59:06Z",
            "mergedAt": "2024-07-22T23:32:43Z",
            "closedAt": "2024-07-22T23:32:43Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The existing Version of 1.22.1 Is showing Security Vulnerabilities when scanned by Prisma\r\n\r\nScan results for: image ollama/ollama:latest sha256:56505af4d7ed5e66de96c124c21312aee6cdd518098efd0fa524738f24b1a701\r\nVulnerabilities\r\n|       CVE        | SEVERITY | CVSS |         PACKAGE          |        VERSION        |          STATUS          | PUBLISHED  | DISCOVERED |                    DESCRIPTION                     |\r\n| CVE-2024-24790   | critical | 9.80 | net/netip                | 1.22.1                | fixed in 1.21.11, 1.22.4 | 42 days    | < 1 hour   | The various Is methods (IsPrivate, IsLoopback,     |\r\n|                  |          |      |                          |                       | 42 days ago              |            |            \r\n\r\nThis minor update to GO 1.22.5 fixes the CRITICAL CVE-2024-24790, as well as corrects the MEDIUM  CVE-2024-24791  .\r\n\r\nI locally built and tested the Docker Build.\r\n\r\nScan results for: image ollama_orig_1_22_5:latest sha256:5b7f98e681c9a7b807d02beecc2eb303a5303a6cd248dcf448ae360e79b759ab\r\nVulnerabilities found for image ollama_orig_1_22_5:latest: total - 16, critical - 0, high - 0, medium - 4, low - 12\r\n\r\nIt would be great to get these fixes in ASAP.\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "marshal json automatically for some template values",
            "url": "https://github.com/ollama/ollama/pull/5758",
            "state": "MERGED",
            "createdAt": "2024-07-17T22:02:46Z",
            "mergedAt": "2024-07-17T22:35:11Z",
            "closedAt": "2024-07-17T22:35:11Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 8
            },
            "additions": 72,
            "deletions": 52,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: only parse tool calls if tools are provided",
            "url": "https://github.com/ollama/ollama/pull/5771",
            "state": "MERGED",
            "createdAt": "2024-07-18T15:26:50Z",
            "mergedAt": "2024-07-18T15:50:24Z",
            "closedAt": "2024-07-18T15:50:24Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "api: always provide content even if empty",
            "url": "https://github.com/ollama/ollama/pull/5778",
            "state": "MERGED",
            "createdAt": "2024-07-18T18:24:11Z",
            "mergedAt": "2024-07-18T18:28:19Z",
            "closedAt": "2024-07-18T18:28:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: check for empty tools array too",
            "url": "https://github.com/ollama/ollama/pull/5779",
            "state": "MERGED",
            "createdAt": "2024-07-18T18:43:28Z",
            "mergedAt": "2024-07-18T18:44:58Z",
            "closedAt": "2024-07-18T18:44:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix parsing tool calls: break on unexpected eofs",
            "url": "https://github.com/ollama/ollama/pull/5780",
            "state": "MERGED",
            "createdAt": "2024-07-18T19:08:33Z",
            "mergedAt": "2024-07-18T19:14:10Z",
            "closedAt": "2024-07-18T19:14:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README: Added LLMStack to the list of UI integrations",
            "url": "https://github.com/ollama/ollama/pull/5799",
            "state": "MERGED",
            "createdAt": "2024-07-19T18:52:41Z",
            "mergedAt": "2024-07-23T18:40:23Z",
            "closedAt": "2024-07-23T18:40:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Also wrote a quick guide to show to how to use `ollama` with `LLMStack` at https://docs.trypromptly.com/guides/using-llama3-with-ollama.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "preserve last assistant message",
            "url": "https://github.com/ollama/ollama/pull/5802",
            "state": "MERGED",
            "createdAt": "2024-07-20T00:50:59Z",
            "mergedAt": "2024-07-20T03:19:26Z",
            "closedAt": "2024-07-20T03:19:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 22,
            "deletions": 1,
            "body": "Fixes https://github.com/ollama/ollama/issues/5775",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "template: fix mistral not adding system prompt if assistant is last message",
            "url": "https://github.com/ollama/ollama/pull/5803",
            "state": "CLOSED",
            "createdAt": "2024-07-20T01:22:01Z",
            "mergedAt": null,
            "closedAt": "2024-09-04T19:08:39Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix generate test flakyness",
            "url": "https://github.com/ollama/ollama/pull/5804",
            "state": "MERGED",
            "createdAt": "2024-07-20T01:43:13Z",
            "mergedAt": "2024-07-20T02:11:25Z",
            "closedAt": "2024-07-20T02:11:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update llama.cpp submodule commit to `d94c6e0c`",
            "url": "https://github.com/ollama/ollama/pull/5805",
            "state": "MERGED",
            "createdAt": "2024-07-20T02:49:45Z",
            "mergedAt": "2024-07-22T16:42:00Z",
            "closedAt": "2024-07-22T16:42:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 9
            },
            "additions": 366,
            "deletions": 81,
            "body": "",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Adjust windows ROCm discovery",
            "url": "https://github.com/ollama/ollama/pull/5815",
            "state": "MERGED",
            "createdAt": "2024-07-20T16:23:10Z",
            "mergedAt": "2024-07-20T23:02:55Z",
            "closedAt": "2024-07-20T23:02:55Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 5
            },
            "additions": 21,
            "deletions": 6,
            "body": "The v5 hip library returns unsupported GPUs which wont enumerate at inference time in the runner so this makes sure we align discovery. The gfx906 cards are no longer supported so we shouldn't compile with that GPU type as it wont enumerate at runtime.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "convert: capture `head_dim` for mistral",
            "url": "https://github.com/ollama/ollama/pull/5818",
            "state": "MERGED",
            "createdAt": "2024-07-20T22:16:36Z",
            "mergedAt": "2024-07-22T20:16:22Z",
            "closedAt": "2024-07-22T20:16:22Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Track GPU discovery failure information ",
            "url": "https://github.com/ollama/ollama/pull/5820",
            "state": "MERGED",
            "createdAt": "2024-07-20T22:35:34Z",
            "mergedAt": "2024-10-14T23:26:45Z",
            "closedAt": "2024-10-14T23:26:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 242,
            "deletions": 92,
            "body": "When ollama is unable to utilize a GPU on the users system, figuring out why can be tricky for many users to figure out.  We log various messages (some at debug) and often have to ask users to re-run the server with OLLAMA_DEBUG=1 and share the server logs and manually look through the logs to figure out and explain the problem.\r\n\r\nThis PR lays some initial foundation to record general discovery errors, as well as per-device unsupported information.\r\n\r\nI'm making this draft for now as the API probably isn't something we want to expose in its current form.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "server: collect nested tool call objects when parsing",
            "url": "https://github.com/ollama/ollama/pull/5824",
            "state": "MERGED",
            "createdAt": "2024-07-21T03:58:05Z",
            "mergedAt": "2024-07-22T16:38:03Z",
            "closedAt": "2024-07-22T16:38:03Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 5
            },
            "additions": 120,
            "deletions": 13,
            "body": "This changes tool calling to handle the output format:\r\n\r\n```\r\n{\r\n    \"tool_calls\": [\r\n        {\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\r\n        ... (more tool calls as required)\r\n    ]\r\n}\r\n```\r\n\r\nTo support\r\n\r\nhttps://huggingface.co/Salesforce/xLAM-1b-fc-r\r\nhttps://huggingface.co/Salesforce/xLAM-7b-fc-r\r\n\r\nand future models that may have tool call objects nested in others",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove out of space test temporarily",
            "url": "https://github.com/ollama/ollama/pull/5825",
            "state": "MERGED",
            "createdAt": "2024-07-21T04:16:51Z",
            "mergedAt": "2024-07-21T04:22:12Z",
            "closedAt": "2024-07-21T04:22:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 37,
            "body": "Removes the out of space test which won't trigger on CI \u2013 I wasn't sure if there was a good way to actually test this since it would involve creating a subprocess in the unit tests",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix \"finish_reason\" when tools are called",
            "url": "https://github.com/ollama/ollama/pull/5829",
            "state": "CLOSED",
            "createdAt": "2024-07-21T12:33:40Z",
            "mergedAt": null,
            "closedAt": "2024-07-23T22:34:32Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 0,
            "body": "Not sure if it fully fixes #5796 \r\nPlease, review and help if possible (I am not familiar with go)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Reduce docker image size",
            "url": "https://github.com/ollama/ollama/pull/5847",
            "state": "MERGED",
            "createdAt": "2024-07-22T09:31:17Z",
            "mergedAt": "2024-09-03T16:25:31Z",
            "closedAt": "2024-09-03T16:25:31Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 11,
            "deletions": 9,
            "body": "The docker image size is approximately reduced by 20MB after cleaning the apt caches.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Refine error reporting for subprocess crash",
            "url": "https://github.com/ollama/ollama/pull/5854",
            "state": "MERGED",
            "createdAt": "2024-07-22T15:56:13Z",
            "mergedAt": "2024-07-22T17:40:22Z",
            "closedAt": "2024-07-22T17:40:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 9,
            "body": "On windows, the exit status winds up being the search term many users search for and end up piling in on issues that are unrelated. This refines the reporting so that if we have a more detailed message we'll suppress the exit status portion of the message.\r\n\r\n\r\nExample:\r\n\r\nBefore\r\n```\r\n> ollama run akuldatta/mistral-nemo-instruct-12b:q5km\r\nError: llama runner process has terminated: exit status 0xc0000409 error loading model: check_tensor_dims: tensor 'blk.0.attn_q.weight' has wrong shape; expected  5120,  5120, got  5120,  4096,     1,     1\r\n```\r\n\r\nAfter:\r\n```\r\n> ollama run akuldatta/mistral-nemo-instruct-12b:q5km\r\nError: llama runner process has terminated: error loading model: check_tensor_dims: tensor 'blk.0.attn_q.weight' has wrong shape; expected  5120,  5120, got  5120,  4096,     1,     1\r\n```\r\n\r\nThis should reduce the amount of users posting unrelated problems on whatever open issue(s) happen to have `0xc0000409` in the title.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove no longer supported max vram var",
            "url": "https://github.com/ollama/ollama/pull/5855",
            "state": "MERGED",
            "createdAt": "2024-07-22T16:09:01Z",
            "mergedAt": "2024-07-22T17:35:29Z",
            "closedAt": "2024-07-22T17:35:29Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 2,
            "deletions": 16,
            "body": "The OLLAMA_MAX_VRAM env var was a temporary workaround for OOM scenarios.  With Concurrency this was no longer wired up, and the simplistic value doesn't map to multi-GPU setups.  Users can still set `num_gpu` to limit memory usage to avoid OOM if we get our predictions wrong.\r\n\r\nFixes #5754 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: fix dupe err message",
            "url": "https://github.com/ollama/ollama/pull/5857",
            "state": "MERGED",
            "createdAt": "2024-07-22T18:36:27Z",
            "mergedAt": "2024-07-22T22:48:15Z",
            "closedAt": "2024-07-22T22:48:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 4,
            "body": "https://github.com/ollama/ollama/pull/5734#discussion_r1686970988 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Simplify input output in testing",
            "url": "https://github.com/ollama/ollama/pull/5858",
            "state": "MERGED",
            "createdAt": "2024-07-22T18:43:05Z",
            "mergedAt": "2024-08-12T17:33:34Z",
            "closedAt": "2024-08-12T17:33:34Z",
            "reviews": {
                "totalCount": 7
            },
            "files": {
                "totalCount": 1
            },
            "additions": 335,
            "deletions": 315,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Prevent partial loading on mixed GPU brands",
            "url": "https://github.com/ollama/ollama/pull/5859",
            "state": "MERGED",
            "createdAt": "2024-07-22T18:59:27Z",
            "mergedAt": "2024-07-30T18:06:42Z",
            "closedAt": "2024-07-30T18:06:42Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 2
            },
            "additions": 66,
            "deletions": 4,
            "body": "In mult-brand GPU setups, if we couldn't fully load the model we would fall through the scheduler and mistakenly try to load across a mix of brands.  This makes sure we find the set of GPU(s) that best fit for the partial load.\r\n\r\nFixes #5476 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Bump Go patch version",
            "url": "https://github.com/ollama/ollama/pull/5864",
            "state": "MERGED",
            "createdAt": "2024-07-22T23:17:38Z",
            "mergedAt": "2024-07-22T23:34:18Z",
            "closedAt": "2024-07-22T23:34:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 11,
            "deletions": 11,
            "body": "Fixes #5774 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update faq.md",
            "url": "https://github.com/ollama/ollama/pull/5873",
            "state": "CLOSED",
            "createdAt": "2024-07-23T12:06:35Z",
            "mergedAt": null,
            "closedAt": "2024-08-14T16:41:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "I tried to find the model storage path in the FAQ, but it was missing: For two of my arch linux based machines, it is \"/var/lib/ollama/.ollama/models\", not \"/usr/share/ollama/.ollama/models\". I don't know about other distributions\r\n\r\nAdded this model storage path to FAQ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "OpenAI: Add Usage to `v1/embeddings`",
            "url": "https://github.com/ollama/ollama/pull/5886",
            "state": "MERGED",
            "createdAt": "2024-07-23T19:34:33Z",
            "mergedAt": "2024-08-01T22:49:37Z",
            "closedAt": "2024-08-01T22:49:37Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 13,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update gpu.md: Add RTX 3050 Ti and RTX 3050 Ti",
            "url": "https://github.com/ollama/ollama/pull/5888",
            "state": "MERGED",
            "createdAt": "2024-07-23T20:24:17Z",
            "mergedAt": "2024-09-05T18:24:26Z",
            "closedAt": "2024-09-05T18:24:26Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Seems strange that the laptop versions of 3050 and 3050 Ti would be supported but not the non-notebook, but this is what the page (https://developer.nvidia.com/cuda-gpus) says.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "api: add stringifier for `Tool`",
            "url": "https://github.com/ollama/ollama/pull/5891",
            "state": "MERGED",
            "createdAt": "2024-07-23T20:58:42Z",
            "mergedAt": "2024-07-29T20:35:16Z",
            "closedAt": "2024-07-29T20:35:16Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix Embed Test Flakes",
            "url": "https://github.com/ollama/ollama/pull/5893",
            "state": "MERGED",
            "createdAt": "2024-07-23T22:05:50Z",
            "mergedAt": "2024-07-24T18:15:46Z",
            "closedAt": "2024-07-24T18:15:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 54,
            "deletions": 5,
            "body": "different results on different taters\r\ne.g.\r\n\r\n=== RUN   TestAllMiniLMEmbeddings\r\n2024/07/23 17:05:36 INFO server connection host=tater21 port=55426\r\n2024/07/23 17:05:36 INFO checking status of model model=all-minilm\r\n2024/07/23 17:05:36 INFO model already present model=all-minilm\r\n    embed_test.go:42: expected 0.06642947345972061, got 0.0664294660091400\r\n--- FAIL: TestAllMiniLMEmbeddings (0.31s)\r\n\r\n\r\nalso added a basic sanity check for /api/embeddings, which had no test before",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "feat: K/V cache quantisation (massive vRAM improvement!)",
            "url": "https://github.com/ollama/ollama/pull/5894",
            "state": "CLOSED",
            "createdAt": "2024-07-23T22:16:36Z",
            "mergedAt": null,
            "closedAt": "2024-08-09T06:45:43Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 21
            },
            "additions": 212,
            "deletions": 86,
            "body": "# THIS PR HAS MOVED TO https://github.com/ollama/ollama/pull/6279\r\n\r\n---\r\n\r\nThis PR introduces optional K/V (context) cache quantisation.\r\n\r\nIn addition the deprecated `F16KV` parameter has been removed, if a user wishes for some reason to run the KV at f32, they can provide that as an option.\r\n\r\n## Impact\r\n\r\n- With defaults (f16) - none, behaviour is the same as the current defaults.\r\n- With q8_0\r\n\t- **The K/V context cache will consume 1/2 the vRAM** (!)\r\n\t- A _very_ small loss in quality within the cache\r\n- With q4_0\r\n\t- **the K/V context cache will consume 1/4 the vRAM** (!!)\r\n\t- A small/medium loss in quality within the cache\r\n\t- For example, loading llama3.1 8b with a 32K context drops vRAM usage by cache from 4GB to 1.1GB \r\n- The and q4_1 -> q5_1 in between.\r\n\r\nAdditional quantisations supported by llama.cpp and this PR that may depend on the quantisation of the model you're running:\r\n\r\n`q5_1`, `q5_0`, `q4_1`, `iq4_nl`\r\n\r\n- Fixes https://github.com/ollama/ollama/issues/5091\r\n- Related discussion in llama.cpp - https://github.com/ggerganov/llama.cpp/discussions/5932\r\n\t- (Note that ExllamaV2 has a similar feature - https://github.com/turboderp/exllamav2/blob/master/doc/qcache_eval.md)\r\n\r\n## Screenshots\r\n\r\nExample of estimated (v)RAM savings - f16 (q8_0,q4_0)\r\n\r\n<img width=\"1211\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a3520770-7b31-40c7-b45b-4aad6db9b117\">\r\n\r\n\r\n### f16\r\n\r\n![kv_cache_f16](https://github.com/user-attachments/assets/af0a3b40-70e2-47f1-90b0-6ecd09dc59df)\r\n\r\n### q4_0\r\n\r\n![kv_cache_q4_0](https://github.com/user-attachments/assets/47ba6578-1f5b-4091-8594-f63ecfada49e)\r\n\r\n### q8_0\r\n\r\n![kv_cache_q8_0](https://github.com/user-attachments/assets/c7c09e62-4b54-4536-9617-6b00b1af6f94)\r\n\r\n\r\n## Performance\r\n\r\nllama.cpp did some perplexity measurements (although looking at the commits things have likely improved even further since May when they were done, and CUDA graphs were later fixed etc....): https://github.com/ggerganov/llama.cpp/pull/7412#issuecomment-2120427347\r\n\r\nAs far as I can tell (at least with q6_k quants) there isn't much of a noticeable hit to performance.\r\n",
            "participants": {
                "totalCount": 11
            },
            "comments": {
                "totalCount": 45
            }
        }
    },
    {
        "node": {
            "title": "Better explain multi-gpu behavior",
            "url": "https://github.com/ollama/ollama/pull/5895",
            "state": "MERGED",
            "createdAt": "2024-07-23T22:16:45Z",
            "mergedAt": "2024-07-29T21:25:41Z",
            "closedAt": "2024-07-29T21:25:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 1,
            "body": "Fixes #5635 #5455 \r\n\r\nThis topic seems to come up ~weekly, so lets explain it more clearly in the docs, and expose the existing env var to force spreading over all GPUs.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: speed up single gguf creates",
            "url": "https://github.com/ollama/ollama/pull/5898",
            "state": "MERGED",
            "createdAt": "2024-07-24T00:25:11Z",
            "mergedAt": "2024-08-12T16:28:55Z",
            "closedAt": "2024-08-12T16:28:55Z",
            "reviews": {
                "totalCount": 9
            },
            "files": {
                "totalCount": 2
            },
            "additions": 96,
            "deletions": 3,
            "body": "the blob for a file with a single gguf is already copied to the server on `/api/blobs/:digest`.\r\non `createModel`, we can avoid the rewriting of this blob.\r\n\r\nThis does not improve create speeds for safetensors or files with multiple gguf files\r\n\r\nNew logs\r\n```\r\n[GIN] 2024/07/23 - 17:21:45 | 201 |  5.298126708s |       127.0.0.1 | POST     \"/api/blobs/sha256:54696cbcadd1959275fc99f9cc67880d2f38419124da06cdf2140bad2dc3d94c\"\r\n[GIN] 2024/07/23 - 17:21:45 | 200 |    8.519125ms |       127.0.0.1 | POST     \"/api/create\"\r\n```\r\n\r\nOld logs\r\n```\r\n[GIN] 2024/07/23 - 17:24:27 | 201 |  5.283626083s |       127.0.0.1 | POST     \"/api/blobs/sha256:54696cbcadd1959275fc99f9cc67880d2f38419124da06cdf2140bad2dc3d94c\"\r\n[GIN] 2024/07/23 - 17:24:32 | 200 |  4.302020959s |       127.0.0.1 | POST     \"/api/create\"\r\n```\r\n\r\n\r\nresolves: https://github.com/ollama/ollama/issues/5388",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Introduce GPU Overhead env var",
            "url": "https://github.com/ollama/ollama/pull/5922",
            "state": "MERGED",
            "createdAt": "2024-07-24T18:33:55Z",
            "mergedAt": "2024-09-05T20:46:35Z",
            "closedAt": "2024-09-05T20:46:35Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 3
            },
            "additions": 28,
            "deletions": 3,
            "body": "Provide a mechanism for users to set aside an amount of VRAM on each GPU to make room for other applications they want to start after Ollama, or workaround memory prediction bugs\r\n\r\nConcurrency and multi-GPU support made the older `OLLAMA_MAX_VRAM` setting untenable (many users have multi-GPU setups with different sizes of VRAM) so this was removed in #5855  This new setting uses an \"overhead\" value so we can subtract that from the reported Free space on each GPU for more reasonable/consistent results.\r\n```\r\n      OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU (bytes)\r\n```\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llm(llama): pass rope factors",
            "url": "https://github.com/ollama/ollama/pull/5924",
            "state": "MERGED",
            "createdAt": "2024-07-24T19:42:31Z",
            "mergedAt": "2024-07-24T20:05:59Z",
            "closedAt": "2024-07-24T20:05:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 71,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Prevent loading too large models on windows",
            "url": "https://github.com/ollama/ollama/pull/5926",
            "state": "MERGED",
            "createdAt": "2024-07-24T20:19:25Z",
            "mergedAt": "2024-08-11T18:30:20Z",
            "closedAt": "2024-08-11T18:30:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "We already blocked linux memory exhaustion, but should apply the same check for Windows as well\r\n\r\nWe can't apply the same logic to MacOS, as it uses fully dynamic swap space and has no concept of free swap space.\r\n\r\nFixes #5882 \r\nFixes #4955\r\nFixes #5958",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Ensure amd gpu nodes are numerically sorted",
            "url": "https://github.com/ollama/ollama/pull/5927",
            "state": "MERGED",
            "createdAt": "2024-07-24T20:46:21Z",
            "mergedAt": "2024-07-29T21:24:57Z",
            "closedAt": "2024-07-29T21:24:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 0,
            "body": "For systems that enumerate over 10 CPUs the default lexicographical sort order interleaves CPUs and GPUs.\r\n\r\nFixes #5908 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Explain font problems on windows 10",
            "url": "https://github.com/ollama/ollama/pull/5932",
            "state": "MERGED",
            "createdAt": "2024-07-24T22:22:25Z",
            "mergedAt": "2024-07-29T20:40:24Z",
            "closedAt": "2024-07-29T20:40:24Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Fixes #2549 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update readme to llama3.1",
            "url": "https://github.com/ollama/ollama/pull/5933",
            "state": "MERGED",
            "createdAt": "2024-07-24T22:45:12Z",
            "mergedAt": "2024-07-28T21:21:38Z",
            "closedAt": "2024-07-28T21:21:38Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 16,
            "deletions": 15,
            "body": "",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Report better error on cuda unsupported os/arch",
            "url": "https://github.com/ollama/ollama/pull/5934",
            "state": "MERGED",
            "createdAt": "2024-07-25T00:11:57Z",
            "mergedAt": "2024-07-29T21:24:20Z",
            "closedAt": "2024-07-29T21:24:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 17,
            "deletions": 3,
            "body": "If we detect an NVIDIA GPU, but nvidia doesn't support the os/arch, this will report a better error for the user and point them to docs to self-install the drivers if possible.\r\n\r\nFixes #3261 #2302 \r\n\r\nExample output on Ubuntu 22.04 on AWS g5g.xlarge (arm64)\r\n```\r\n% sh ./install.sh\r\n>>> Downloading ollama...\r\n######################################################################################################################################### 100.0%######################################################################################################################################### 100.0%\r\n>>> Installing ollama to /usr/local/bin...\r\n>>> Adding ollama user to render group...\r\n>>> Adding ollama user to video group...\r\n>>> Adding current user to ollama group...\r\n>>> Creating ollama systemd service...\r\n>>> Enabling and starting ollama service...\r\n>>> Installing NVIDIA repository...\r\nERROR NVIDIA GPU detected, but your OS and Architecture are not supported by NVIDIA. Please install the CUDA driver manually https://docs.nvidia.com/cuda/cuda-installation-guide-linux/\r\n% echo $?\r\n1\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: reuse original download URL for images",
            "url": "https://github.com/ollama/ollama/pull/5962",
            "state": "MERGED",
            "createdAt": "2024-07-25T20:32:06Z",
            "mergedAt": "2024-07-25T22:58:30Z",
            "closedAt": "2024-07-25T22:58:30Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 2
            },
            "additions": 79,
            "deletions": 2,
            "body": "This changes the registry client to reuse the original download URL it gets on the first redirect response for all subsequent requests, preventing thundering herd issues when hot new LLMs are released.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix typo and improve readability",
            "url": "https://github.com/ollama/ollama/pull/5964",
            "state": "MERGED",
            "createdAt": "2024-07-26T00:13:38Z",
            "mergedAt": "2024-08-14T00:54:20Z",
            "closedAt": "2024-08-14T00:54:20Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 11,
            "deletions": 11,
            "body": "* Rename updatAvailableMenuID to updateAvailableMenuID\r\n* Replace unused cmd parameter with _ in RunServer function\r\n* Fix typos in comments\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Use llama3.1 in tools example",
            "url": "https://github.com/ollama/ollama/pull/5985",
            "state": "MERGED",
            "createdAt": "2024-07-26T14:07:11Z",
            "mergedAt": "2024-08-07T21:20:50Z",
            "closedAt": "2024-08-07T21:20:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Running this example with `mistral` produces the error \"mistral does not support tools\". What wasn't obvious to me until I made this PR was that my copy of mistral needed upgrading for tools (`ollama pull mistral`). Making the example be `llama3.1` will lead to more success for other long time ollama users.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: keep patch for llama 3 rope factors",
            "url": "https://github.com/ollama/ollama/pull/5987",
            "state": "MERGED",
            "createdAt": "2024-07-26T15:16:34Z",
            "mergedAt": "2024-07-26T22:20:53Z",
            "closedAt": "2024-07-26T22:20:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 70,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: model save",
            "url": "https://github.com/ollama/ollama/pull/5992",
            "state": "MERGED",
            "createdAt": "2024-07-26T20:26:10Z",
            "mergedAt": "2024-07-29T16:53:19Z",
            "closedAt": "2024-07-29T16:53:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 60,
            "deletions": 61,
            "body": "stop parameter is saved as a slice which is incompatible with modelfile parsing. this change saves stop parameters as individual parameters\r\n\r\ne.g.\r\n\r\n```\r\n/set parameter stop word1 word2\r\n/save new-model\r\n```\r\n\r\nproduces `PARAMETER stop [word1 word2]`\r\n\r\ninstead it should produce\r\n\r\n```\r\nPARAMETER stop word1\r\nPARAMETER stop word2\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: fix race conditions during download",
            "url": "https://github.com/ollama/ollama/pull/5994",
            "state": "MERGED",
            "createdAt": "2024-07-26T20:44:18Z",
            "mergedAt": "2024-07-26T21:24:24Z",
            "closedAt": "2024-07-26T21:24:24Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 36,
            "deletions": 23,
            "body": "This fixes various data races scattered throughout the download/pull client where the client was accessing the download state concurrently.\r\n\r\nThis commit is mostly a hot-fix and will be replaced by a new client one day soon.\r\n\r\nAlso, remove the unnecessary opts argument from downloadChunk.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "return tool calls finish reason for openai",
            "url": "https://github.com/ollama/ollama/pull/5995",
            "state": "MERGED",
            "createdAt": "2024-07-26T20:46:29Z",
            "mergedAt": "2024-07-29T20:56:57Z",
            "closedAt": "2024-07-29T20:56:57Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add charla project to Terminal section",
            "url": "https://github.com/ollama/ollama/pull/5996",
            "state": "CLOSED",
            "createdAt": "2024-07-26T20:53:54Z",
            "mergedAt": null,
            "closedAt": "2024-09-09T21:06:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Charla is a simple terminal based chat application that works with local language models. I'd appreciate if you consider it as an example project.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "fix nil deref in auth.go",
            "url": "https://github.com/ollama/ollama/pull/5999",
            "state": "MERGED",
            "createdAt": "2024-07-26T21:10:57Z",
            "mergedAt": "2024-07-26T21:28:34Z",
            "closedAt": "2024-07-26T21:28:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "`server/auth.go` uses `makeRequest` with `nil` `registryOptions` so `registryOptions.CheckRedirect` will dereference `nil` and panic",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update install.sh\uff1aReplace \"command -v\" with encapsulated functionality",
            "url": "https://github.com/ollama/ollama/pull/6035",
            "state": "MERGED",
            "createdAt": "2024-07-29T02:12:51Z",
            "mergedAt": "2024-09-05T16:49:48Z",
            "closedAt": "2024-09-05T16:49:48Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Replace \"command -v\" with encapsulated functionality",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp submodule to `6eeaeba1`",
            "url": "https://github.com/ollama/ollama/pull/6039",
            "state": "MERGED",
            "createdAt": "2024-07-29T07:11:08Z",
            "mergedAt": "2024-07-29T20:20:26Z",
            "closedAt": "2024-07-29T20:20:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 8,
            "deletions": 89,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix typo in chat with image docs",
            "url": "https://github.com/ollama/ollama/pull/6041",
            "state": "MERGED",
            "createdAt": "2024-07-29T09:39:35Z",
            "mergedAt": "2024-07-29T15:50:53Z",
            "closedAt": "2024-07-29T15:50:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "It looks like the docs were just copy-pasted form the conversaiton history above. This PR fixes that part of the docs with a short explanation.\r\n\r\nCheers",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: update README.md",
            "url": "https://github.com/ollama/ollama/pull/6059",
            "state": "MERGED",
            "createdAt": "2024-07-29T17:37:09Z",
            "mergedAt": "2024-07-29T17:53:30Z",
            "closedAt": "2024-07-29T17:53:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "HuggingFace -> Hugging Face",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: OLLAMA in modelfile and manifests",
            "url": "https://github.com/ollama/ollama/pull/6062",
            "state": "CLOSED",
            "createdAt": "2024-07-29T21:34:00Z",
            "mergedAt": null,
            "closedAt": "2024-08-07T22:26:25Z",
            "reviews": {
                "totalCount": 11
            },
            "files": {
                "totalCount": 8
            },
            "additions": 203,
            "deletions": 17,
            "body": "new optional parameter `OLLAMA` in modelfile to specify minimum version of ollama to run this model:\r\n`ollama create newmodel`\r\n```\r\nFROM mymodel.gguf\r\nOLLAMA 0.2.3\r\n```\r\n\r\nusing another model with the `FROM` command defaults to the version if they specify it right now. otherwise, you can set it yourself\r\n```\r\nFROM newmodel\r\n```\r\nthis will inherit the `OLLAMA 0.2.3` requirement",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "convert: update llama conversion for llama3.1",
            "url": "https://github.com/ollama/ollama/pull/6064",
            "state": "MERGED",
            "createdAt": "2024-07-29T22:33:17Z",
            "mergedAt": "2024-08-21T19:57:09Z",
            "closedAt": "2024-08-21T19:57:09Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 9
            },
            "additions": 44,
            "deletions": 9,
            "body": "llama3.1 contains a new tensor for rope scaling factors. derive this new tensor from llama3.1 configs if they exist",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update and Fix example models ",
            "url": "https://github.com/ollama/ollama/pull/6065",
            "state": "MERGED",
            "createdAt": "2024-07-29T23:53:24Z",
            "mergedAt": "2024-07-30T06:56:37Z",
            "closedAt": "2024-07-30T06:56:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 19
            },
            "additions": 32,
            "deletions": 24,
            "body": "I updated the llama2/3 models to llama3.1 and gemma models to gemma2 in examples where needed and where I could test the example work. Fixed the dockerit example, it was pointing to a non-existing model. Lastly, I removed a blank unused README.md file in one of the Go examples.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Nix and Flox to package manager listing",
            "url": "https://github.com/ollama/ollama/pull/6074",
            "state": "MERGED",
            "createdAt": "2024-07-30T14:18:13Z",
            "mergedAt": "2024-08-29T16:45:35Z",
            "closedAt": "2024-08-29T16:45:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Ollama is available in both the Nix and Flox package manager.\r\nI thought it'd be a good idea to list them here as well. \ud83d\ude01",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add metrics to `api/embed` docs",
            "url": "https://github.com/ollama/ollama/pull/6079",
            "state": "MERGED",
            "createdAt": "2024-07-30T20:36:07Z",
            "mergedAt": "2024-08-07T21:43:44Z",
            "closedAt": "2024-08-07T21:43:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README to include Firebase Genkit",
            "url": "https://github.com/ollama/ollama/pull/6083",
            "state": "MERGED",
            "createdAt": "2024-07-31T01:38:31Z",
            "mergedAt": "2024-07-31T01:40:09Z",
            "closedAt": "2024-07-31T01:40:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Firebase Genkit",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add BoltAI as an Ollama Desktop GUI",
            "url": "https://github.com/ollama/ollama/pull/6096",
            "state": "MERGED",
            "createdAt": "2024-07-31T12:25:15Z",
            "mergedAt": "2024-07-31T15:44:58Z",
            "closedAt": "2024-07-31T15:44:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "### Overview\r\n\r\nBoltAI supports Ollama natively. It automatically synchronize with Ollama model lists, and allows users to use advanced features such as AI Command and AI Inline.\r\n\r\n### Screenshots\r\n\r\n**Main Chat UI**\r\n\r\n![CleanShot 2024-07-16 at 22 28 14@2x](https://github.com/user-attachments/assets/224f80bb-49e0-48fb-80ba-bfb8187d4ee2)\r\n\r\n**Model Management**\r\n\r\n![CleanShot 2024-07-16 at 22 28 34@2x](https://github.com/user-attachments/assets/b94826bf-3f11-4d63-b5dd-92aecc649652)\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add gollm to the list of librairies",
            "url": "https://github.com/ollama/ollama/pull/6099",
            "state": "MERGED",
            "createdAt": "2024-07-31T15:57:01Z",
            "mergedAt": "2024-09-04T18:19:42Z",
            "closedAt": "2024-09-04T18:19:42Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "cmd: spinner progress for transfer model data",
            "url": "https://github.com/ollama/ollama/pull/6100",
            "state": "MERGED",
            "createdAt": "2024-07-31T17:15:54Z",
            "mergedAt": "2024-08-12T18:46:32Z",
            "closedAt": "2024-08-12T18:46:32Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 52,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update the import docs",
            "url": "https://github.com/ollama/ollama/pull/6104",
            "state": "MERGED",
            "createdAt": "2024-07-31T20:27:36Z",
            "mergedAt": "2024-08-27T02:57:26Z",
            "closedAt": "2024-08-27T02:57:26Z",
            "reviews": {
                "totalCount": 26
            },
            "files": {
                "totalCount": 3
            },
            "additions": 141,
            "deletions": 45,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "patches: phi3 optional sliding window attention",
            "url": "https://github.com/ollama/ollama/pull/6106",
            "state": "MERGED",
            "createdAt": "2024-07-31T21:48:27Z",
            "mergedAt": "2024-07-31T23:12:06Z",
            "closedAt": "2024-07-31T23:12:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 43,
            "deletions": 0,
            "body": "this change allows models that do not set `phi3.attention.sliding_window` to revert to the previous behaviour instead of segfaulting\r\n\r\nresolves #5956 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Fix parallel requests",
            "url": "https://github.com/ollama/ollama/pull/6107",
            "state": "MERGED",
            "createdAt": "2024-07-31T22:05:57Z",
            "mergedAt": "2024-07-31T23:36:49Z",
            "closedAt": "2024-07-31T23:36:49Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 10,
            "deletions": 17,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: fix json marshalling of downloadBlobPart",
            "url": "https://github.com/ollama/ollama/pull/6108",
            "state": "MERGED",
            "createdAt": "2024-07-31T22:30:46Z",
            "mergedAt": "2024-07-31T23:01:25Z",
            "closedAt": "2024-07-31T23:01:25Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 30,
            "deletions": 0,
            "body": "The json marshalling of downloadBlobPart was incorrect and racey. This fixes it by implementing customer json marshalling for downloadBlobPart which correctly handles serialization of shared memory.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix modelfile message quotes",
            "url": "https://github.com/ollama/ollama/pull/6109",
            "state": "MERGED",
            "createdAt": "2024-07-31T23:54:18Z",
            "mergedAt": "2024-08-01T00:05:44Z",
            "closedAt": "2024-08-01T00:05:44Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "message commands should quote the content but it's not correctly formatted so it outputs content verbatim. fix the formatting will fix quoting\r\n\r\nhttps://github.com/ollama/ollama/blob/main/parser/parser.go#L41-L43\r\n\r\nresolves #6103",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Get embeddings working",
            "url": "https://github.com/ollama/ollama/pull/6110",
            "state": "MERGED",
            "createdAt": "2024-08-01T00:04:05Z",
            "mergedAt": "2024-08-01T14:59:35Z",
            "closedAt": "2024-08-01T14:59:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 29,
            "deletions": 14,
            "body": "Truncation doesn't pass, but the other embeddings tests pass",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Braina AI as an Ollama Desktop GUI",
            "url": "https://github.com/ollama/ollama/pull/6112",
            "state": "CLOSED",
            "createdAt": "2024-08-01T04:38:53Z",
            "mergedAt": null,
            "closedAt": "2024-09-06T02:22:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 0
            },
            "additions": 0,
            "deletions": 0,
            "body": "### Overview\r\n[Braina](https://www.brainasoft.com/braina/) supports Ollama natively on Windows. It automatically synchronizes with Ollama model lists, and allows users to use advanced features such as Voice (Both Speech to Text and Text to Speech), Web Search, File and Webpage attachments, Custom Prompts etc.\r\n\r\n### Screenshots\r\n**Main Chat UI**\r\n\r\n<img width=\"657\" alt=\"LLM_Movies_results_with_web_access_Ollama_UI\" src=\"https://github.com/user-attachments/assets/53e22fab-3977-4aa0-bdd3-9834de97f2d1\">\r\n\r\n**Model Management**\r\n\r\n<img width=\"515\" alt=\"manage-language-models\" src=\"https://github.com/user-attachments/assets/059f48f5-eeaf-4dd8-9b56-2c096be9859b\">\r\n\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix context in /api/generate grows too much (#5980).",
            "url": "https://github.com/ollama/ollama/pull/6115",
            "state": "MERGED",
            "createdAt": "2024-08-01T08:47:37Z",
            "mergedAt": "2024-08-01T22:13:59Z",
            "closedAt": "2024-08-01T22:13:59Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 7,
            "body": "This PR fixes [Context in /api/generate response grows too big. #5980\r\n](https://github.com/ollama/ollama/issues/5980)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "llama: Runtime selection of new or old runners",
            "url": "https://github.com/ollama/ollama/pull/6123",
            "state": "MERGED",
            "createdAt": "2024-08-01T16:03:33Z",
            "mergedAt": "2024-08-01T22:51:51Z",
            "closedAt": "2024-08-01T22:51:51Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 8
            },
            "additions": 481,
            "deletions": 176,
            "body": "This change pulls out the ~minimal set of changes from #5287 to be able to build locally and run either the C++ or Go runner\r\n\r\nCarries #6122\r\n\r\nThis wont be ready to merge to main until other build rigging changes from the other PR are factored in.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "enable gofmt/gofumpt/goimports/tenv",
            "url": "https://github.com/ollama/ollama/pull/6128",
            "state": "MERGED",
            "createdAt": "2024-08-01T21:52:39Z",
            "mergedAt": "2024-08-02T21:58:40Z",
            "closedAt": "2024-08-02T21:58:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 68
            },
            "additions": 199,
            "deletions": 149,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix OpenAI models/{model} URL",
            "url": "https://github.com/ollama/ollama/pull/6132",
            "state": "MERGED",
            "createdAt": "2024-08-01T23:05:20Z",
            "mergedAt": "2024-08-01T23:31:47Z",
            "closedAt": "2024-08-01T23:31:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "I added a note about this after the PR was merged @royjhan.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Adjust arm cuda repo paths",
            "url": "https://github.com/ollama/ollama/pull/6133",
            "state": "MERGED",
            "createdAt": "2024-08-02T00:24:03Z",
            "mergedAt": "2024-08-08T19:33:35Z",
            "closedAt": "2024-08-08T19:33:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 6,
            "body": "Ubuntu distros fail to install cuda drivers since aarch64 isn't valid\r\n\r\nhttps://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/\r\n\r\nFixes #5797 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix crash on startup when trying to clean up unused files (#5840)",
            "url": "https://github.com/ollama/ollama/pull/6145",
            "state": "MERGED",
            "createdAt": "2024-08-02T21:23:34Z",
            "mergedAt": "2024-08-07T18:24:15Z",
            "closedAt": "2024-08-07T18:24:15Z",
            "reviews": {
                "totalCount": 44
            },
            "files": {
                "totalCount": 4
            },
            "additions": 64,
            "deletions": 37,
            "body": "Improve validation and error handling of manifest files in the event of corruption. This prevents nil pointer errors and possible unintended deletion of data.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "use testing tempdirs",
            "url": "https://github.com/ollama/ollama/pull/6146",
            "state": "MERGED",
            "createdAt": "2024-08-02T22:57:17Z",
            "mergedAt": "2024-08-05T20:00:05Z",
            "closedAt": "2024-08-05T20:00:05Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "setting OLLAMA_MODELS in createRequest ensures the request is handled in a temporary directory",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Reference ollama integration with Harbor",
            "url": "https://github.com/ollama/ollama/pull/6147",
            "state": "MERGED",
            "createdAt": "2024-08-03T00:02:10Z",
            "mergedAt": "2024-08-03T00:03:46Z",
            "closedAt": "2024-08-03T00:03:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Harbor runs Ollama as a default LLM backend:\r\n```bash\r\n# Everything is dockerized, User doesn't have to install \r\n# ollama on their host\r\nharbor up ollama\r\n\r\n# Ollama CLI/API is available as is\r\nharbor ollama list\r\ncurl $(harbor url ollama)/api/ps\r\n```\r\n\r\nOn Harbor's end, Ollama is integrated with the supported frontends (currently Open WebUI, LibreChat, Hollama, BionicGPT)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add Gemma 2 2b",
            "url": "https://github.com/ollama/ollama/pull/6151",
            "state": "MERGED",
            "createdAt": "2024-08-03T11:57:24Z",
            "mergedAt": "2024-08-04T14:58:39Z",
            "closedAt": "2024-08-04T14:58:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Disable paging for journalctl",
            "url": "https://github.com/ollama/ollama/pull/6154",
            "state": "MERGED",
            "createdAt": "2024-08-03T17:33:43Z",
            "mergedAt": "2024-08-05T04:10:53Z",
            "closedAt": "2024-08-05T04:10:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Users using `journalctl` to get logs for issue logging sometimes don't realize that paging is causing information to be missed.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "line feed",
            "url": "https://github.com/ollama/ollama/pull/6167",
            "state": "MERGED",
            "createdAt": "2024-08-05T00:27:46Z",
            "mergedAt": "2024-08-05T07:06:28Z",
            "closedAt": "2024-08-05T07:06:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 83,
            "deletions": 83,
            "body": "use line feed instead of carriage return line feed for all files",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "removeall to remove non-empty temp dirs",
            "url": "https://github.com/ollama/ollama/pull/6171",
            "state": "MERGED",
            "createdAt": "2024-08-05T07:05:59Z",
            "mergedAt": "2024-08-09T22:47:13Z",
            "closedAt": "2024-08-09T22:47:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 26,
            "deletions": 23,
            "body": "`os.Remove()` does not remove non-empty directories so it'll error with `directory not empty`. instead, remove the expected content (`ollama.pid` and `runners`) individually, then remove the parent directory. remove the content explicitly so as to not accidentally remove things ollama does not own",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Catch one more error log",
            "url": "https://github.com/ollama/ollama/pull/6182",
            "state": "MERGED",
            "createdAt": "2024-08-05T16:30:05Z",
            "mergedAt": "2024-08-08T19:33:17Z",
            "closedAt": "2024-08-08T19:33:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Example from recent user reported error log on a fine tune that didn't load correctly\r\n```\r\nC:\\a\\ollama\\ollama\\llm\\llama.cpp\\src\\llama.cpp:5511: GGML_ASSERT(vocab.id_to_token.size() == vocab.token_to_id.size()) failed\r\ntime=2024-08-05T10:44:06.625+02:00 level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server not responding\"\r\ntime=2024-08-05T10:44:08.717+02:00 level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-08-05T10:44:09.223+02:00 level=ERROR source=sched.go:451 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000409\"\r\n```\r\n\r\nThis will help bubble up the underlying error instead of the ~useless exit status.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Implement linux NUMA detection",
            "url": "https://github.com/ollama/ollama/pull/6186",
            "state": "MERGED",
            "createdAt": "2024-08-05T20:01:08Z",
            "mergedAt": "2024-08-05T22:20:07Z",
            "closedAt": "2024-08-05T22:20:07Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 29,
            "deletions": 4,
            "body": "If the system has multiple numa nodes, enable numa support in llama.cpp If we detect `numactl` in the path, use that, else use the basic \"distribute\" mode.\r\n\r\nThis also removes the `use_numa` setting as the bool type is no longer useful given this parameter now requires a mode.\r\n\r\nFixes #6093 \r\nFixes #2496 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Sort Batch Embed Results",
            "url": "https://github.com/ollama/ollama/pull/6189",
            "state": "MERGED",
            "createdAt": "2024-08-05T22:58:11Z",
            "mergedAt": "2024-08-05T23:55:34Z",
            "closedAt": "2024-08-05T23:55:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 0,
            "body": "batch embed returns results out of order, fix by sorting results by task id\r\n\r\nResolves #6187",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix concurrency test",
            "url": "https://github.com/ollama/ollama/pull/6190",
            "state": "MERGED",
            "createdAt": "2024-08-05T23:36:34Z",
            "mergedAt": "2024-08-05T23:45:50Z",
            "closedAt": "2024-08-05T23:45:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 17,
            "deletions": 18,
            "body": "errors were hidden by `integration` build tag",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix Invalid Option Name Warning Bug",
            "url": "https://github.com/ollama/ollama/pull/6202",
            "state": "MERGED",
            "createdAt": "2024-08-06T14:12:49Z",
            "mergedAt": "2024-08-06T18:37:16Z",
            "closedAt": "2024-08-06T18:37:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Fixed invalid option provided not displaying the invalid option name problem.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Ensure sparse files on windows during download",
            "url": "https://github.com/ollama/ollama/pull/6207",
            "state": "MERGED",
            "createdAt": "2024-08-06T17:47:49Z",
            "mergedAt": "2024-08-06T18:06:06Z",
            "closedAt": "2024-08-06T18:06:06Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 28,
            "deletions": 0,
            "body": "The file.Truncate call on windows will write the whole file unless you set the sparse flag, leading to heavy I/O at the beginning of download.  This should improve our I/O behavior on windows and put less stress on the users disk.\r\n\r\nFixes #5852",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update llama.cpp submodule to `1e6f6554`",
            "url": "https://github.com/ollama/ollama/pull/6208",
            "state": "MERGED",
            "createdAt": "2024-08-06T18:35:14Z",
            "mergedAt": "2024-08-06T19:11:45Z",
            "closedAt": "2024-08-06T19:11:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 25,
            "deletions": 45,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llm: reserve required number of slots for embeddings",
            "url": "https://github.com/ollama/ollama/pull/6219",
            "state": "MERGED",
            "createdAt": "2024-08-07T02:50:14Z",
            "mergedAt": "2024-08-07T03:20:49Z",
            "closedAt": "2024-08-07T03:20:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 7,
            "body": "Fixes https://github.com/ollama/ollama/issues/6217",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: parallelize embeddings in API web handler instead of in subprocess runner",
            "url": "https://github.com/ollama/ollama/pull/6220",
            "state": "MERGED",
            "createdAt": "2024-08-07T03:20:36Z",
            "mergedAt": "2024-08-11T18:57:11Z",
            "closedAt": "2024-08-11T18:57:11Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 4
            },
            "additions": 53,
            "deletions": 71,
            "body": "This moves the \"batching\" for the `/api/embed` endpoint to the Go server instead of in the runner, helping to keep the interface in `llm` simpler",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Set *.png and *.ico to be treated as binary files.",
            "url": "https://github.com/ollama/ollama/pull/6235",
            "state": "MERGED",
            "createdAt": "2024-08-07T16:21:44Z",
            "mergedAt": "2024-08-10T00:06:30Z",
            "closedAt": "2024-08-10T00:06:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "The change b732beba6 makes all files text files and sets lf as eol. This will automatically change all files to have lf if they are touched by git (e.g. via git status). This change cannot be stashed and makes it hard to work with the repo (rebase and checkout don't really work). See also #6183.\r\n\r\nHere, we set the offending files (*.png and *.ico, but that might be more in the future) to be treated as binary files and not be changed by git.",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Store layers inside manifests consistently as values.",
            "url": "https://github.com/ollama/ollama/pull/6247",
            "state": "MERGED",
            "createdAt": "2024-08-08T00:31:42Z",
            "mergedAt": "2024-08-08T17:46:43Z",
            "closedAt": "2024-08-08T17:46:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 36,
            "deletions": 36,
            "body": "This consistently uses layers as values (instead of pointers) inside of manifest after the change to make the config be passed by value. The interface is clearer and it reduces the need dereference and take address of in some places.\r\n\r\nI'm not sure if the changes in layer.go are considered canonical Go, so I would appreciate some feedback there. In particular, the New functions return a layer by reference but the receiver functions take a pointer.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "server/download.go: Fix a typo in log",
            "url": "https://github.com/ollama/ollama/pull/6258",
            "state": "MERGED",
            "createdAt": "2024-08-08T12:29:37Z",
            "mergedAt": "2024-08-10T00:19:48Z",
            "closedAt": "2024-08-10T00:19:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama3.1 memory",
            "url": "https://github.com/ollama/ollama/pull/6260",
            "state": "MERGED",
            "createdAt": "2024-08-08T18:19:39Z",
            "mergedAt": "2024-09-05T20:22:08Z",
            "closedAt": "2024-09-05T20:22:08Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 2,
            "body": "llama3.1 has a larger vocab which on smaller context sizes increase the graph requirements",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Parse cpuinfo and set default threads",
            "url": "https://github.com/ollama/ollama/pull/6264",
            "state": "MERGED",
            "createdAt": "2024-08-08T21:51:23Z",
            "mergedAt": "2024-10-15T18:36:08Z",
            "closedAt": "2024-10-15T18:36:08Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 7
            },
            "additions": 408,
            "deletions": 24,
            "body": "Set the default thread count to the number of performance cores detected on the system.  Without this change, the new Go server winds up picking `runtime.NumCPU` from Go, which equates to logical processors, and that results in thrashing on hyperthreading CPUs and poor CPU inference speed.\r\n\r\nWe need to reduce down to just the number of cores in a single socket given current limitations in the C++ code.\r\n\r\nFixes #5554 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Harden intel boostrap for nil pointers",
            "url": "https://github.com/ollama/ollama/pull/6290",
            "state": "MERGED",
            "createdAt": "2024-08-09T18:34:16Z",
            "mergedAt": "2024-08-09T19:14:43Z",
            "closedAt": "2024-08-09T19:14:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 32,
            "deletions": 29,
            "body": "If the user enables intel GPU discovery, but the library doesn't initialize, we'd crash over a nil pointer.\r\n\r\nFixes #6284 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Don't hard fail on sparse setup error",
            "url": "https://github.com/ollama/ollama/pull/6291",
            "state": "MERGED",
            "createdAt": "2024-08-09T18:58:48Z",
            "mergedAt": "2024-08-09T19:30:25Z",
            "closedAt": "2024-08-09T19:30:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 5,
            "deletions": 7,
            "body": "It seems this can fail in some casees, but proceed with the download anyway.\r\n\r\nI haven't reproduced the users failure, but based on the log message and the recent regression, this seems highly likely to be the cause.\r\n\r\nFixes #6263 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add  integration obook-summary",
            "url": "https://github.com/ollama/ollama/pull/6305",
            "state": "MERGED",
            "createdAt": "2024-08-11T01:38:11Z",
            "mergedAt": "2024-08-11T01:43:09Z",
            "closedAt": "2024-08-11T01:43:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "an app which automatically splits e-books by section and chunks those sections one at a time. saved to csv, then you can also ask the same question of the entire book, one chunk at a time, in addition to the bulleted notes core-functionality.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert \"server: speed up single gguf creates\"",
            "url": "https://github.com/ollama/ollama/pull/6323",
            "state": "MERGED",
            "createdAt": "2024-08-12T16:53:01Z",
            "mergedAt": "2024-08-12T16:57:51Z",
            "closedAt": "2024-08-12T16:57:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 96,
            "body": "Reverts ollama/ollama#5898",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cmd: speed up gguf creates",
            "url": "https://github.com/ollama/ollama/pull/6324",
            "state": "MERGED",
            "createdAt": "2024-08-12T17:25:42Z",
            "mergedAt": "2024-08-12T18:46:09Z",
            "closedAt": "2024-08-12T18:46:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 96,
            "deletions": 3,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Load Embedding Model on Empty Input",
            "url": "https://github.com/ollama/ollama/pull/6325",
            "state": "MERGED",
            "createdAt": "2024-08-12T18:23:22Z",
            "mergedAt": "2024-08-13T17:19:56Z",
            "closedAt": "2024-08-13T17:19:56Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 9,
            "deletions": 77,
            "body": "Resolves #6295 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "convert safetensor adapters into GGUF",
            "url": "https://github.com/ollama/ollama/pull/6327",
            "state": "MERGED",
            "createdAt": "2024-08-12T21:24:33Z",
            "mergedAt": "2024-08-23T18:29:56Z",
            "closedAt": "2024-08-23T18:29:56Z",
            "reviews": {
                "totalCount": 13
            },
            "files": {
                "totalCount": 16
            },
            "additions": 697,
            "deletions": 101,
            "body": "This change converts a Safetensors based LoRA into GGUF and ties it w/ a base model. Only llama2/llama3/mistral/gemma2 will work initially. You can create the Modelfile to look like:\r\n\r\n```\r\nFROM llama3\r\nADAPTER /path/to/my/safetensor/adapter/directory\r\n```\r\n\r\nI'll add in some tests, but wanted to get this out so people could try it out.\r\n\r\nReplaces #5524\r\n",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: initial vision support for `runner`",
            "url": "https://github.com/ollama/ollama/pull/6331",
            "state": "CLOSED",
            "createdAt": "2024-08-13T05:33:27Z",
            "mergedAt": null,
            "closedAt": "2024-09-21T00:09:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 341,
            "deletions": 166,
            "body": "This adds vision support for the `runner` including parallel support. It still need simplification/cleanup - the \"main\" `for` loop in `run()` is more complicated now that we process both tokens and embeddings. Also, ideally instead of using dedicated `Llava` methods we get the embedding directly from the vision model and incorporate them into the batches we build up in `run`\r\n\r\n```\r\n curl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"[INST] What is in this image? [img-0] [/INST]\", \"images\": [\"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\"]}' http://localhost:8080/completion\r\n{\"content\":\" The\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" image\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" shows\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" a\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" cute\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\",\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" cart\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\"oon\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" animal\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" that\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" appears\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" to\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" be\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" a\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" pig\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" or\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" similar\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" creature\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\".\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" It\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" has\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" a\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" simple\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\",\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" st\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\"yl\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\"ized\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" design\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" with\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" big\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" eyes\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" and\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" a\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" small\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" body\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\".\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" The\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" character\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" seems\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" to\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" be\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" animated\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\",\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" as\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" indicated\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" by\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" the\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" direction\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" of\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" its\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" head\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" and\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" arms\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\".\",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\" \",\"stop\":false,\"timings\":{\"predicted_n\":0,\"predicted_ms\":0,\"prompt_n\":0,\"prompt_ms\":0}}\r\n{\"content\":\"\",\"stop\":true,\"timings\":{\"predicted_n\":56,\"predicted_ms\":1112,\"prompt_n\":37,\"prompt_ms\":1105}}\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add new chat app LLMChat.co",
            "url": "https://github.com/ollama/ollama/pull/6340",
            "state": "CLOSED",
            "createdAt": "2024-08-13T16:55:56Z",
            "mergedAt": null,
            "closedAt": "2024-09-23T13:40:19Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 48
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Go back to a pinned Go version",
            "url": "https://github.com/ollama/ollama/pull/6343",
            "state": "MERGED",
            "createdAt": "2024-08-13T18:45:14Z",
            "mergedAt": "2024-08-13T18:53:50Z",
            "closedAt": "2024-08-13T18:53:50Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 11,
            "deletions": 11,
            "body": "Go version 1.22.6 is triggering AV false positives, so go back to 1.22.5",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update chatml template format to latest",
            "url": "https://github.com/ollama/ollama/pull/6344",
            "state": "MERGED",
            "createdAt": "2024-08-13T19:23:24Z",
            "mergedAt": "2024-08-13T23:39:18Z",
            "closedAt": "2024-08-13T23:39:18Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 6,
            "body": "The chatml template format appears to be out of date in this template doc and the default chatml template we apply on conversion. Update these templates to match the most up to date version of this template.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update openai.md to remove extra checkbox for vision",
            "url": "https://github.com/ollama/ollama/pull/6345",
            "state": "MERGED",
            "createdAt": "2024-08-13T20:33:50Z",
            "mergedAt": "2024-08-13T20:36:05Z",
            "closedAt": "2024-08-13T20:36:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "The list has Vision twice- once checked, the other unchecked. I'm removing the second one, optimistically, but I haven't verified a vision model works yet. So maybe the first one should be removed instead?",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "lint",
            "url": "https://github.com/ollama/ollama/pull/6346",
            "state": "MERGED",
            "createdAt": "2024-08-13T21:36:59Z",
            "mergedAt": "2024-08-13T21:58:35Z",
            "closedAt": "2024-08-13T21:58:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 70,
            "deletions": 75,
            "body": "- fixes printf: non-constant format string in call to fmt.Printf\r\n- fixes SA1032: arguments have the wrong order\r\n- disables testifylint",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: reduce max connections used in download",
            "url": "https://github.com/ollama/ollama/pull/6347",
            "state": "MERGED",
            "createdAt": "2024-08-13T22:16:23Z",
            "mergedAt": "2024-08-13T23:47:35Z",
            "closedAt": "2024-08-13T23:47:35Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The previous value of 64 was WAY too high and unnecessary. It reached diminishing returns and blew past it. This is a more reasonable number for _most_ normal cases. For users on cloud servers with excellent network quality, this will keep screaming for them, without hitting our CDN limits. For users with relatively poor network quality, this will keep them from saturating their network and causing other issues.",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "fix: noprune on pull",
            "url": "https://github.com/ollama/ollama/pull/6363",
            "state": "MERGED",
            "createdAt": "2024-08-14T21:40:01Z",
            "mergedAt": "2024-08-15T19:20:38Z",
            "closedAt": "2024-08-15T19:20:38Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 2
            },
            "additions": 31,
            "deletions": 68,
            "body": "`OLLAMA_NOPRUNE` is ignored on pull requests. switch to `envconfig.NoPrune()`; `noprune` was check but never set\r\n\r\nignore any manifest that's not actually a manifest. this can be files such as `.DS_Store` on macOS\r\n\r\nresolves #6333",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add confichat to README.md",
            "url": "https://github.com/ollama/ollama/pull/6378",
            "state": "MERGED",
            "createdAt": "2024-08-15T20:16:19Z",
            "mergedAt": "2024-09-04T21:26:02Z",
            "closedAt": "2024-09-04T21:26:02Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 1,
            "body": "- Added ConfiChat to Community Integration > Web & Desktop\r\n- Added ConfiChat to Community Integration > Mobile",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Separate ARM64 CPU builds from x64 CPU builds and use Clang instead",
            "url": "https://github.com/ollama/ollama/pull/6379",
            "state": "CLOSED",
            "createdAt": "2024-08-15T21:35:53Z",
            "mergedAt": null,
            "closedAt": "2024-08-24T08:07:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 74,
            "deletions": 4,
            "body": "This enhances https://github.com/ollama/ollama/pull/3972 by making builds that are much, much faster:\r\n- Separate the ARM64 CPU builds from x64.\r\n- Make the Low Common Denominator CPU, for ARM64, to be ARMv8.2-A and use the matrix multiplication kernels tailored for NEON and ARMv8.2-A (See https://justine.lol/matmul/).\r\n- Create a separate runner built for ARMv8.7-A (e.g.: Snapdragon Plus/Elite X) that uses the MATMUL instructions.\r\n\r\nThe build instructions for ARM64 stay the same as in https://github.com/ollama/ollama/pull/5268. Clang was already required to build the llama.cpp static_library anyway so switching the runners to be built with Clang (on ARM64) doesn't add new requirements.",
            "participants": {
                "totalCount": 1
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: Add tooltip to system tray icon",
            "url": "https://github.com/ollama/ollama/pull/6381",
            "state": "MERGED",
            "createdAt": "2024-08-15T22:00:34Z",
            "mergedAt": "2024-08-15T22:31:15Z",
            "closedAt": "2024-08-15T22:31:15Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 8,
            "deletions": 1,
            "body": "- Updated setIcon method to include tooltip text for the system tray icon.\r\n- Added NIF_TIP flag and set the tooltip text using UTF16 encoding.\r\n\r\nResolves: #6372",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "fix: chmod new layer to 0o644 when creating it",
            "url": "https://github.com/ollama/ollama/pull/6386",
            "state": "MERGED",
            "createdAt": "2024-08-16T03:44:05Z",
            "mergedAt": "2024-08-21T17:58:46Z",
            "closedAt": "2024-08-21T17:58:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "fix https://github.com/ollama/ollama/issues/6373",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "doc: fixed spelling error",
            "url": "https://github.com/ollama/ollama/pull/6391",
            "state": "MERGED",
            "createdAt": "2024-08-16T14:16:08Z",
            "mergedAt": "2024-09-04T13:42:33Z",
            "closedAt": "2024-09-04T13:42:33Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Changed \"dorrect\" to \"correct\".",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Make new tokenizer logic conditional",
            "url": "https://github.com/ollama/ollama/pull/6395",
            "state": "MERGED",
            "createdAt": "2024-08-16T20:30:36Z",
            "mergedAt": "2024-08-25T00:25:37Z",
            "closedAt": "2024-08-25T00:25:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 95,
            "deletions": 5,
            "body": "Only use the new cgo tokenizer/detokenizer if we're using the new runners",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Override numParallel in pickBestPartialFitByLibrary() only if unset.",
            "url": "https://github.com/ollama/ollama/pull/6402",
            "state": "MERGED",
            "createdAt": "2024-08-18T01:02:31Z",
            "mergedAt": "2024-08-19T18:07:22Z",
            "closedAt": "2024-08-19T18:07:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 1,
            "body": "pickBestPartialFitByLibrary() sets numParallel = 1, but doesn't adjust req.opts.NumCtx.  If OLLAMA_NUM_PARALLEL has been set, NumCtx = OLLAMA_NUM_PARALLEL * defaultParallel.  Unconditionally setting numParallel to 1 causes problems in needsReload()  - because NumCtx hasn't been reset,  ` !reflect.DeepEqual(optsExisting, optsNew)` always fails, causing the model to be reloaded for every request.\r\n\r\nIt's not clear why numParallel is forced to 1, testing indicates that models operate normally with a partial load and parallelism, so this PR changes the code to set numParallel and req.opts.NumCtx only if unset.\r\n\r\nFixes https://github.com/ollama/ollama/issues/6148\r\nFixes https://github.com/ollama/ollama/issues/6271",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: limit upload parts to 16",
            "url": "https://github.com/ollama/ollama/pull/6411",
            "state": "MERGED",
            "createdAt": "2024-08-19T04:52:54Z",
            "mergedAt": "2024-08-19T16:20:52Z",
            "closedAt": "2024-08-19T16:20:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "In similar vein as https://github.com/ollama/ollama/pull/6347, limit the number of upload connections to 16.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add gitlab.com/tozd/go/fun Go package",
            "url": "https://github.com/ollama/ollama/pull/6421",
            "state": "MERGED",
            "createdAt": "2024-08-19T11:15:06Z",
            "mergedAt": "2024-09-04T14:52:46Z",
            "closedAt": "2024-09-04T14:52:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "`gitlab.com/tozd/go/fun` is a Go package which provides high-level abstraction to define functions with code (the usual way), data (providing examples of inputs and expected outputs which are then used with an AI model), or natural language description. It is the simplest but powerful way to use large language models (LLMs) in Go.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Fix overlapping artifact name on CI",
            "url": "https://github.com/ollama/ollama/pull/6424",
            "state": "MERGED",
            "createdAt": "2024-08-19T19:08:23Z",
            "mergedAt": "2024-08-19T19:11:58Z",
            "closedAt": "2024-08-19T19:11:58Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 2,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI: handle directories during checksum",
            "url": "https://github.com/ollama/ollama/pull/6427",
            "state": "MERGED",
            "createdAt": "2024-08-19T20:45:58Z",
            "mergedAt": "2024-08-19T20:48:45Z",
            "closedAt": "2024-08-19T20:48:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Runner.go Context Window Shifting",
            "url": "https://github.com/ollama/ollama/pull/6428",
            "state": "MERGED",
            "createdAt": "2024-08-19T22:11:46Z",
            "mergedAt": "2024-08-22T17:18:31Z",
            "closedAt": "2024-08-22T17:18:31Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 6
            },
            "additions": 203,
            "deletions": 41,
            "body": "This series implements context window shifting for the new go server runner. It also fixes a number of issues in the related code.\r\n\r\nMy intention is to start adding tests for some of the issues encountered here but I wanted to start getting reviews on this code in the meantime.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Linux Doc cosmetic fixes.",
            "url": "https://github.com/ollama/ollama/pull/6430",
            "state": "MERGED",
            "createdAt": "2024-08-19T22:31:20Z",
            "mergedAt": "2024-09-04T18:45:09Z",
            "closedAt": "2024-09-04T18:45:09Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 3,
            "body": "minor doc update for Linux Users.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 4
            }
        }
    },
    {
        "node": {
            "title": "Split rocm back out of bundle",
            "url": "https://github.com/ollama/ollama/pull/6432",
            "state": "MERGED",
            "createdAt": "2024-08-20T00:12:54Z",
            "mergedAt": "2024-08-20T14:26:38Z",
            "closedAt": "2024-08-20T14:26:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 5
            },
            "additions": 16,
            "deletions": 3,
            "body": "We're [over budget for github's maximum release artifact size](https://github.com/ollama/ollama/actions/runs/10461795539/job/28973022210) with rocm + 2 cuda versions.  This splits rocm back out as a discrete artifact, but keeps the layout so it can be extracted into the same location as the main bundle.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update manual instructions with discrete ROCm bundle",
            "url": "https://github.com/ollama/ollama/pull/6445",
            "state": "MERGED",
            "createdAt": "2024-08-20T15:56:57Z",
            "mergedAt": "2024-08-27T20:42:28Z",
            "closedAt": "2024-08-27T20:42:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "WSL 2 is not an upgrade, it's a different type",
            "url": "https://github.com/ollama/ollama/pull/6450",
            "state": "MERGED",
            "createdAt": "2024-08-20T20:59:47Z",
            "mergedAt": "2024-09-04T13:34:53Z",
            "closedAt": "2024-09-04T13:34:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Align cmake define for cuda no peer copy",
            "url": "https://github.com/ollama/ollama/pull/6455",
            "state": "MERGED",
            "createdAt": "2024-08-21T22:09:03Z",
            "mergedAt": "2024-08-23T18:20:40Z",
            "closedAt": "2024-08-23T18:20:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 2,
            "body": "This feel out of sync on recent updates to llama.cpp.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Adding 'Ollama App' as community integrations",
            "url": "https://github.com/ollama/ollama/pull/6465",
            "state": "MERGED",
            "createdAt": "2024-08-22T17:08:06Z",
            "mergedAt": "2024-10-15T16:57:33Z",
            "closedAt": "2024-10-15T16:57:33Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "A little while back, I created an app to interact with Ollama from your phone called 'Ollama App'. I know, very creative, but the name stuck. I'd be very happy if it could be added to the integrations list.\r\n\r\nAt the moment, I only recommend using the mobile version, it's the most stable one, and the only one with a stable build, but I am cooking around a few other platforms (Flutter really is fabulous with multiplatform), like Desktop and maybe Web. When I'd decide to remove the experimental tag, where should I put them? Should I copy the entry and put it into multiple categories?\r\n\r\nThank you",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix embeddings memory corruption",
            "url": "https://github.com/ollama/ollama/pull/6467",
            "state": "MERGED",
            "createdAt": "2024-08-22T19:40:07Z",
            "mergedAt": "2024-08-22T21:51:43Z",
            "closedAt": "2024-08-22T21:51:43Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 16,
            "deletions": 65,
            "body": "The patch was leading to a buffer overrun corruption.  Once removed though, parallism in server.cpp lead to hitting an assert due to slot/seq IDs being >= token count.  To work around this, only use slot 0 for embeddings.\r\n\r\nFixes #6435 \r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Ensure driver version set before variant",
            "url": "https://github.com/ollama/ollama/pull/6480",
            "state": "MERGED",
            "createdAt": "2024-08-23T16:45:05Z",
            "mergedAt": "2024-08-23T18:21:12Z",
            "closedAt": "2024-08-23T18:21:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "During rebasing, the ordering was inverted causing the cuda version selection logic to break.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "passthrough OLLAMA_HOST path to client",
            "url": "https://github.com/ollama/ollama/pull/6482",
            "state": "MERGED",
            "createdAt": "2024-08-23T20:21:56Z",
            "mergedAt": "2024-08-30T16:40:34Z",
            "closedAt": "2024-08-30T16:40:34Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 27,
            "deletions": 30,
            "body": "this allows users to use ollama behind a proxy with a non-root path",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "gpu: Group GPU Library sets by variant",
            "url": "https://github.com/ollama/ollama/pull/6483",
            "state": "MERGED",
            "createdAt": "2024-08-23T21:06:35Z",
            "mergedAt": "2024-08-23T22:11:56Z",
            "closedAt": "2024-08-23T22:11:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 26,
            "deletions": 1,
            "body": "The recent cuda variant changes uncovered a bug in ByLibrary which failed to group by common variant for GPU types.\r\n\r\nNew unit test fails without the 1 line fix, and passes with.\r\n\r\nFixes #6479 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Only enable numa on CPUs",
            "url": "https://github.com/ollama/ollama/pull/6484",
            "state": "MERGED",
            "createdAt": "2024-08-24T00:06:16Z",
            "mergedAt": "2024-08-25T00:24:50Z",
            "closedAt": "2024-08-25T00:24:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The numa flag may be having a performance impact on multi-socket systems with GPU loads",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Optimize container images for startup",
            "url": "https://github.com/ollama/ollama/pull/6485",
            "state": "CLOSED",
            "createdAt": "2024-08-24T00:58:42Z",
            "mergedAt": null,
            "closedAt": "2024-08-28T21:15:07Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 22
            },
            "additions": 634,
            "deletions": 504,
            "body": "This change adjusts how to handle runner payloads to support container builds where we keep them extracted in the filesystem. This makes it easier to optimize the cpu/cuda vs cpu/rocm images for size, and should result in faster startup times for container images.\r\n\r\nLooks like container startup time is down to ~100ms on a warm system.\r\n\r\n~~The ROCm image is still massive due to the base layer containing compilers and other tools, so I'll see if I can find a way to try to slim it down a bit more, but that could come in a follow up PR.~~\r\n\r\nROCm image updated to use a base ubuntu image and just use our libraries.  The official images and packages pull in compilers as dependencies so this seems to be the optimal lean setup.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Detect running in a container",
            "url": "https://github.com/ollama/ollama/pull/6495",
            "state": "MERGED",
            "createdAt": "2024-08-25T00:01:39Z",
            "mergedAt": "2024-09-05T20:24:51Z",
            "closedAt": "2024-09-05T20:24:51Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: add OllamaFarm to README.md",
            "url": "https://github.com/ollama/ollama/pull/6508",
            "state": "MERGED",
            "createdAt": "2024-08-26T03:34:28Z",
            "mergedAt": "2024-09-02T20:05:36Z",
            "closedAt": "2024-09-02T20:05:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "OllamaFarm is a Go library that builds on the native Ollama Go API with automated tracking of the heartbeat and model availability of multiple Ollama servers to provide increased reliability and failover access to your Ollama models. This is especially helpful for protection from transient issues with networks or server hardware.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Implicit openai model parameter multiplication disabled",
            "url": "https://github.com/ollama/ollama/pull/6514",
            "state": "MERGED",
            "createdAt": "2024-08-26T10:40:00Z",
            "mergedAt": "2024-09-07T00:45:45Z",
            "closedAt": "2024-09-07T00:45:45Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 4,
            "deletions": 4,
            "body": "Current state of openai.go setup makes absolutely valid openai config to be broken. This happens because of implicit doubling the config numbers performed in it.\r\n\r\nI see the idea of making OpenAI API endpoint compatible with native ollama endpoint, but I think it've made wrong, as again, it leads to completely valid OpenAI config makes model goes wild.\r\n\r\nThus user is unable to use the same config for openai and ollama without modifications outside of model's field.\r\n\r\n```json\r\n{\r\n    \"url\": \"http://localhost:11434\",\r\n    \"token\": \"sk-your-token\",\r\n    \"status_hint\": [\r\n        \"name\",\r\n        \"prompt_mode\",\r\n        \"chat_model\"\r\n    ],\r\n    \"assistants\": [\r\n        {\r\n            \"name\": \"qwen2\",\r\n            \"chat_model\": \"qwen2:1.5b\",\r\n            \"assistant_role\": \"You are a senior python and sublime text 4 code assistant\",\r\n            \"prompt_mode\": \"panel\",\r\n            \"temperature\": 1, // makes model go insane, coz the temperature is 2 on ollama's side.\r\n            \"max_tokens\": 1048,\r\n            \"top_p\": 1,\r\n            \"frequency_penalty\": 0, // doubles as well\r\n            \"presence_penalty\": 0, // doubles as well\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nCloses: #6492 \r\nAffects: https://github.com/yaroslavyaroslav/OpenAI-sublime-text/issues/57",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 6
            }
        }
    },
    {
        "node": {
            "title": "Go Server Fixes",
            "url": "https://github.com/ollama/ollama/pull/6521",
            "state": "MERGED",
            "createdAt": "2024-08-27T00:03:14Z",
            "mergedAt": "2024-08-27T17:49:12Z",
            "closedAt": "2024-08-27T17:49:12Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 2
            },
            "additions": 156,
            "deletions": 140,
            "body": "Fixes for the go server branch primarily around concurrency and resource allocation",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "detect chat template from configs that contain lists",
            "url": "https://github.com/ollama/ollama/pull/6522",
            "state": "MERGED",
            "createdAt": "2024-08-27T00:32:21Z",
            "mergedAt": "2024-08-28T18:04:18Z",
            "closedAt": "2024-08-28T18:04:18Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 225,
            "deletions": 5,
            "body": "models like [hermes3](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B/blob/main/tokenizer_config.json#L2053) have a list of chat templates\r\n\r\n```json\r\n  \"chat_template\": [\r\n    {\r\n      \"name\": \"default\",\r\n      \"template\": \"{{bos_token}}{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\r\n    },\r\n    {\r\n      \"name\": \"tool_use\",\r\n      \"template\": \"{%- macro json_to_python_type(json_spec) %}\\n{%- set basic_type_map = {\\n    \\\"string\\\": \\\"str\\\",\\n    \\\"number\\\": \\\"float\\\",\\n    \\\"integer\\\": \\\"int\\\",\\n    \\\"boolean\\\": \\\"bool\\\"\\n} %}\\n\\n{%- if basic_type_map[json_spec.type] is defined %}\\n    {{- basic_type_map[json_spec.type] }}\\n{%- elif json_spec.type == \\\"array\\\" %}\\n    {{- \\\"list[\\\" +  json_to_python_type(json_spec|items) + \\\"]\\\"}}\\n{%- elif json_spec.type == \\\"object\\\" %}\\n    {%- if json_spec.additionalProperties is defined %}\\n        {{- \\\"dict[str, \\\" + json_to_python_type(json_spec.additionalProperties) + ']'}}\\n    {%- else %}\\n        {{- \\\"dict\\\" }}\\n    {%- endif %}\\n{%- elif json_spec.type is iterable %}\\n    {{- \\\"Union[\\\" }}\\n    {%- for t in json_spec.type %}\\n      {{- json_to_python_type({\\\"type\\\": t}) }}\\n      {%- if not loop.last %}\\n        {{- \\\",\\\" }} \\n    {%- endif %}\\n    {%- endfor %}\\n    {{- \\\"]\\\" }}\\n{%- else %}\\n    {{- \\\"Any\\\" }}\\n{%- endif %}\\n{%- endmacro %}\\n\\n\\n{{- bos_token }}\\n{{- \\\"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \\\" }}\\n{%- for tool in tools %}\\n    {%- if tool.function is defined %}\\n        {%- set tool = tool.function %}\\n    {%- endif %}\\n    {{- '{\\\"type\\\": \\\"function\\\", \\\"function\\\": ' }}\\n    {{- '{\\\"name\\\": \\\"' + tool.name + '\\\", ' }}\\n    {{- '\\\"description\\\": \\\"' + tool.name + '(' }}\\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\\n        {{- param_name + \\\": \\\" + json_to_python_type(param_fields) }}\\n        {%- if not loop.last %}\\n            {{- \\\", \\\" }}\\n        {%- endif %}\\n    {%- endfor %}\\n    {{- \\\")\\\" }}\\n    {%- if tool.return is defined %}\\n        {{- \\\" -> \\\" + json_to_python_type(tool.return) }}\\n    {%- endif %}\\n    {{- \\\" - \\\" + tool.description + \\\"\\\\n\\\\n\\\" }}\\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\\n        {%- if loop.first %}\\n            {{- \\\"    Args:\\\\n\\\" }}\\n        {%- endif %}\\n        {{- \\\"        \\\" + param_name + \\\"(\\\" + json_to_python_type(param_fields) + \\\"): \\\" + param_fields.description|trim }}\\n    {%- endfor %}\\n    {%- if tool.return is defined and tool.return.description is defined %}\\n        {{- \\\"\\\\n    Returns:\\\\n        \\\" + tool.return.description }}\\n    {%- endif %}\\n    {{- '\\\"' }}\\n    {{- ', \\\"parameters\\\": ' }}\\n    {%- if tool.parameters.properties | length == 0 %}\\n        {{- \\\"{}\\\" }}\\n    {%- else %}\\n        {{- tool.parameters|tojson }}\\n    {%- endif %}\\n    {{- \\\"}\\\" }}\\n    {%- if not loop.last %}\\n        {{- \\\"\\\\n\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{{- \\\" </tools>\\\" }}\\n{{- 'Use the following pydantic model json schema for each tool call you will make: {\\\"properties\\\": {\\\"name\\\": {\\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"arguments\\\": {\\\"title\\\": \\\"Arguments\\\", \\\"type\\\": \\\"object\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"arguments\\\"], \\\"title\\\": \\\"FunctionCall\\\", \\\"type\\\": \\\"object\\\"}}\\n' }}\\n{{- \\\"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n\\\" }}\\n{{- \\\"<tool_call>\\n\\\" }}\\n{{- '{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-dict>}\\n' }}\\n{{- '</tool_call><|im_end|>' }}\\n{%- for message in messages %}\\n    {%- if message.role == \\\"user\\\" or message.role == \\\"system\\\" or (message.role == \\\"assistant\\\" and message.tool_calls is not defined) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n    {%- for tool_call in message.tool_calls %}\\n       {{- '\\n<tool_call>\\n' }}           {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '{' }}\\n            {{- '\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\"}' }}\\n            {{- ', '}}\\n            {%- if tool_call.arguments is defined %}\\n                {{- '\\\"arguments\\\": ' }}\\n                {{- tool_call.arguments|tojson }}\\n            {%- endif %}\\n            {{- '\\\\n</tool_call>' }}\\n    {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if not message.name is defined %}\\n            {{- raise_exception(\\\"Tool response dicts require a 'name' key indicating the name of the called function!\\\") }}\\n        {%- endif %}\\n        {%- if loop.previtem and loop.previtem.role != \\\"tool\\\" %}\\n            {{- '<|im_start|>tool\\\\n' }}\\n        {%- endif %}\\n        {{- '<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {%- if not loop.last %}\\n            {{- '\\\\n</tool_response>\\\\n' }}\\n        {%- else %}\\n            {{- '\\\\n</tool_response>' }}\\n        {%- endif %}\\n        {%- if not loop.last and loop.nextitem.role != \\\"tool\\\" %}\\n            {{- '<|im_end|>' }}\\n        {%- elif loop.last %}\\n            {{- '<|im_end|>' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\"\r\n    }\r\n  ],\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llama: clean up sync",
            "url": "https://github.com/ollama/ollama/pull/6523",
            "state": "MERGED",
            "createdAt": "2024-08-27T02:03:02Z",
            "mergedAt": "2024-08-30T00:30:11Z",
            "closedAt": "2024-08-30T00:30:11Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 5
            },
            "additions": 1,
            "deletions": 459,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: clean up route names for consistency",
            "url": "https://github.com/ollama/ollama/pull/6524",
            "state": "MERGED",
            "createdAt": "2024-08-27T02:16:33Z",
            "mergedAt": "2024-08-27T02:36:12Z",
            "closedAt": "2024-08-27T02:36:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 57,
            "deletions": 57,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix import image width",
            "url": "https://github.com/ollama/ollama/pull/6528",
            "state": "MERGED",
            "createdAt": "2024-08-27T18:26:09Z",
            "mergedAt": "2024-08-27T21:19:48Z",
            "closedAt": "2024-08-27T21:19:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 3,
            "body": "This gives more reasonable output for the images:\r\n\r\n<img width=\"1183\" alt=\"Screenshot 2024-08-27 at 14 10 33\" src=\"https://github.com/user-attachments/assets/428cee86-ba62-4270-b308-2bacc07a1460\">\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: comment typo",
            "url": "https://github.com/ollama/ollama/pull/6530",
            "state": "MERGED",
            "createdAt": "2024-08-27T19:06:21Z",
            "mergedAt": "2024-08-27T20:28:30Z",
            "closedAt": "2024-08-27T20:28:30Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add safetensors to the modelfile docs",
            "url": "https://github.com/ollama/ollama/pull/6532",
            "state": "MERGED",
            "createdAt": "2024-08-27T21:41:53Z",
            "mergedAt": "2024-08-27T21:46:47Z",
            "closedAt": "2024-08-27T21:46:48Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 38,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "update templates to use messages",
            "url": "https://github.com/ollama/ollama/pull/6534",
            "state": "MERGED",
            "createdAt": "2024-08-27T22:35:15Z",
            "mergedAt": "2024-08-30T16:39:59Z",
            "closedAt": "2024-08-30T16:39:59Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 19
            },
            "additions": 144,
            "deletions": 83,
            "body": "messages support has been out in the wild for over a month at this point. it's time to update the built-in templates to use the messages format instead of the previous prompt/response format",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Move ollama executable out of bin dir",
            "url": "https://github.com/ollama/ollama/pull/6535",
            "state": "MERGED",
            "createdAt": "2024-08-27T22:39:06Z",
            "mergedAt": "2024-08-27T23:19:00Z",
            "closedAt": "2024-08-27T23:19:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 21,
            "deletions": 10,
            "body": "Draft until I can verify I didn't miss anything...",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Embeddings fixes",
            "url": "https://github.com/ollama/ollama/pull/6536",
            "state": "MERGED",
            "createdAt": "2024-08-27T23:35:34Z",
            "mergedAt": "2024-08-27T23:49:14Z",
            "closedAt": "2024-08-27T23:49:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 45,
            "deletions": 52,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "throw an error when encountering unsupport tensor sizes",
            "url": "https://github.com/ollama/ollama/pull/6538",
            "state": "MERGED",
            "createdAt": "2024-08-28T00:31:38Z",
            "mergedAt": "2024-08-28T00:54:04Z",
            "closedAt": "2024-08-28T00:54:04Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 106,
            "deletions": 0,
            "body": "The `bitsandbytes` package creates an 8 bit quantized version of a model which is unsupported by the llama.cpp back end. It does this by creating two tensors for each of the layers which look like:\r\n\r\n```\r\nmodel.layers.0.mlp.down_proj.weight dtype=I8 shape=[4096, 14336]\r\nmodel.layers.0.mlp.down_proj.weight_format dtype=U8 shape=[]\r\n```\r\n\r\nThis change just looks to see if there is a tensor with no shape and returns an error. Right now the server will panic instead.\r\n\r\nWe already support quantizing directly from the Safetensors model, so users should use the `--quantize` flag w/ `ollama create` instead of relying on `bitsandbytes`.\r\n\r\nFixes #6357\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix: validate modelpath",
            "url": "https://github.com/ollama/ollama/pull/6539",
            "state": "MERGED",
            "createdAt": "2024-08-28T00:58:32Z",
            "mergedAt": "2024-08-28T21:38:27Z",
            "closedAt": "2024-08-28T21:38:27Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 26,
            "deletions": 20,
            "body": "ensure model path resolves to a local path",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add llama3.1 chat template",
            "url": "https://github.com/ollama/ollama/pull/6545",
            "state": "MERGED",
            "createdAt": "2024-08-28T20:30:23Z",
            "mergedAt": "2024-08-28T21:03:20Z",
            "closedAt": "2024-08-28T21:03:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "This change adds the llama3.1 chat template do that it will be autodetected in `ollama create`.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(test): do not clobber models directory",
            "url": "https://github.com/ollama/ollama/pull/6546",
            "state": "MERGED",
            "createdAt": "2024-08-28T21:11:17Z",
            "mergedAt": "2024-08-28T22:37:47Z",
            "closedAt": "2024-08-28T22:37:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "these tests create these blobs in the user's blobs directory\r\n\r\n```\r\nsha256-4e06a933feaf7e21f9ea4442bec46e1a5c99f5931d06caecb5603653628ad9f1\r\nsha256-5738747671c0649396ed0b138cf8daed5bdb7140df5ee18d0a520295045feac3\r\nsha256-d45102d885e542798514de3c17c3d431732d90ff3bd5c743e411dcd93c7f8d1a\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Optimize container images for startup",
            "url": "https://github.com/ollama/ollama/pull/6547",
            "state": "MERGED",
            "createdAt": "2024-08-28T21:14:51Z",
            "mergedAt": "2024-09-12T19:10:30Z",
            "closedAt": "2024-09-12T19:10:30Z",
            "reviews": {
                "totalCount": 35
            },
            "files": {
                "totalCount": 32
            },
            "additions": 860,
            "deletions": 688,
            "body": "Replaces #6485 \r\n\r\nMove the payload handling logic to a discrete go module so we can start to lay the foundation to toggle between C++ and Go runner implementation at build time.\r\n\r\nThis change adjusts how to handle runner payloads to support container builds where we keep them extracted in the filesystem. This makes it easier to optimize the cpu/cuda vs cpu/rocm images for size, and should result in faster startup times for container images.\r\n\r\nLooks like container startup time is down to ~100ms on a warm system.\r\n\r\nROCm image updated to use a base ubuntu image and just use our libraries. The official images and packages pull in compilers as dependencies so this seems to be the optimal lean setup.\r\n\r\nFixes #6541 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update the openai docs to explain how to set the context size",
            "url": "https://github.com/ollama/ollama/pull/6548",
            "state": "MERGED",
            "createdAt": "2024-08-29T00:09:37Z",
            "mergedAt": "2024-08-29T00:11:46Z",
            "closedAt": "2024-08-29T00:11:46Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 25,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Go server command line options support",
            "url": "https://github.com/ollama/ollama/pull/6559",
            "state": "MERGED",
            "createdAt": "2024-08-29T16:58:25Z",
            "mergedAt": "2024-09-03T20:53:53Z",
            "closedAt": "2024-09-03T20:53:53Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 7
            },
            "additions": 97,
            "deletions": 737,
            "body": "Support for command line options for controlling resource usage such as mlock, mmap and GPU allocation. In addition, switches support to more recent LoRA formats.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "remove any unneeded build artifacts",
            "url": "https://github.com/ollama/ollama/pull/6562",
            "state": "MERGED",
            "createdAt": "2024-08-29T20:41:43Z",
            "mergedAt": "2024-08-30T16:40:50Z",
            "closedAt": "2024-08-30T16:40:50Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "metal lib is embedded so the file isn't necessary. this shaves off roughly 50KB",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: opt-in at build time",
            "url": "https://github.com/ollama/ollama/pull/6570",
            "state": "CLOSED",
            "createdAt": "2024-08-30T18:25:10Z",
            "mergedAt": null,
            "closedAt": "2024-09-15T18:19:16Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 299
            },
            "additions": 165277,
            "deletions": 898,
            "body": "This PR layers on #6547 for the new Go server.   \r\n\r\nUnfortunately the sizes are too large to try to make the opt-in strategy work at runtime (the linux tgz would significantly exceed the 2G github artifact size limit) so this makes the opt-in strategy work at build time.\r\n\r\nNotable refinements:\r\n- the ggml library is now moved out as a payload in the tar file to reduce the binary size, and the names are adjusted to avoid clashing between cuda v11, v12, and rocm.\r\n- The static cgo wiring for the main app is shifted over to the new llama package and the old `go generate` wiring for the static build is removed as no longer needed.\r\n- An initial foundation for requirement information is added to the runner so eventually we can pick compatible runners more easily\r\n- Use the CPU vector flags when compiling the GPU runners\r\n\r\nI'm still working through verifying all the build stages, so I'll mark it draft for now until I confirm they're all correct.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update documentation: Change .bin to .gguf in GGUF file and adapter examples",
            "url": "https://github.com/ollama/ollama/pull/6577",
            "state": "MERGED",
            "createdAt": "2024-08-31T13:39:52Z",
            "mergedAt": "2024-09-01T02:34:25Z",
            "closedAt": "2024-09-01T02:34:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 3,
            "body": "This pull request updates the documentation to reflect the change from GGML to GGUF format.\r\n\r\nChanges made:\r\n- In the \"Build from a GGUF file\" section, updated the example Modelfile to use the .gguf extension instead of .bin\r\n- Modified the explanatory text to refer to \"GGUF file\" instead of \"GGUF bin file\"\r\n- In the \"GGUF adapter\" section, updated the example Modelfile to use the .gguf extension for the adapter file\r\n\r\n(This is my first contribution to OSS, so I'm excited about it. Thank you.)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix(cmd): show info may have nil ModelInfo",
            "url": "https://github.com/ollama/ollama/pull/6579",
            "state": "MERGED",
            "createdAt": "2024-08-31T14:42:32Z",
            "mergedAt": "2024-09-01T04:12:17Z",
            "closedAt": "2024-09-01T04:12:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 5,
            "body": "add a `nil` check for `ModelInfo`\r\nfixes: #6578 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add findutils to base images",
            "url": "https://github.com/ollama/ollama/pull/6581",
            "state": "MERGED",
            "createdAt": "2024-08-31T17:32:32Z",
            "mergedAt": "2024-08-31T17:40:05Z",
            "closedAt": "2024-08-31T17:40:05Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "This caused missing internal files",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/6583",
            "state": "MERGED",
            "createdAt": "2024-09-01T03:53:54Z",
            "mergedAt": "2024-09-02T19:34:26Z",
            "closedAt": "2024-09-02T19:34:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "New links: \r\nGo-CREW and Ollamaclient for Golang",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update faq.md",
            "url": "https://github.com/ollama/ollama/pull/6587",
            "state": "MERGED",
            "createdAt": "2024-09-02T01:38:58Z",
            "mergedAt": "2024-09-02T19:31:29Z",
            "closedAt": "2024-09-02T19:31:29Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "I found many issues with the error \"could not connect to ollama app, is it running?\"\r\n\r\nIf not mention the ownership of the changed model path, many beginners and carfuless developer will failed to start the ollama service, which would cost plenty time on troubleshoting.\r\n\r\nMake it clear in FaQ will help developers to save their time.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Added the tool to generate 3D CAD models using Ollama",
            "url": "https://github.com/ollama/ollama/pull/6605",
            "state": "MERGED",
            "createdAt": "2024-09-03T09:19:49Z",
            "mergedAt": "2024-09-03T16:28:01Z",
            "closedAt": "2024-09-03T16:28:01Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Updated Ollama4j link",
            "url": "https://github.com/ollama/ollama/pull/6608",
            "state": "MERGED",
            "createdAt": "2024-09-03T17:12:20Z",
            "mergedAt": "2024-09-03T20:08:50Z",
            "closedAt": "2024-09-03T20:08:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Updated Ollama4j link and added link to Ollama4j Web UI tool.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llama: fix sync script ggml-metal_darwin_arm64.m filename",
            "url": "https://github.com/ollama/ollama/pull/6610",
            "state": "MERGED",
            "createdAt": "2024-09-03T17:50:25Z",
            "mergedAt": "2024-09-03T18:01:52Z",
            "closedAt": "2024-09-03T18:01:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Make stall duration timeout configurable",
            "url": "https://github.com/ollama/ollama/pull/6611",
            "state": "MERGED",
            "createdAt": "2024-09-03T18:04:25Z",
            "mergedAt": "2024-09-05T21:00:08Z",
            "closedAt": "2024-09-05T21:00:08Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 60,
            "deletions": 7,
            "body": "With the new very large parameter models, some users are willing to wait for a very long time for models to load.\r\n\r\nFixes #6031 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Log system memory at info",
            "url": "https://github.com/ollama/ollama/pull/6617",
            "state": "MERGED",
            "createdAt": "2024-09-03T21:43:13Z",
            "mergedAt": "2024-09-03T21:55:21Z",
            "closedAt": "2024-09-03T21:55:21Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "On systems with low system memory, we can hit allocation failures that are difficult to diagnose without debug logs.  This will make it easier to spot.\r\n\r\nResolves #6558",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: update llama.cpp commit to 8962422",
            "url": "https://github.com/ollama/ollama/pull/6618",
            "state": "MERGED",
            "createdAt": "2024-09-03T21:48:17Z",
            "mergedAt": "2024-09-04T01:12:40Z",
            "closedAt": "2024-09-04T01:12:40Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 7
            },
            "additions": 32,
            "deletions": 422,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Go Server Health Reporting",
            "url": "https://github.com/ollama/ollama/pull/6619",
            "state": "MERGED",
            "createdAt": "2024-09-03T22:04:10Z",
            "mergedAt": "2024-09-03T22:19:34Z",
            "closedAt": "2024-09-03T22:19:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 68,
            "deletions": 31,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use cuda v11 for driver 525 and older",
            "url": "https://github.com/ollama/ollama/pull/6620",
            "state": "MERGED",
            "createdAt": "2024-09-03T22:45:52Z",
            "mergedAt": "2024-09-04T00:15:31Z",
            "closedAt": "2024-09-04T00:15:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "It looks like driver 525 (aka, cuda driver 12.0) has problems with the cuda v12 library we compile against, so run v11 on those older drivers if detected.\r\n\r\nFixes #6556 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: sync llama.cpp to commit 8962422",
            "url": "https://github.com/ollama/ollama/pull/6621",
            "state": "MERGED",
            "createdAt": "2024-09-04T01:27:11Z",
            "mergedAt": "2024-09-04T19:14:50Z",
            "closedAt": "2024-09-04T19:14:50Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 231
            },
            "additions": 13181,
            "deletions": 8326,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md with PyOllaMx",
            "url": "https://github.com/ollama/ollama/pull/6624",
            "state": "MERGED",
            "createdAt": "2024-09-04T03:04:25Z",
            "mergedAt": "2024-09-04T03:10:53Z",
            "closedAt": "2024-09-04T03:10:53Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 1,
            "body": "Based on [this comment](https://github.com/ollama/ollama/issues/5937#issuecomment-2327760726), creating this PR to add PyOllaMx to list of ollama based applications",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "docs(integrations): add claude-dev",
            "url": "https://github.com/ollama/ollama/pull/6630",
            "state": "MERGED",
            "createdAt": "2024-09-04T07:50:19Z",
            "mergedAt": "2024-09-04T13:32:26Z",
            "closedAt": "2024-09-04T13:32:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "- Claude Dev [just added](https://github.com/saoudrizwan/claude-dev/releases/tag/v1.5.19) support for Ollama.\r\n\r\nIt's currently via the OpenAI compatible API, but specifically calls out Ollama as an option.\r\n\r\n<img width=\"594\" alt=\"image\" src=\"https://github.com/user-attachments/assets/21167eb3-5020-4f21-b354-27d4e7e04275\">\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "readme: add Cherry Studio to community integrations",
            "url": "https://github.com/ollama/ollama/pull/6633",
            "state": "MERGED",
            "createdAt": "2024-09-04T14:07:16Z",
            "mergedAt": "2024-09-04T14:53:36Z",
            "closedAt": "2024-09-04T14:53:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add an open source software\uff1ahttps://github.com/kangfenmao/cherry-studio\r\n\r\n<img width=\"1131\" alt=\"Cherry Studio 2024-07-18 16 15 51\" src=\"https://github.com/user-attachments/assets/aa4cd71b-576e-406f-afb8-c1dcf25dcd8e\">\r\n\r\n<img width=\"1277\" alt=\"image\" src=\"https://github.com/user-attachments/assets/22998e3b-ff34-4760-83af-3b2c10256846\">\r\n\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: use json.hpp from common",
            "url": "https://github.com/ollama/ollama/pull/6642",
            "state": "MERGED",
            "createdAt": "2024-09-04T19:54:36Z",
            "mergedAt": "2024-09-04T23:34:42Z",
            "closedAt": "2024-09-04T23:34:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 1,
            "deletions": 24597,
            "body": "The version of json.hpp from the 'common' module was no longer the same as the one within the 'ext_server' module. The discrepancy can cause linking errors depending on the functions used. This patch remove the old version in favor of using the one found in the common module.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Minor Go Server Fixes",
            "url": "https://github.com/ollama/ollama/pull/6643",
            "state": "MERGED",
            "createdAt": "2024-09-04T23:23:25Z",
            "mergedAt": "2024-09-04T23:50:38Z",
            "closedAt": "2024-09-04T23:50:39Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 7,
            "deletions": 3,
            "body": "A few fixes to avoid surprises as we get wider testing",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/6644",
            "state": "MERGED",
            "createdAt": "2024-09-04T23:39:55Z",
            "mergedAt": "2024-09-04T23:46:03Z",
            "closedAt": "2024-09-04T23:46:03Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "This is a pull request to include my Discord bot project, vnc-lm into the community integrations section. https://github.com/jk011ru/vnc-lm . Thanks",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Fix gemma2 2b conversion",
            "url": "https://github.com/ollama/ollama/pull/6645",
            "state": "MERGED",
            "createdAt": "2024-09-05T00:20:31Z",
            "mergedAt": "2024-09-06T00:02:28Z",
            "closedAt": "2024-09-06T00:02:28Z",
            "reviews": {
                "totalCount": 10
            },
            "files": {
                "totalCount": 4
            },
            "additions": 376,
            "deletions": 19,
            "body": "Gemma2 added some tensors which were not getting named correctly which caused a collision for the `ffm_norm` tensors. This change fixes the tensor names and adds a new unit test for converting gemma2 2b.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fixed redirect check if direct URL is already Present",
            "url": "https://github.com/ollama/ollama/pull/6656",
            "state": "MERGED",
            "createdAt": "2024-09-05T12:36:50Z",
            "mergedAt": "2024-09-05T17:48:27Z",
            "closedAt": "2024-09-05T17:48:27Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This is a fix regarding #6308 where the redirect check would fail with\r\n\r\n`unexpected status code 200`.\r\n\r\nThe problem is, that if you try to pull a Model from an internal Registry, there would be no redirect, but the current logic expects at least one redirect. So i've added a the StatusCode 200 - OK to the check and return the Location of the redirect.\r\n\r\nThe bug was introduced with [Pull#5962](https://github.com/ollama/ollama/pull/5962)",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Detect running in a container follow up",
            "url": "https://github.com/ollama/ollama/pull/6659",
            "state": "CLOSED",
            "createdAt": "2024-09-05T20:42:59Z",
            "mergedAt": null,
            "closedAt": "2024-09-05T21:19:30Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "Address additional review comments post merge.\r\n\r\nFollow up to #6495 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Revert \"Detect running in a container\"",
            "url": "https://github.com/ollama/ollama/pull/6662",
            "state": "MERGED",
            "createdAt": "2024-09-05T21:20:37Z",
            "mergedAt": "2024-09-05T21:26:00Z",
            "closedAt": "2024-09-05T21:26:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 23,
            "body": "Reverts ollama/ollama#6495\r\n\r\nTurns out this doesn't actually work on many platforms, so it doesn't serve much purpose.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Document uninstall on windows",
            "url": "https://github.com/ollama/ollama/pull/6663",
            "state": "MERGED",
            "createdAt": "2024-09-05T22:23:38Z",
            "mergedAt": "2024-09-05T22:57:38Z",
            "closedAt": "2024-09-05T22:57:38Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 0,
            "body": "Fixes #4920 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix \"presence_penalty_penalty\" typo, add test.",
            "url": "https://github.com/ollama/ollama/pull/6665",
            "state": "MERGED",
            "createdAt": "2024-09-06T00:04:35Z",
            "mergedAt": "2024-09-06T08:16:28Z",
            "closedAt": "2024-09-06T08:16:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 45,
            "deletions": 3,
            "body": "Fixes: https://github.com/ollama/ollama/issues/6640",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 3
            }
        }
    },
    {
        "node": {
            "title": "Improve logging on GPU too small",
            "url": "https://github.com/ollama/ollama/pull/6666",
            "state": "MERGED",
            "createdAt": "2024-09-06T00:22:17Z",
            "mergedAt": "2024-09-06T15:29:37Z",
            "closedAt": "2024-09-06T15:29:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 15,
            "deletions": 1,
            "body": "When we determine a GPU is too small for any layers, it's not always clear why. This will help troubleshoot those scenarios.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "adding Archyve to community integrations list",
            "url": "https://github.com/ollama/ollama/pull/6680",
            "state": "MERGED",
            "createdAt": "2024-09-06T19:05:57Z",
            "mergedAt": "2024-09-06T21:06:02Z",
            "closedAt": "2024-09-06T21:06:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "readme: add Plasmoid Ollama Control to community integrations",
            "url": "https://github.com/ollama/ollama/pull/6681",
            "state": "MERGED",
            "createdAt": "2024-09-06T20:08:47Z",
            "mergedAt": "2024-09-06T21:04:12Z",
            "closedAt": "2024-09-06T21:04:12Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "![Screenshot1](https://github.com/user-attachments/assets/f6c40de2-0a2a-49b1-8c39-0805889ceeb3)\r\n![Screenshot2](https://github.com/user-attachments/assets/2722c3b8-cf45-4803-90e8-4fc7a892fe0e)\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Remove go server debug logging",
            "url": "https://github.com/ollama/ollama/pull/6682",
            "state": "MERGED",
            "createdAt": "2024-09-06T23:47:54Z",
            "mergedAt": "2024-09-07T00:05:13Z",
            "closedAt": "2024-09-07T00:05:13Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 2
            },
            "additions": 3,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Align OpenAI Chat option processing with Completion option processing",
            "url": "https://github.com/ollama/ollama/pull/6688",
            "state": "MERGED",
            "createdAt": "2024-09-07T14:19:13Z",
            "mergedAt": "2024-09-07T16:08:09Z",
            "closedAt": "2024-09-07T16:08:09Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 6,
            "deletions": 6,
            "body": "https://github.com/ollama/ollama/pull/6514 removed the option scaling for OpenAI Completion requests.  Do the same for Chat requests.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/6696",
            "state": "MERGED",
            "createdAt": "2024-09-08T06:08:35Z",
            "mergedAt": "2024-09-08T07:36:00Z",
            "closedAt": "2024-09-08T07:36:00Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "added crewai with mesop anew since there was a conflict previously",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "catch when model vocab size is set correctly",
            "url": "https://github.com/ollama/ollama/pull/6714",
            "state": "MERGED",
            "createdAt": "2024-09-09T21:19:20Z",
            "mergedAt": "2024-09-10T00:18:55Z",
            "closedAt": "2024-09-10T00:18:55Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 3,
            "body": "This check catches if there are too many tokens in the tokenizer vs. the expected number of tokens specified in the `vocab_size` field of `config.json`. This typically happens if the `added_tokens` array in `tokenizer.json` ends up has too many tokens.\r\n\r\nRight now this results in the back end barfing during inference instead of catching it during `ollama create`.",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Quiet down dockers new lint warnings",
            "url": "https://github.com/ollama/ollama/pull/6716",
            "state": "MERGED",
            "createdAt": "2024-09-09T21:43:14Z",
            "mergedAt": "2024-09-10T00:22:20Z",
            "closedAt": "2024-09-10T00:22:20Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 24,
            "deletions": 24,
            "body": "Docker has recently added lint warnings to build.  This cleans up those warnings.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "docs: update llama3 to llama3.1",
            "url": "https://github.com/ollama/ollama/pull/6718",
            "state": "MERGED",
            "createdAt": "2024-09-09T22:31:50Z",
            "mergedAt": "2024-09-10T05:47:16Z",
            "closedAt": "2024-09-10T05:47:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 45,
            "deletions": 45,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "add *_proxy to env map for debugging",
            "url": "https://github.com/ollama/ollama/pull/6732",
            "state": "MERGED",
            "createdAt": "2024-09-10T16:37:53Z",
            "mergedAt": "2024-09-10T23:13:26Z",
            "closedAt": "2024-09-10T23:13:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 14,
            "deletions": 0,
            "body": "this adds entries into the env map on server start up so it logs http/https/no_proxy and their upper case variants. it allows easier debugging for proxy related issues\r\n\r\ne.g.\r\n```\r\n2024/09/10 09:33:32 routes.go:1125: INFO server config env=\"map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/path/to/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: http_proxy: https_proxy: no_proxy:]\"\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "runner.go: Prompt caching",
            "url": "https://github.com/ollama/ollama/pull/6735",
            "state": "MERGED",
            "createdAt": "2024-09-10T21:06:37Z",
            "mergedAt": "2024-09-11T03:45:00Z",
            "closedAt": "2024-09-11T03:45:00Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 4
            },
            "additions": 378,
            "deletions": 24,
            "body": "Currently, KV cache entries from a sequence are discarded at the end of each processing run. In a typical chat conversation, this results in each message taking longer and longer to process as the entire history of the conversation needs to be replayed.\r\n\r\nPrompt caching retains the KV entries as long as possible so that we only need to process the newest message in the converation, at least until there are too many simultaneous conversations and something needs to be evicted.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Verify permissions for AMD GPU",
            "url": "https://github.com/ollama/ollama/pull/6736",
            "state": "MERGED",
            "createdAt": "2024-09-10T22:03:00Z",
            "mergedAt": "2024-09-11T18:38:25Z",
            "closedAt": "2024-09-11T18:38:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 32,
            "deletions": 0,
            "body": "This adds back a check which was lost many releases back to verify /dev/kfd permissions which when lacking, can lead to confusing failure modes of:\r\n  \"rocBLAS error: Could not initialize Tensile host: No devices found\"\r\n\r\nThis implementation does not hard fail the serve command but instead will fall back to CPU with an error log.  In the future we can include this in the GPU discovery UX to show detected but unsupported devices we discovered.\r\n\r\nFixes #6685 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "add \"stop\" command",
            "url": "https://github.com/ollama/ollama/pull/6739",
            "state": "MERGED",
            "createdAt": "2024-09-11T01:44:17Z",
            "mergedAt": "2024-09-11T23:36:22Z",
            "closedAt": "2024-09-11T23:36:22Z",
            "reviews": {
                "totalCount": 8
            },
            "files": {
                "totalCount": 5
            },
            "additions": 172,
            "deletions": 25,
            "body": "This change adds the `ollama stop <model>` command which can be used to stop a running model. This is triggered by a call to `/api/generate` with an empty prompt and duration 0, or to `/api/chat` with empty messages and duration 0. The model will *expire*, but won't force being unloaded until any currently running instances have finished inference. When it's in this state it will show `Stopping...` in `ollama ps` to show that it's about to stop.\r\n\r\nIn the future we can add a _force_ argument to immediately unload a model and stop all inference, but that's beyond the scope of this change.\r\n\r\nFixes #6738 #4764 \r\nSupercedes #5328 ",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Polish loganalyzer example",
            "url": "https://github.com/ollama/ollama/pull/6744",
            "state": "MERGED",
            "createdAt": "2024-09-11T07:34:57Z",
            "mergedAt": "2024-09-12T01:37:38Z",
            "closedAt": "2024-09-12T01:37:38Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 3
            },
            "additions": 4,
            "deletions": 2,
            "body": "Just added a couple commands in case folks aren't familiar with python.\r\n\r\n```bash\r\n$  python loganalysis.py logtest.logfile\r\n>>>bash\r\n2023-11-10 07:17:44 192.168.65.1 - - [10/Nov/2023:13:17:43 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\" \"-\"\r\n2023-11-10 07:17:44 2023/11/10 13:17:44 [error] 29#29: *1 open() \"/usr/share/nginx/html/favicon.ico\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /favicon.ico HTTP/1.1\", host: \"localhost:8080\", referrer: \"http://localhost:8080/\"\r\n2023-11-10 07:17:44 192.168.65.1 - - [10/Nov/2023:13:17:44 +0000] \"GET /favicon.ico HTTP/1.1\" 404 555 \"http://localhost:8080/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\" \"-\"\r\n2023-11-10 07:17:50 2023/11/10 13:17:50 [error] 29#29: *1 open() \"/usr/share/nginx/html/ahstat\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /ahstat HTTP/1.1\", host: \"localhost:8080\"\r\n2023-11-10 07:17:50 192.168.65.1 - - [10/Nov/2023:13:17:50 +0000] \"GET /ahstat HTTP/1.1\" 404 555 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\" \"-\"\r\n2023-11-10 07:18:53 2023/11/10 13:18:53 [error] 29#29: *1 open() \"/usr/share/nginx/html/ahstat\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /ahstat HTTP/1.1\", host: \"localhost:8080\"\r\n2023-11-10 07:18:53 192.168.65.1 - - [10/Nov/2023:13:18:53 +0000] \"GET /ahstat HTTP/1.1\" 404 555 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\" \"-\"\r\n>>>\r\nThis log shows that there are four requests made to your Nginx server:\r\n\r\n1. A successful request for the root directory \"/\" which returns a `200 OK` status code and 615 bytes of data.\r\n2. An unsuccessful request for `/favicon.ico` which returns a `404 Not Found` status code and 555 bytes of data. This is because the file was not found in the expected location (`/usr/share/nginx/html/favicon.ico`).\r\n3. Two unsuccessful requests for `/ahstat`, each returning a `404 Not Found` status code and 555 bytes of data. This is because the file was not found in the expected location (`/usr/share/nginx/html/ahstat`).\r\n\r\nThe first error message indicates that Nginx could not find the `favicon.ico` file in the specified location, but it's not a critical error and does not affect the operation of your server. The other two errors are for requests to `/ahstat`, which means that there is no such file or directory on your server.\r\n\r\nTo fix these errors, you should make sure that all requested files exist in the appropriate locations and update your Nginx configuration if necessary. \r\n```\r\n\r\n\r\np.s. if you switch from codebooga to qwen2, the results are still pretty good despite a much smaller model. It didn't make my machine crawl rendering it, which was a nice plus.\r\n\r\n```bash\r\n$ python loganalysis.py logtest.logfile\r\nThe log entries you've provided seem to be from an Nginx server, which is commonly used as a web server and reverse proxy. Here's what each entry tells us:\r\n\r\n### 2023-11-10 07:17:40 2023/11/10 13:17:40 [notice] 1#1: start worker process 37\r\nThis line indicates that a new worker process (process ID 37) has started in the Nginx server.\r\n\r\n### 2023-11-10 07:17:44 192.168.65.1 - - [10/Nov/2023:13:17:43 +0000] \"GET / HTTP/1.1\" 200 615 \"-\"\r\nThis line shows a request for the root URL (\"/\") from an IP address (192.168.65.1) with status code `200` indicating success, and a response size of `615`. The `-` symbol typically indicates that no user agent or referer was sent.\r\n\r\n### 2023-11-10 07:17:44 [error] 29#29: *1 open() \"/usr/share/nginx/html/favicon.ico\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /favicon.ico HTTP/1.1\", host: \"localhost:8080\", referrer: \"http://localhost:8080/\"\r\nThis error indicates that the Nginx server was unable to find the file `favicon.ico` in the specified directory (`/usr/share/nginx/html`). The client attempting this request is at IP address 192.168.65.1, and it refers back to a previous request made from the same IP.\r\n\r\n### 2023-11-10 07:17:44 [error] 29#29: *1 open() \"/usr/share/nginx/html/ahstat\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /ahstat HTTP/1.1\", host: \"localhost:8080\"\r\nThis error is similar to the previous one but concerns a file named `ahstat`. The same IP address (192.168.65.1) and client context are involved.\r\n\r\n### 2023-11-10 07:18:53 [error] 29#29: *1 open() \"/usr/share/nginx/html/ahstat\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /ahstat HTTP/1.1\", host: \"localhost:8080\"\r\nThis is a repeat of the previous error message from 7 minutes later.\r\n\r\n### 2023-11-10 07:18:53 [error] 29#29: *1 open() \"/usr/share/nginx/html/ahstat\" failed (2: No such file or directory), client: 192.168.65.1, server: localhost, request: \"GET /ahstat HTTP/1.1\", host: \"localhost:8080\"\r\nThis is a repeat of the previous error message from another instance.\r\n\r\n### Other Entries\r\nThe other entries are similar to the first two but show requests for different files (`favicon.ico` and `ahstat`) that do not exist in the specified directory, resulting in 404 Not Found errors.\r\n\r\n### Conclusion\r\nThese logs indicate issues with Nginx attempting to serve non-existent files. The server is unable to locate `favicon.ico` and `ahstat`, which are likely custom resources intended for web pages but are missing from their designated directories (`/usr/share/nginx/html`). This could be due to incorrect file paths, missing files during deployment, or changes in the filesystem that Nginx does not recognize. To resolve these issues:\r\n\r\n1. **Check File Paths**: Ensure that the file paths specified in your configuration (e.g., `root /usr/share/nginx/html;`) are correct and match where your files actually reside.\r\n2. **File Existence**: Confirm that the files (`favicon.ico`, `ahstat`) exist at the specified locations on the filesystem.\r\n3. **Configuration Update**: If you've moved or deleted these files, update your Nginx configuration to reflect the new file paths or remove references to non-existent files.\r\n\r\nBy addressing these points, you can ensure that Nginx serves the correct resources without errors.\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/6752",
            "state": "MERGED",
            "createdAt": "2024-09-11T11:56:49Z",
            "mergedAt": "2024-09-12T01:36:26Z",
            "closedAt": "2024-09-12T01:36:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Added Ollama Mixture of Experts repository to terminal apps.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Added QodeAssist link to README.md",
            "url": "https://github.com/ollama/ollama/pull/6754",
            "state": "MERGED",
            "createdAt": "2024-09-11T13:22:04Z",
            "mergedAt": "2024-09-11T20:19:49Z",
            "closedAt": "2024-09-11T20:19:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "QodeAssist is using ollama to provide an AI-powered coding assistant plugin for Qt Creator",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "refactor show ouput",
            "url": "https://github.com/ollama/ollama/pull/6762",
            "state": "MERGED",
            "createdAt": "2024-09-11T19:46:47Z",
            "mergedAt": "2024-09-11T21:58:40Z",
            "closedAt": "2024-09-11T21:58:40Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 3
            },
            "additions": 277,
            "deletions": 105,
            "body": "fixes line wrapping on long texts. the previous code was doing multiple passes through a tablewriting and breaking the intermediate outputs into lines to feed back into a table writer. since some fields are much longer than others, the column widths become inflated causing everything to be filled with whitespace\r\n\r\nthis change fixes the root issue by using individual tablewriters for each section which allows each section to be rendered independently. any long text will not impact the width of unrelated tables. the output itself should be largely unchanged\r\n\r\n```\r\n$ ollama show maybe\r\n  Model\r\n        parameters              8.0B\r\n        quantization            Q4_0\r\n        architecture            llama\r\n        context length          131072\r\n        embedding length        4096\r\n\r\n  Parameters\r\n        stop    \"<|start_header_id|>\"\r\n        stop    \"<|end_header_id|>\"\r\n        stop    \"<|eot_id|>\"\r\n\r\n  System\r\n        You are a world-class AI system, capable of complex reasoning and reflection. Reason through the\r\n        query inside <thinking> tags, and then provide your final response inside <output> tags. If you\r\n        detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection>\r\n        tags.\r\n\r\n  License\r\n        LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\r\n        Llama 3.1 Version Release Date: July 23, 2024\r\n```\r\n\r\nresolves #6740 \r\nresolves #6763",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Flush pending responses before returning (#6707)",
            "url": "https://github.com/ollama/ollama/pull/6765",
            "state": "MERGED",
            "createdAt": "2024-09-11T23:03:04Z",
            "mergedAt": "2024-09-11T23:38:25Z",
            "closedAt": "2024-09-11T23:38:25Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 55,
            "deletions": 51,
            "body": "I'll cross-port the server.cpp portion to main after this goes in.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "documentation for stopping a model",
            "url": "https://github.com/ollama/ollama/pull/6766",
            "state": "MERGED",
            "createdAt": "2024-09-11T23:23:40Z",
            "mergedAt": "2024-09-18T23:26:42Z",
            "closedAt": "2024-09-18T23:26:42Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 105,
            "deletions": 4,
            "body": "This PR includes documentation changes which should be merged when the release with the `ollama stop` command goes live (from PR #6739)",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "runner: Flush pending responses before returning",
            "url": "https://github.com/ollama/ollama/pull/6767",
            "state": "MERGED",
            "createdAt": "2024-09-11T23:42:07Z",
            "mergedAt": "2024-09-12T00:20:22Z",
            "closedAt": "2024-09-12T00:20:22Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 33,
            "deletions": 27,
            "body": "If there are any pending reponses (such as from potential stop tokens) then we should send them back before ending the sequence. Otherwise, we can be missing tokens at the end of a response.\r\n\r\nFixes #6707",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use GOARCH for build dirs",
            "url": "https://github.com/ollama/ollama/pull/6779",
            "state": "MERGED",
            "createdAt": "2024-09-12T22:54:22Z",
            "mergedAt": "2024-09-12T23:38:05Z",
            "closedAt": "2024-09-12T23:38:05Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 12,
            "deletions": 12,
            "body": "Corrects x86_64 vs amd64 discrepancy\r\n\r\nSee https://github.com/ollama/ollama/blob/main/llm/generate/gen_common.sh#L6\r\n\r\nThis went unnoticed from #6547 due to the wildcards used in the payload, but once I tried to carry this over to the Go server branch we noticed when toggling between the C++ and Go servers they weren't correctly swapping back and forth on Linux.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix incremental builds on linux",
            "url": "https://github.com/ollama/ollama/pull/6780",
            "state": "MERGED",
            "createdAt": "2024-09-13T00:24:11Z",
            "mergedAt": "2024-09-13T15:24:08Z",
            "closedAt": "2024-09-13T15:24:08Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Building in containers worked fine.  Building iteratively on a persistent filesystem doesn't. Running the install function twice on a second generate caused an error cp: cannot stat '../build/linux/amd64/cpu/bin/libllama.so': No such file or directory",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add Agents-Flex Libraries in README.md",
            "url": "https://github.com/ollama/ollama/pull/6788",
            "state": "MERGED",
            "createdAt": "2024-09-13T06:37:08Z",
            "mergedAt": "2024-09-16T20:42:52Z",
            "closedAt": "2024-09-16T20:42:52Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "readme: add Obsidian Quiz Generator plugin to community integrations",
            "url": "https://github.com/ollama/ollama/pull/6789",
            "state": "MERGED",
            "createdAt": "2024-09-13T07:48:01Z",
            "mergedAt": "2024-09-15T03:52:37Z",
            "closedAt": "2024-09-15T03:52:37Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "**Plugin Demo**\r\n\r\nhttps://github.com/user-attachments/assets/24e57fcf-2cbf-4797-a161-4c4a05e518bf\r\n\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add vim-intelligence-bridge to Terminal section  in README",
            "url": "https://github.com/ollama/ollama/pull/6818",
            "state": "MERGED",
            "createdAt": "2024-09-15T23:39:00Z",
            "mergedAt": "2024-09-16T01:20:36Z",
            "closedAt": "2024-09-16T01:20:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I'm proposing to add vim-intelligence-bridge to the Terminal section. This plugin is unique as it's the only one that integrates Ollama directly with traditional Vim (not Neovim). Key benefits:\r\n\r\n- Expands Ollama's reach to Vim users\r\n- Promotes local, private AI processing within Vim\r\n- Enhances developer productivity by integrating Ollama in the Vim workflow\r\n\r\nThis addition showcases Ollama's versatility and encourages wider adoption in the developer community.\r\nProposed change:\r\n\r\n[vim-intelligence-bridge](https://github.com/pepo-ec/vim-intelligence-bridge) Simple interaction of \"Ollama\" with the Vim editor",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: doc: explain golang objc linker warning",
            "url": "https://github.com/ollama/ollama/pull/6830",
            "state": "MERGED",
            "createdAt": "2024-09-16T21:00:15Z",
            "mergedAt": "2024-09-16T21:21:35Z",
            "closedAt": "2024-09-16T21:21:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "cache: Clear old KV cache entries when evicting a slot",
            "url": "https://github.com/ollama/ollama/pull/6831",
            "state": "MERGED",
            "createdAt": "2024-09-16T21:06:24Z",
            "mergedAt": "2024-09-16T21:15:56Z",
            "closedAt": "2024-09-16T21:15:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "When forking a cache entry, if no empty slots are available we evict the least recently used one and copy over the KV entries from the closest match. However, this copy does not overwrite existing values but only adds new ones. Therefore, we need to clear the old slot first.\r\n\r\nThis change fixes two issues:\r\n - The KV cache fills up and runs out of space even though we think we are managing it correctly\r\n - Performance gets worse over time as we use new cache entries that are not hot in the processor caches",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI: clean up naming, fix tagging latest",
            "url": "https://github.com/ollama/ollama/pull/6832",
            "state": "MERGED",
            "createdAt": "2024-09-16T22:38:51Z",
            "mergedAt": "2024-09-16T23:18:41Z",
            "closedAt": "2024-09-16T23:18:41Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 10,
            "deletions": 32,
            "body": "The rocm CI step for RCs was incorrectly tagging them as the latest rocm build.  This also fixes the latest tagging script.\r\n\r\nI've manually retagged and pushed the `ollama/ollama:rocm` tag so it points to 0.3.10 instead of 0.3.11-rc2\r\n\r\nVerified the tagging script by running the following and confirming the digests\r\n```\r\nDOCKER_ORG=dhiltgen VERSION=0.3.10-rc1-144-g6989ddd ./scripts/tag_latest.sh\r\n```\r\nhttps://hub.docker.com/r/dhiltgen/ollama/tags",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "make patches git am-able",
            "url": "https://github.com/ollama/ollama/pull/6833",
            "state": "MERGED",
            "createdAt": "2024-09-16T23:14:47Z",
            "mergedAt": "2024-09-17T23:33:23Z",
            "closedAt": "2024-09-17T23:33:23Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 10
            },
            "additions": 130,
            "deletions": 55,
            "body": "raw diffs are `git apply`-able but not `git am`-able. git patches, e.g. through `git format-patch`, are both `git apply`-able and `git am`-able.\r\n\r\n`git am`-able patches is very useful because changes can made incrementally and exported with `git format-patch`. this makes developing patches much easier since each iteration doesn't require regenerating the full diff. it also allows patches to be develop in the order they're applied.\r\n\r\nchecking out patched files is unnecessary since the submodule is reset (`update -f`) before patches are applied",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add python examples for `bespoke-minicheck`",
            "url": "https://github.com/ollama/ollama/pull/6841",
            "state": "MERGED",
            "createdAt": "2024-09-17T16:57:14Z",
            "mergedAt": "2024-09-18T16:35:25Z",
            "closedAt": "2024-09-18T16:35:25Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 6
            },
            "additions": 346,
            "deletions": 0,
            "body": "Adds two examples `python-grounded-factuality-rag-check` and  `python-grounded-factuality-simple-check` which showcase the `bespoke-minicheck` model. ",
            "participants": {
                "totalCount": 4
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Refine developer docs for Go server",
            "url": "https://github.com/ollama/ollama/pull/6842",
            "state": "MERGED",
            "createdAt": "2024-09-17T19:09:06Z",
            "mergedAt": "2024-09-27T22:12:40Z",
            "closedAt": "2024-09-27T22:12:40Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 39,
            "deletions": 32,
            "body": "This enhances the documentation for development focusing on a minimal single known to work set of tools.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "llama: fix race in parallel make",
            "url": "https://github.com/ollama/ollama/pull/6845",
            "state": "CLOSED",
            "createdAt": "2024-09-17T21:19:07Z",
            "mergedAt": null,
            "closedAt": "2024-09-23T19:03:54Z",
            "reviews": {
                "totalCount": 5
            },
            "files": {
                "totalCount": 1
            },
            "additions": 3,
            "deletions": 2,
            "body": "Ensure the cleanup step completes before starting to build targets",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "add solar pro (preview)",
            "url": "https://github.com/ollama/ollama/pull/6846",
            "state": "MERGED",
            "createdAt": "2024-09-17T22:19:38Z",
            "mergedAt": "2024-09-18T01:11:26Z",
            "closedAt": "2024-09-18T01:11:26Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 402,
            "deletions": 0,
            "body": "solar-pro introduces block skip connections where blocks are connected to other, non-sequential blocks with a scale multiple\r\n\r\nthis change adds 4 new keys to store the skip connections and one new tensor to store the scalar. the scalar is implemented as a 1-dimensional tensor with 2 elements derived from the model's `bskcn_tv` configuration. in general, the values are `bskcn_tv, 1 - bskcn_tv`\r\n\r\nhttps://huggingface.co/upstage/solar-pro-preview-instruct",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: gather transitive dependencies for rocm for dist packaging",
            "url": "https://github.com/ollama/ollama/pull/6848",
            "state": "MERGED",
            "createdAt": "2024-09-17T22:40:29Z",
            "mergedAt": "2024-09-18T15:32:36Z",
            "closedAt": "2024-09-18T15:32:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 5,
            "deletions": 2,
            "body": "Go generate equivalent logic is [here](https://github.com/ollama/ollama/blob/main/llm/generate/gen_linux.sh#L283-L290)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: add tool parsing support for nemotron-mini",
            "url": "https://github.com/ollama/ollama/pull/6849",
            "state": "MERGED",
            "createdAt": "2024-09-17T22:57:11Z",
            "mergedAt": "2024-09-18T01:06:16Z",
            "closedAt": "2024-09-18T01:06:16Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 4
            },
            "additions": 144,
            "deletions": 39,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix missing dep path on windows CPU runners",
            "url": "https://github.com/ollama/ollama/pull/6884",
            "state": "MERGED",
            "createdAt": "2024-09-19T22:20:44Z",
            "mergedAt": "2024-09-21T23:28:29Z",
            "closedAt": "2024-09-21T23:28:29Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 6,
            "body": "GPUs handled the dependency path properly, but CPU runners didn't which results in missing vc redist libraries on systems where the user didn't already have it installed from some other app.\r\n\r\nVerified on a CPU only windows system without vc redist installed.  (Failed without this fix)\r\n\r\nFixes #6804 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI: adjust step ordering for win arm to match x64",
            "url": "https://github.com/ollama/ollama/pull/6895",
            "state": "MERGED",
            "createdAt": "2024-09-20T21:09:30Z",
            "mergedAt": "2024-09-20T21:20:57Z",
            "closedAt": "2024-09-20T21:20:57Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 26,
            "deletions": 26,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI: win arm adjustments",
            "url": "https://github.com/ollama/ollama/pull/6898",
            "state": "MERGED",
            "createdAt": "2024-09-20T23:53:51Z",
            "mergedAt": "2024-09-20T23:58:56Z",
            "closedAt": "2024-09-20T23:58:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 20,
            "deletions": 7,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "runner: Set windows above normal priority for consistent CPU inference performance",
            "url": "https://github.com/ollama/ollama/pull/6905",
            "state": "MERGED",
            "createdAt": "2024-09-21T23:17:22Z",
            "mergedAt": "2024-09-21T23:54:49Z",
            "closedAt": "2024-09-21T23:54:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 8,
            "deletions": 2,
            "body": "When running the subprocess as a background service windows may throttle, which can lead to thrashing and very poor token rate.\r\n\r\nFixes #3511 \r\n\r\nI've now reproduced the performance problem and understand the nature of what's going on leading to poor CPU inference performance for some users on Windows.\r\n\r\nWindows treats GUI apps and background services differently.  By default, priority is given to GUI apps (\"Programs\")\r\n\r\n![image](https://github.com/user-attachments/assets/33f6d5f4-74d5-4d35-9e33-6c3d45865687)\r\n\r\nWhile you can change this, this wouldn't typically be recommended.  The result is when the tray app is automatically started when user logs in, or by clicking \"Ollama\" from the start menu, it runs as a \"background service\" which the subprocess inherits.  In some situations, this can lead to the subprocess runner being throttled, and since we try to create as many threads as there are cores, this results in thrashing behavior, and very poor token rates.\r\n\r\n![image](https://github.com/user-attachments/assets/55207d05-4db3-4180-8d8d-9775bb531049)\r\n\r\nBy setting the scheduler priority class to \"above normal\", this allows more CPU usage, and higher CPU load, leading to much better token rates.\r\n\r\n![image](https://github.com/user-attachments/assets/70d958ab-3492-4419-9130-5895bbcd1294)\r\n![image](https://github.com/user-attachments/assets/2f2c0ff8-69ad-4062-bdc1-aad28cd0235f)\r\n\r\n\r\nI also tried setting the scheduler priority to \"high\" (not recommended in the API docs) and that did lead to the UI freezing, and after ~1 minute, a BSOD and reboot due to a watchdog detecting the system becoming unresponsive.  The setting of \"Above Normal\" does not appear to have any noticeable impact on UI performance, and token rates match when the ollama server is running from a terminal (and inherits the \"Program\" priority). I do see the CPU utilization drop down slightly over time, so it seems Windows does penalize CPU saturating background processes so for very large context size requests there may still be room to improve performance further.\r\n\r\n\r\nReference:\r\n- https://learn.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Use punkt_tab instead of punkt.",
            "url": "https://github.com/ollama/ollama/pull/6907",
            "state": "MERGED",
            "createdAt": "2024-09-22T01:38:57Z",
            "mergedAt": "2024-09-22T01:55:28Z",
            "closedAt": "2024-09-22T01:55:28Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "This was causing an error since we depend on punkt_tab.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Add LLMChat to community apps",
            "url": "https://github.com/ollama/ollama/pull/6919",
            "state": "MERGED",
            "createdAt": "2024-09-23T13:45:39Z",
            "mergedAt": "2024-09-24T00:49:47Z",
            "closedAt": "2024-09-24T00:49:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Go server refine gpu build",
            "url": "https://github.com/ollama/ollama/pull/6924",
            "state": "MERGED",
            "createdAt": "2024-09-23T19:00:49Z",
            "mergedAt": "2024-09-26T18:32:14Z",
            "closedAt": "2024-09-26T18:32:14Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 9
            },
            "additions": 445,
            "deletions": 409,
            "body": "This breaks up the monolithic Makefile for the Go based runners into a set of utility files as well as recursive Makefiles for the runners. Files starting with the name \"Makefile\" are buildable, while files that end with \".make\" are utilities to include in other Makefiles.  This reduces the amount of nearly identical targets and helps set a pattern for future community contributions for new GPU runner architectures.\r\n\r\nWhen we are ready to switch over to the Go runners, these files should move to the top of the repo, and we should add targets for the main CLI, as well as a helper \"install\" (put all the built binaries on the local system in a runnable state) and \"dist\" target (generate the various tar/zip files for distribution) for local developer use.\r\n\r\nReplaces #6845 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "docs: update llamaindex links",
            "url": "https://github.com/ollama/ollama/pull/6939",
            "state": "MERGED",
            "createdAt": "2024-09-24T18:44:21Z",
            "mergedAt": "2024-09-24T19:15:43Z",
            "closedAt": "2024-09-24T19:15:43Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "CI: Fix win arm version defect",
            "url": "https://github.com/ollama/ollama/pull/6940",
            "state": "MERGED",
            "createdAt": "2024-09-24T20:21:17Z",
            "mergedAt": "2024-09-24T22:18:10Z",
            "closedAt": "2024-09-24T22:18:11Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "Build 0.3.12-rc5 reports a pre-release string on win-arm due to the version not being set properly in CI.\r\n\r\nwrite-host in powershell writes directly to the console and will not be picked\r\nup by a pipe.  Echo, or write-output will.\r\n",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "README: Fix llama3.1 -> llama3.2 typo",
            "url": "https://github.com/ollama/ollama/pull/6962",
            "state": "MERGED",
            "createdAt": "2024-09-25T18:48:09Z",
            "mergedAt": "2024-09-25T18:53:47Z",
            "closedAt": "2024-09-25T18:53:47Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "server: close response body on error",
            "url": "https://github.com/ollama/ollama/pull/6986",
            "state": "MERGED",
            "createdAt": "2024-09-26T18:20:57Z",
            "mergedAt": "2024-09-26T19:00:31Z",
            "closedAt": "2024-09-26T19:00:31Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 4,
            "deletions": 0,
            "body": "This change closes the response body when an error occurs in makeRequestWithRetry. Previously, the first, non-200 response body was not closed before reattempting the request. This change ensures that the response body is closed in all cases where an error occurs, preventing leaks of file descriptors.\r\n\r\nFixes #6974",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: don't create extraneous directories",
            "url": "https://github.com/ollama/ollama/pull/6988",
            "state": "MERGED",
            "createdAt": "2024-09-26T20:43:04Z",
            "mergedAt": "2024-09-26T21:05:31Z",
            "closedAt": "2024-09-26T21:05:31Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 1,
            "body": "With the .WAIT this shouldn't be necessary any more, and was causing payload processing glitches.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Exercise the new build in CI",
            "url": "https://github.com/ollama/ollama/pull/6989",
            "state": "MERGED",
            "createdAt": "2024-09-26T21:05:00Z",
            "mergedAt": "2024-09-27T21:49:55Z",
            "closedAt": "2024-09-27T21:49:56Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 42,
            "deletions": 0,
            "body": "Conditionally build the new runners if any code in that tree is touched.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "llama: add compiler tags for cpu features",
            "url": "https://github.com/ollama/ollama/pull/7009",
            "state": "CLOSED",
            "createdAt": "2024-09-27T21:48:58Z",
            "mergedAt": null,
            "closedAt": "2024-10-08T15:53:59Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 16
            },
            "additions": 234,
            "deletions": 110,
            "body": "Replaced by #7137 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Update README.md",
            "url": "https://github.com/ollama/ollama/pull/7027",
            "state": "MERGED",
            "createdAt": "2024-09-29T06:11:33Z",
            "mergedAt": "2024-09-29T20:01:01Z",
            "closedAt": "2024-09-29T20:01:01Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "add our new LLM tool on Ollama: argo\r\nplease approve, thank you.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Stop model before deletion if loaded (fixed #6957)",
            "url": "https://github.com/ollama/ollama/pull/7050",
            "state": "MERGED",
            "createdAt": "2024-09-30T17:09:35Z",
            "mergedAt": "2024-10-01T22:45:43Z",
            "closedAt": "2024-10-01T22:45:43Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 4
            },
            "additions": 310,
            "deletions": 58,
            "body": "This PR adds the same logic as `StopHandler` before model removal. If the model is not loaded, no error is raised.\r\n\r\nI also added a few tests on the server DeleteHandler, and fixed an error formatting bug where `ollama rm` would return a \"file not found\" error for the manifest file instead of a proper error message if the model does not exist\r\n\r\nFixes #6957 \r\n\r\n\r\nTested with:\r\n```\r\n$ ./ollama run localhost/phi/phi:latest\r\n$ ./ollama ps\r\nNAME                        ID              SIZE      PROCESSOR    UNTIL\r\nlocalhost/phi/phi:latest    e2fd6321a5fe    5.5 GB    100% GPU     4 minutes from now\r\n$ ./ollama rm localhost/phi/phi:latest\r\ndeleted 'localhost/phi/phi:latest'\r\n$ ./ollama ps\r\nNAME    ID    SIZE    PROCESSOR    UNTIL\r\n```",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: adjust clip patch for mingw utf-16",
            "url": "https://github.com/ollama/ollama/pull/7065",
            "state": "MERGED",
            "createdAt": "2024-10-01T16:35:34Z",
            "mergedAt": "2024-10-01T22:24:26Z",
            "closedAt": "2024-10-01T22:24:26Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 44,
            "deletions": 7,
            "body": "Fix the patch to compile under mingw, and remove extraneous runtime dependencies",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: Add CI to verify all vendored changes have patches",
            "url": "https://github.com/ollama/ollama/pull/7066",
            "state": "MERGED",
            "createdAt": "2024-10-01T17:54:45Z",
            "mergedAt": "2024-10-01T18:16:10Z",
            "closedAt": "2024-10-01T18:16:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 12,
            "deletions": 0,
            "body": "With the new vendoring model we want to make sure we don't accidentally merge changes in the vendored code without having those changes covered by a patch that applies cleanly on the current baseline.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "runner.go: Enable llamafile (all platforms) and BLAS (Mac OS)",
            "url": "https://github.com/ollama/ollama/pull/7068",
            "state": "MERGED",
            "createdAt": "2024-10-01T21:26:52Z",
            "mergedAt": "2024-10-01T22:58:50Z",
            "closedAt": "2024-10-01T22:58:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 6
            },
            "additions": 484,
            "deletions": 4,
            "body": "These are two features that are shown on llama.cpp's system info that are currently different between the two runners. On my test systems the performance difference is very small to negligible but it is probably still good to equalize the features.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: Don't add BOS/EOS for tokenize requests",
            "url": "https://github.com/ollama/ollama/pull/7071",
            "state": "MERGED",
            "createdAt": "2024-10-01T23:29:53Z",
            "mergedAt": "2024-10-01T23:46:23Z",
            "closedAt": "2024-10-01T23:46:23Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 2,
            "body": "This is consistent with what server.cpp currently does. It affects things like token processing counts for embedding requests.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: fix compiler flag differences",
            "url": "https://github.com/ollama/ollama/pull/7082",
            "state": "MERGED",
            "createdAt": "2024-10-02T20:15:43Z",
            "mergedAt": "2024-10-03T20:22:24Z",
            "closedAt": "2024-10-03T20:22:24Z",
            "reviews": {
                "totalCount": 2
            },
            "files": {
                "totalCount": 3
            },
            "additions": 19,
            "deletions": 15,
            "body": "Adjust the flags for the new Go server to more closely match the generate flow\r\n\r\nStill more refinement to do, but I think this gets us closer.  Comparisons on an Ryzen 9 7950X system (windows) (linux)\r\n\r\nThis PR:  llama3.2 tps\r\n- cpu: 9.17(w) 4.07(l)\r\n- cpu_avx: 14.19(w) 14.78(l)\r\n- cpu_avx2: 14.3(w) 15.3(l)\r\n\r\nC++ server (0.3.12 release)\r\n- cpu: 5.9(w) 7.34(l)\r\n- cpu_avx: 17.5(w) 21.47(l)\r\n- cpu_avx2: 19 tps(w) 22.63(l)",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "Add G1 to list of integrations",
            "url": "https://github.com/ollama/ollama/pull/7096",
            "state": "MERGED",
            "createdAt": "2024-10-03T23:49:55Z",
            "mergedAt": "2024-10-05T18:57:53Z",
            "closedAt": "2024-10-05T18:57:53Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "I added g1 to the list of integrations in the readme file. Hopefully this can bring this project more attention.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Update README.md - replace stale links",
            "url": "https://github.com/ollama/ollama/pull/7117",
            "state": "MERGED",
            "createdAt": "2024-10-07T14:20:52Z",
            "mergedAt": "2024-10-08T01:16:56Z",
            "closedAt": "2024-10-08T01:16:56Z",
            "reviews": {
                "totalCount": 3
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "The links were outdated and led to 404 pages.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: refine developer docs",
            "url": "https://github.com/ollama/ollama/pull/7121",
            "state": "MERGED",
            "createdAt": "2024-10-07T19:18:10Z",
            "mergedAt": "2024-10-07T19:43:46Z",
            "closedAt": "2024-10-07T19:43:46Z",
            "reviews": {
                "totalCount": 6
            },
            "files": {
                "totalCount": 1
            },
            "additions": 23,
            "deletions": 4,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llama: doc and example clean up",
            "url": "https://github.com/ollama/ollama/pull/7122",
            "state": "MERGED",
            "createdAt": "2024-10-07T20:51:53Z",
            "mergedAt": "2024-10-07T21:42:17Z",
            "closedAt": "2024-10-07T21:42:17Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 9
            },
            "additions": 181,
            "deletions": 210,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Fix build leakages",
            "url": "https://github.com/ollama/ollama/pull/7141",
            "state": "MERGED",
            "createdAt": "2024-10-08T18:00:51Z",
            "mergedAt": "2024-10-08T20:05:00Z",
            "closedAt": "2024-10-08T20:05:00Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 2,
            "deletions": 10,
            "body": "The recent change to applying patches leaves the submodule dirty based on \"new commits\" being present.  This ensures we clean up so the tree no longer reports dirty after a `go generate ./...` run.\r\n\r\nThe Makefile was being a bit too aggressive in cleaning things up and would result in deleting the placeholder files which someone might accidentally commit.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "prompt: Retain image through messages in a conversation",
            "url": "https://github.com/ollama/ollama/pull/7145",
            "state": "CLOSED",
            "createdAt": "2024-10-09T00:08:04Z",
            "mergedAt": null,
            "closedAt": "2024-10-10T19:04:49Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 22,
            "deletions": 16,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 1
            }
        }
    },
    {
        "node": {
            "title": "update .gitattributes with proper linguist-vendored entry",
            "url": "https://github.com/ollama/ollama/pull/7154",
            "state": "CLOSED",
            "createdAt": "2024-10-09T20:52:51Z",
            "mergedAt": null,
            "closedAt": "2024-10-10T00:25:10Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 1,
            "body": "",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix vendoring attribute",
            "url": "https://github.com/ollama/ollama/pull/7155",
            "state": "MERGED",
            "createdAt": "2024-10-09T21:04:20Z",
            "mergedAt": "2024-10-09T21:21:02Z",
            "closedAt": "2024-10-09T21:21:02Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 7,
            "deletions": 1,
            "body": "Expand out the file extensions for vendored code so git reports the status correctly\r\n\r\ne.g.:\r\n```\r\n% git check-attr -a -- ./llama/ggml.c\r\n./llama/ggml.c: text: auto\r\n./llama/ggml.c: linguist-vendored: set\r\n```",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "fix vendoring attribute for metal",
            "url": "https://github.com/ollama/ollama/pull/7156",
            "state": "MERGED",
            "createdAt": "2024-10-09T21:25:20Z",
            "mergedAt": "2024-10-09T22:22:36Z",
            "closedAt": "2024-10-09T22:22:36Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 1,
            "deletions": 0,
            "body": "Add missing metal files to vendoring list",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "runner.go: Handle truncation of tokens for stop sequences",
            "url": "https://github.com/ollama/ollama/pull/7158",
            "state": "MERGED",
            "createdAt": "2024-10-10T01:03:55Z",
            "mergedAt": "2024-10-10T03:39:04Z",
            "closedAt": "2024-10-10T03:39:04Z",
            "reviews": {
                "totalCount": 4
            },
            "files": {
                "totalCount": 4
            },
            "additions": 64,
            "deletions": 35,
            "body": "When a single token contains both text to be return and a stop sequence, this causes an out of bounds error when we update the cache to match our text. This is because we currently assume that the removing the stop sequence will consume at least one token.\r\n\r\nThis also inverts the logic to deal with positive numbers, rather than a value to be subtracted, which is easier to reason about.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "Send all images in conversation history",
            "url": "https://github.com/ollama/ollama/pull/7164",
            "state": "MERGED",
            "createdAt": "2024-10-10T17:39:19Z",
            "mergedAt": "2024-10-10T18:21:51Z",
            "closedAt": "2024-10-10T18:21:51Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 0,
            "deletions": 7,
            "body": "@pdevine I'm not entirely sure what the history of this check was - it sounds like there were previously some models that didn't do well with images in the history. However, it worked well with the models that I tried it on.\r\n\r\nRegardless, I don't think the check belongs in the CLI.",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 0
            }
        }
    },
    {
        "node": {
            "title": "llm: Remove GGML_CUDA_NO_PEER_COPY for ROCm",
            "url": "https://github.com/ollama/ollama/pull/7174",
            "state": "MERGED",
            "createdAt": "2024-10-11T19:28:45Z",
            "mergedAt": "2024-10-12T16:56:50Z",
            "closedAt": "2024-10-12T16:56:50Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 2
            },
            "additions": 1,
            "deletions": 2,
            "body": "This workaround logic in llama.cpp is causing crashes for users with less system memory than VRAM.\r\n\r\nThe original change adding this was meant to resolve gibberish responses on multi-GPU radeon setups.  I'll focus my testing on that to see if I see any regressions, and if not, drop the draft status.\r\n\r\nFixes #6756 ",
            "participants": {
                "totalCount": 3
            },
            "comments": {
                "totalCount": 2
            }
        }
    },
    {
        "node": {
            "title": "Fix regression on older macos versions",
            "url": "https://github.com/ollama/ollama/pull/7192",
            "state": "MERGED",
            "createdAt": "2024-10-13T17:08:14Z",
            "mergedAt": "2024-10-13T17:47:42Z",
            "closedAt": "2024-10-13T17:47:42Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 6,
            "deletions": 6,
            "body": "The new cgo compilation requires a flag to target older macos versions\r\n\r\nFixes #7190 ",
            "participants": {
                "totalCount": 2
            },
            "comments": {
                "totalCount": 5
            }
        }
    },
    {
        "node": {
            "title": "Add missing BF16 tensor type.",
            "url": "https://github.com/ollama/ollama/pull/7193",
            "state": "MERGED",
            "createdAt": "2024-10-13T17:46:23Z",
            "mergedAt": "2024-10-15T00:06:35Z",
            "closedAt": "2024-10-15T00:06:35Z",
            "reviews": {
                "totalCount": 1
            },
            "files": {
                "totalCount": 1
            },
            "additions": 2,
            "deletions": 0,
            "body": "Models with BF16 tensors are not imported because the typeSize is 0.\r\n\r\nFixes: https://github.com/ollama/ollama/issues/7188",
            "participants": {
                "totalCount": 5
            },
            "comments": {
                "totalCount": 1
            }
        }
    }
]